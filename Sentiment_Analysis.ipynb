{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to the Sentiment Analysis Project."
      ],
      "metadata": {
        "id": "TIudY0sCyxtH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPMs8Ea01HR3"
      },
      "source": [
        "# loading important library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUM947g90Qde",
        "outputId": "bf3005ae-4d6c-4c6e-cfda-fbccc4d06123"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# important url for word2vec = \"https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g\"\n",
        "# connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# dirictory to data file\n",
        "dir = \"/content/drive/MyDrive/AI-projects/SentimentAnalysis/data.csv\"\n",
        "dir_models = \"/content/drive/MyDrive/AI-projects/SentimentAnalysis/models\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "p3KxTdNSAhSa"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install tensorflow_text\n",
        "!pip install transformers\n",
        "!pip install keras_preprocessing\n",
        "!pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi9Md-vHAhSb",
        "outputId": "f22020f4-d5b8-4a92-dca8-fa1c972cbd0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# importing tensorflow and keras\n",
        "# import keras\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "# from keras.layers import Dense, Activation, Dropout\n",
        "\n",
        "# from keras.utils import np_utilts\n",
        "from keras.layers import Bidirectional, SpatialDropout1D\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as text\n",
        "# gensim imports\n",
        "from gensim.models import KeyedVectors\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "import joblib\n",
        "import nltk as nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# sklearn imports\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, accuracy_score,  confusion_matrix, make_scorer  # classification Metrics\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "from sklearn import pipeline\n",
        "from sklearn import linear_model\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import model_from_json\n",
        "\n",
        "# Hugging Face Transformers\n",
        "from transformers import (pipeline , BertTokenizer,\n",
        "                          TFBertForSequenceClassification,\n",
        "                          InputExample, InputFeatures ,\n",
        "                         AutoTokenizer, TFAutoModelForSequenceClassification,\n",
        "                         TFRobertaModel, TFGPT2Model, RobertaTokenizer, GPT2Tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfsjvuOl1ZD9"
      },
      "source": [
        "# loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0UO0BBguJ3l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "451Kk7yQ0QrZ"
      },
      "outputs": [],
      "source": [
        "old_df = pd.read_csv(dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yog-Frk32zcl",
        "outputId": "c6bffa64-6dfc-48ee-ac24-6c5efc613821"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-edf4a995-b8ca-438f-97ee-a64128a60983\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The GeoSolutions technology will leverage Bene...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>$ESI on lows, down $1.50 to $2.50 BK a real po...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>According to the Finnish-Russian Chamber of Co...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The Swedish buyout firm has sold its remaining...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-edf4a995-b8ca-438f-97ee-a64128a60983')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-edf4a995-b8ca-438f-97ee-a64128a60983 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-edf4a995-b8ca-438f-97ee-a64128a60983');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                            Sentence Sentiment\n",
              "0  The GeoSolutions technology will leverage Bene...  positive\n",
              "1  $ESI on lows, down $1.50 to $2.50 BK a real po...  negative\n",
              "2  For the last quarter of 2010 , Componenta 's n...  positive\n",
              "3  According to the Finnish-Russian Chamber of Co...   neutral\n",
              "4  The Swedish buyout firm has sold its remaining...   neutral"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "old_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFcx8w552zfk",
        "outputId": "52066549-0b89-4e58-b63f-4d5a1fde447c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5842 entries, 0 to 5841\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   Sentence   5842 non-null   object\n",
            " 1   Sentiment  5842 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 91.4+ KB\n"
          ]
        }
      ],
      "source": [
        "old_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHW8QKObuQCu",
        "outputId": "01b3e843-7dd4-4f51-b155-6b96d0c57c41"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c372ad67-d26f-4076-8ded-57be1be53b25\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet_ID</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2401</th>\n",
              "      <td>Borderlands</td>\n",
              "      <td>Positive</td>\n",
              "      <td>im getting on borderlands and i will murder yo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2401</th>\n",
              "      <td>Borderlands</td>\n",
              "      <td>Positive</td>\n",
              "      <td>I am coming to the borders and I will kill you...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2401</th>\n",
              "      <td>Borderlands</td>\n",
              "      <td>Positive</td>\n",
              "      <td>im getting on borderlands and i will kill you ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2401</th>\n",
              "      <td>Borderlands</td>\n",
              "      <td>Positive</td>\n",
              "      <td>im coming on borderlands and i will murder you...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2401</th>\n",
              "      <td>Borderlands</td>\n",
              "      <td>Positive</td>\n",
              "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c372ad67-d26f-4076-8ded-57be1be53b25')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c372ad67-d26f-4076-8ded-57be1be53b25 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c372ad67-d26f-4076-8ded-57be1be53b25');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         Tweet_ID Sentiment                                           Sentence\n",
              "2401  Borderlands  Positive  im getting on borderlands and i will murder yo...\n",
              "2401  Borderlands  Positive  I am coming to the borders and I will kill you...\n",
              "2401  Borderlands  Positive  im getting on borderlands and i will kill you ...\n",
              "2401  Borderlands  Positive  im coming on borderlands and i will murder you...\n",
              "2401  Borderlands  Positive  im getting on borderlands 2 and i will murder ..."
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "column_names =['Tweet_ID','Sentiment','Sentence']\n",
        "Path = \"/content/drive/MyDrive/AI-projects/SentimentAnalysis/training.csv\"\n",
        "New_df = pd.read_csv(Path,sep=',',names = column_names )\n",
        "New_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIfo3BiIuQXp",
        "outputId": "a25ad39a-9056-44a8-ba05-edcfcd60bc59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Negative      22542\n",
              "Positive      20832\n",
              "Neutral       18318\n",
              "Irrelevant    12990\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "New_df[\"Sentiment\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvaMWIHuuRnu"
      },
      "outputs": [],
      "source": [
        "\n",
        "New_df = New_df[(New_df[\"Sentiment\"] == \"Negative\") | (New_df[\"Sentiment\"] == \"Positive\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA1Yhiw-uQbV",
        "outputId": "2834edcb-58b0-4368-8bac-95d5b8f9a044"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Negative    22542\n",
              "Positive    20832\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "New_df[\"Sentiment\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-zXGMaU3uEH",
        "outputId": "a3cb6cae-a71c-4809-9412-ae95da32f45a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Tweet_ID', 'Sentiment', 'Sentence'], dtype='object')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "New_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DedbdfPC4U5O"
      },
      "outputs": [],
      "source": [
        "\n",
        "New_df = New_df.drop(\"Tweet_ID\", axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrBf4l0T6lMC"
      },
      "outputs": [],
      "source": [
        "merged_df = pd.concat([old_df, New_df], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nb8X7EQj7OlN"
      },
      "outputs": [],
      "source": [
        "\n",
        "def modify_value(value):\n",
        "    if value == 'negative':\n",
        "        return 'Negative'\n",
        "    elif value == \"positive\":\n",
        "        return \"Positive\"\n",
        "\n",
        "    else:\n",
        "        return value\n",
        "\n",
        "# Apply the modify_value function to the 'name' column\n",
        "merged_df['Sentiment'] =merged_df['Sentiment'].apply(modify_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oerNRi0Q7Odl",
        "outputId": "7bd0fe6c-f2b6-40d3-e914-1c5e9e1d2e2c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Negative    23402\n",
              "Positive    22684\n",
              "neutral      3130\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "merged_df[\"Sentiment\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORLd-lfF7OpJ"
      },
      "outputs": [],
      "source": [
        "df = merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfO-XNsK-_jL"
      },
      "outputs": [],
      "source": [
        "df.dropna(axis = 0 , inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQjQ8brV-ZGH",
        "outputId": "4bca61d2-4683-44d4-c672-711536bcf22b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sentence     0\n",
              "Sentiment    0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIc124PvBIXb",
        "outputId": "2e79a968-04bb-4e7a-ab14-2948bd78c68f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mx0_vI5uU2Kj",
        "outputId": "acfcd686-e87a-4d76-c67d-dc9691056307"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 48855 entries, 0 to 9200\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   Sentence   48855 non-null  object\n",
            " 1   Sentiment  48855 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 1.1+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6Hw-_OpNqR6"
      },
      "outputs": [],
      "source": [
        "df.to_csv('AllData.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F76q3G991c_J"
      },
      "source": [
        "# DATA Preprosseing\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDXLBVsMAhSi"
      },
      "source": [
        "### If we remove all the stop words we will make a wrong meaning in some cases ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfXze6rM-IIF"
      },
      "outputs": [],
      "source": [
        "# Define stop words with our data conditions\n",
        "stop_words = [word for word in stopwords.words('english') if \"n't\" not in word and word not in ('not', 'no')]\n",
        "\n",
        "def preprocessing_text(text):\n",
        "    # Tokenize words in text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove any non-alphabetic characters and convert to lowercase\n",
        "    words = [re.sub('[^A-Za-z]+', '', word).lower() for word in words]\n",
        "\n",
        "    # Filter out stop words\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Lemmatize words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    # Join words into text again\n",
        "    processed_text = ' '.join(words)\n",
        "\n",
        "    return processed_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlRCUEOtTAcZ"
      },
      "outputs": [],
      "source": [
        "df['Sentence'] = df['Sentence'].apply(preprocessing_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CDUeK_hvbNw"
      },
      "source": [
        "## note : what is the difference between Lemmatiza and stemming ?\n",
        "Word Lemmatization:\n",
        "\n",
        "Word lemmatization is the process of reducing words to their base or dictionary form, called the lemma. The lemmatized word represents the canonical, or normalized, form of the word. Lemmatization takes into account the word's part of speech to ensure accurate transformation.\n",
        "\n",
        "Example of Lemmatization:\n",
        "\n",
        "makefile\n",
        "Copy code\n",
        "Word: running\n",
        "Lemma: run\n",
        "\n",
        "Word: ate\n",
        "Lemma: eat\n",
        "\n",
        "Word: better\n",
        "Lemma: good\n",
        "\n",
        "Word: dogs\n",
        "Lemma: dog\n",
        "In **lemmatization**, **the goal is to obtain a meaningful base form of the word**. This can be useful in various NLP tasks, such as information retrieval, text classification, and sentiment analysis. Lemmatization aims to maintain the integrity of the word by ensuring it belongs to the correct word class and retains its semantic meaning.\n",
        "\n",
        "Word Stemming:\n",
        "\n",
        "Word stemming, on the other hand, is the process of reducing words to their root or base form by removing suffixes or prefixes. The resulting stem may not always be a valid word or have semantic meaning.\n",
        "\n",
        "Example of Stemming:\n",
        "\n",
        "makefile\n",
        "Copy code\n",
        "Word: running\n",
        "Stem: run\n",
        "\n",
        "Word: ate\n",
        "Stem: at\n",
        "\n",
        "Word: better\n",
        "Stem: bet\n",
        "\n",
        "Word: dogs\n",
        "Stem: dog\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LJax3-tAhSa"
      },
      "source": [
        "## Helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVxo0ao2AhSe"
      },
      "outputs": [],
      "source": [
        "# mapping to the sentiment column\n",
        "\n",
        "dicto = {'Positive': 1, 'neutral': 0 , 'Negative': -1}\n",
        "\n",
        "df['Sentiment'] = df['Sentiment'].map(dicto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwDFQHQrT4el",
        "outputId": "98fdfcbb-c930-4331-a888-f02d84873880"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    1\n",
              "1   -1\n",
              "2    1\n",
              "3    0\n",
              "4    0\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.Sentiment.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEJd4EqVVQXL",
        "outputId": "bd57790a-2459-43d2-e1e8-1a2728f1a06c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 48855 entries, 0 to 9200\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   Sentence   48855 non-null  object\n",
            " 1   Sentiment  48855 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 1.1+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJE4PqWWKr5Q"
      },
      "outputs": [],
      "source": [
        "def ML_classical(df, bow=False, TFIDF=False,\n",
        "                 model=linear_model.LogisticRegression(solver='liblinear')):\n",
        "\n",
        "    df['folds'] = -1  # Add a column to split data later\n",
        "    df = df.sample(frac=1).reset_index(drop=True)  # Shuffle data and reset index\n",
        "\n",
        "    # Initiate StratifiedKFold object\n",
        "    np.random.seed(0)\n",
        "    n_splits = 5\n",
        "    kf = StratifiedKFold(n_splits=n_splits)\n",
        "\n",
        "    for f, (train, val) in enumerate(kf.split(X=df, y=df.Sentiment)):\n",
        "        df.loc[val, 'folds'] = f  # Assign each row to its validation set number\n",
        "\n",
        "    if bow:\n",
        "        vectorizer = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
        "    elif TFIDF:\n",
        "        vectorizer = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
        "\n",
        "    true_labels = []  # Store true labels for all folds\n",
        "    predicted_labels = []  # Store predicted labels for all folds\n",
        "\n",
        "    for fold in range(n_splits):\n",
        "        train_df = df[df.folds != fold].reset_index(drop=True)\n",
        "        test_df = df[df.folds == fold].reset_index(drop=True)\n",
        "\n",
        "        vectorizer.fit(train_df.Sentence)\n",
        "        x_train = vectorizer.transform(train_df.Sentence)\n",
        "        x_test = vectorizer.transform(test_df.Sentence)\n",
        "\n",
        "        model.fit(x_train, train_df.Sentiment)\n",
        "        preds = model.predict(x_test)\n",
        "\n",
        "        true_labels.extend(test_df.Sentiment)\n",
        "        predicted_labels.extend(preds)\n",
        "\n",
        "    accuracy_precision = precision_score(true_labels, predicted_labels, average='macro')\n",
        "    accuracy_recall = recall_score(true_labels, predicted_labels, average='macro')\n",
        "    accuracy_f1_score = f1_score(true_labels, predicted_labels, average='macro')\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "\n",
        "    joblib.dump(model, dir + model.__class__.__name__ + '.h5')\n",
        "    print(\"Accuracy: \", accuracy  )\n",
        "    print(\"Precision: \", accuracy_precision  )\n",
        "    print(\"Recall: \", accuracy_recall  )\n",
        "    print(\"F1-Score: \", accuracy_f1_score )\n",
        "    print(classification_report(true_labels, predicted_labels))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKtji1XwmBdt"
      },
      "source": [
        "3. TF-IDF Encoding:\n",
        "Term Frequency â€” Inverse Document Frequency\n",
        "\n",
        "Term Frequency: Is the occurrence of the current word in the current sentence w.r.t the total number of words in the current sentence.\n",
        "\n",
        "Inverse Data Frequency: Log of Total number of words in the whole data corpus w.r.t the total number of sentences containing the current word.\n",
        "![Tfidf.png](data:image/png;base64,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)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJl4l_xGPozU",
        "outputId": "45790c12-1d0b-473d-86d0-b688083421cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-1    23218\n",
              " 1    22507\n",
              " 0     3130\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_counts = df['Sentiment'].value_counts()\n",
        "label_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_XvWeAmNmPS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBWOtrf-UOGE"
      },
      "source": [
        "# Undersampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zA-2iI8UTZW",
        "outputId": "cce3a36c-e37b-400c-e89d-8b936bf2ed62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                               Sentence  Sentiment\n",
            "0                      esi low      bk real possibility         -1\n",
            "1     shell   billion bg deal meet shareholder skept...         -1\n",
            "2     ssh communication security corp stock exchange...         -1\n",
            "3      sap q disappoints  software license  real pro...         -1\n",
            "4      aapl afternoon selloff usual brutal  get read...         -1\n",
            "...                                                 ...        ...\n",
            "2575  operating profit net sale threemonth period in...          1\n",
            "2576  google  inc completes acquisition icoa  inc ht...          1\n",
            "2577  loaded  bsx yesterday  looking good  still thi...          1\n",
            "2578  nokian tyre prof high safety excellently impor...          1\n",
            "2579         ftse edge investor cheer kingfisher result          1\n",
            "\n",
            "[2580 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "rus = RandomUnderSampler()\n",
        "\n",
        "X_resampled, y_resampled = rus.fit_resample(df.Sentence.values.reshape(-1, 1), df.Sentiment)\n",
        "\n",
        "# Create a DataFrame from the resampled data\n",
        "df_resampled = pd.DataFrame({'Sentence': X_resampled.flatten(), 'Sentiment': y_resampled})\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "print(df_resampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0HqdYvPVN-a",
        "outputId": "f6a9529e-d06f-4bf2-d656-fe9426e25a1b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-1    860\n",
              " 0    860\n",
              " 1    860\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_resampled['Sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99_lxPWs1ibp"
      },
      "source": [
        "# Data analisis and viualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPogohxP21Pr"
      },
      "outputs": [],
      "source": [
        "# cheek if there is null values\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQqVaaJpLhFX",
        "outputId": "daa8bbff-c6ea-4763-d9cd-3bdc004286f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    1\n",
              "1   -1\n",
              "2    1\n",
              "3    0\n",
              "4    0\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"Sentiment\"].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9Qf2PIuAhSd",
        "outputId": "028df5e6-d7d3-49c0-b578-8a9cc0cf3328"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 1, -1,  0])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unique_Labels = df[\"Sentiment\"].unique()\n",
        "unique_Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "6I5hFyfn5yGV",
        "outputId": "362fb1c7-c919-4d9a-cff0-b6df34a19b5d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"7fb713bc-b803-4069-b6e8-c405c97c28dd\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"7fb713bc-b803-4069-b6e8-c405c97c28dd\")) {                    Plotly.newPlot(                        \"7fb713bc-b803-4069-b6e8-c405c97c28dd\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Sentiment=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"auto\",\"x\":[0,1,-1],\"xaxis\":\"x\",\"y\":[3130,1852,860],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Sentiment\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Count\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Sentiment Distribution\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('7fb713bc-b803-4069-b6e8-c405c97c28dd');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "sentiment_counts = df['Sentiment'].value_counts().reset_index()\n",
        "sentiment_counts.columns = ['Sentiment', 'Count']\n",
        "fig = px.bar(sentiment_counts, x='Sentiment', y='Count', title='Sentiment Distribution')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9HauPo3VE1n"
      },
      "outputs": [],
      "source": [
        "sentiment_counts = df['Sentiment'].value_counts().reset_index()\n",
        "sentiment_counts.columns = ['Sentiment', 'Count']\n",
        "fig = px.bar(sentiment_counts, x='Sentiment', y='Count', title='Sentiment Distribution')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkfFdsQec1zQ"
      },
      "outputs": [],
      "source": [
        "# # Create a dictionary from the DataFrame columns\n",
        "# word_counts = dict(zip(tbl[\"Word\"], tbl[\"Count\"]))\n",
        "\n",
        "# # Create a WordCloud object with desired configurations\n",
        "# wordcloud = WordCloud(background_color=\"white\", colormap=\"seismic\")\n",
        "\n",
        "# # Generate the word cloud using the word counts\n",
        "# wordcloud.generate_from_frequencies(word_counts)\n",
        "\n",
        "# # Plot the word cloud with colored text\n",
        "# plt.figure(figsize=(10, 8))\n",
        "# plt.imshow(wordcloud, interpolation=\"none\")\n",
        "# plt.savefig(\"wordcloud.png\", dpi=300)\n",
        "# plt.axis(\"off\")\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ao2HhX9Gh_Wi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "all_text = ' '.join(df[\"Sentence\"])\n",
        "\n",
        "# Create a word cloud\n",
        "wordcloud = WordCloud(background_color=\"white\", colormap=\"seismic\")\n",
        "\n",
        "wordcloud.generate(all_text)\n",
        "\n",
        "# Plot the word cloud with colored text\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.imshow(wordcloud, interpolation=\"none\")\n",
        "plt.savefig(\"wordcloud.png\", dpi=300)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9261fQCu4pxA"
      },
      "source": [
        "\n",
        "# Using Classical machine Learnig models (SVM and NaiveBayes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3YDbGhc3Gh3"
      },
      "source": [
        "## NaiveBayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRfQjaALpxLy"
      },
      "source": [
        "why log in tf-idf\n",
        "[logarithm graph](https://en.wikipedia.org/wiki/Logarithm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXQJWxKyAhSg"
      },
      "source": [
        "### Using NaiveBayes and  BOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwMDyYMBTXux",
        "outputId": "1b077a0e-db15-4188-b06a-4adaffec40de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(48855, 2)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4LLQnmmAhSg",
        "outputId": "9a6c7cda-f3ef-4ea5-f637-7f5b6bd9fa51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.84269777914236\n",
            "Precision:  0.7804939863259109\n",
            "Recall:  0.8558344477017213\n",
            "F1-Score:  0.808376556039803\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.86      0.88      0.87     23218\n",
            "           0       0.60      0.89      0.72      3130\n",
            "           1       0.88      0.80      0.84     22507\n",
            "\n",
            "    accuracy                           0.84     48855\n",
            "   macro avg       0.78      0.86      0.81     48855\n",
            "weighted avg       0.85      0.84      0.84     48855\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Lets try with NaiveBayes model\n",
        "\n",
        "ML_classical(df,bow = True,model = MultinomialNB()) # multiclassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jGkspEbAhSg"
      },
      "source": [
        "### Using Naive Bayes and TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuvMNcNhAhSh",
        "outputId": "08b90eea-97b4-43a3-9f20-4a0af0732e92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.8402210623272951\n",
            "Precision:  0.7883815436786001\n",
            "Recall:  0.7528179677304317\n",
            "F1-Score:  0.7679410266602374\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.84      0.90      0.87     23218\n",
            "           0       0.66      0.54      0.59      3130\n",
            "           1       0.86      0.82      0.84     22507\n",
            "\n",
            "    accuracy                           0.84     48855\n",
            "   macro avg       0.79      0.75      0.77     48855\n",
            "weighted avg       0.84      0.84      0.84     48855\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Now lets try with TF-IDF vectorizer instead of bag of words to MultinomialNB().\n",
        "\n",
        "ML_classical(df, model= MultinomialNB(),TFIDF=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hvkUYDU2KwP"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrp5_aODAhSf"
      },
      "source": [
        "### using SVM (rbf) and BOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BK1teKFRAhSf",
        "outputId": "36fe1d46-7fae-41fd-f7d0-8dc55d8d724f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.8942585201105312\n",
            "Precision:  0.8322593834116012\n",
            "Recall:  0.8291196878561768\n",
            "F1-Score:  0.83054274927016\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.92      0.90      0.91     23218\n",
            "           0       0.68      0.67      0.67      3130\n",
            "           1       0.89      0.92      0.91     22507\n",
            "\n",
            "    accuracy                           0.89     48855\n",
            "   macro avg       0.83      0.83      0.83     48855\n",
            "weighted avg       0.89      0.89      0.89     48855\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ML_classical(df, bow =True,model=svm.SVC(kernel='rbf'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8x4g0gdbdLI"
      },
      "source": [
        "### SVM and TFIF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diemMDEybc8x",
        "outputId": "b43c0676-aa15-40b7-9ff6-11daaab1c6fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.9152594412035615\n",
            "Precision:  0.8487344735002135\n",
            "Recall:  0.8462714269200414\n",
            "F1-Score:  0.8473929746906051\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.94      0.92      0.93     23218\n",
            "           0       0.68      0.68      0.68      3130\n",
            "           1       0.92      0.94      0.93     22507\n",
            "\n",
            "    accuracy                           0.92     48855\n",
            "   macro avg       0.85      0.85      0.85     48855\n",
            "weighted avg       0.92      0.92      0.92     48855\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ML_classical(df, TFIDF =True,model=svm.SVC(kernel='rbf'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzHclEIaQOh3"
      },
      "source": [
        "Precision: **Precision measures the accuracy of the positive predictions made by a model**. It is the ratio of true positives (the number of correctly predicted positive instances) to the sum of true positives and false positives (instances predicted as positive but are actually negative). In simple terms, precision answers the question: \"Of all the instances predicted as positive, how many were actually positive?\" A higher precision indicates a lower rate of false positives.\n",
        "\n",
        "Recall: Recall, also known as sensitivity or true positive rate, measures the model's ability to identify all the positive instances correctly. It is the ratio of true positives to the sum of true positives and false negatives (instances predicted as negative but are actually positive). In simple terms, recall answers the question: \"Of all the actual positive instances, how many were correctly predicted as positive?\" A higher recall indicates a lower rate of false negatives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVKvRBY1XkiE"
      },
      "source": [
        "# RandomForest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geR0LJWJYaUD"
      },
      "source": [
        "## RandomForest with TFIF.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4EexcZRXqh2",
        "outputId": "7dcba624-df76-40e7-851c-992fa538acd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.899334766144714\n",
            "Precision:  0.8242834763213498\n",
            "Recall:  0.8240235571613436\n",
            "F1-Score:  0.8241351728506641\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.92      0.91      0.92     23218\n",
            "           0       0.64      0.64      0.64      3130\n",
            "           1       0.91      0.92      0.92     22507\n",
            "\n",
            "    accuracy                           0.90     48855\n",
            "   macro avg       0.82      0.82      0.82     48855\n",
            "weighted avg       0.90      0.90      0.90     48855\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ML_classical(df, TFIDF = True ,model = RandomForestClassifier() )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZdRLgrzcBoE"
      },
      "source": [
        "## RandomForest with BOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnB5xks3aymG"
      },
      "outputs": [],
      "source": [
        "ML_classical(df, bow = True ,model = RandomForestClassifier() )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dFdZLZuXq7Q"
      },
      "source": [
        "# XGboast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b1ZJdoFdQX5"
      },
      "source": [
        "## XGboast with TFIDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH6KikMKBuzJ",
        "outputId": "75212ab1-9cb7-475f-d582-56ec6584aa36"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sentence     0\n",
              "Sentiment    0\n",
              "folds        0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9oCCrpBCXwXb",
        "outputId": "11a89dc6-4f2f-491a-e514-2080234808c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7356053628083103\n",
            "Precision:  0.72048226968999\n",
            "Recall:  0.6243887522197297\n",
            "F1-Score:  0.6523203118038595\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.81      0.71      0.75     23218\n",
            "           0       0.67      0.35      0.46      3130\n",
            "           1       0.69      0.82      0.75     22507\n",
            "\n",
            "    accuracy                           0.74     48855\n",
            "   macro avg       0.72      0.62      0.65     48855\n",
            "weighted avg       0.74      0.74      0.73     48855\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ML_classical(df , TFIDF = True , model = GradientBoostingClassifier())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmjE-aA-dZxy"
      },
      "source": [
        "## XGbost with BOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "syGcv3KzXweM",
        "outputId": "5dcd9105-0648-40f2-c4f3-9ece032cecf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7339269266195886\n",
            "Precision:  0.7185625865784523\n",
            "Recall:  0.6232741648334673\n",
            "F1-Score:  0.6511070423165811\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.80      0.70      0.75     23218\n",
            "           0       0.67      0.35      0.46      3130\n",
            "           1       0.69      0.82      0.75     22507\n",
            "\n",
            "    accuracy                           0.73     48855\n",
            "   macro avg       0.72      0.62      0.65     48855\n",
            "weighted avg       0.74      0.73      0.73     48855\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ML_classical(df , bow = True , model = GradientBoostingClassifier())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLjf4dofXxBk"
      },
      "source": [
        "# Desion Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6jtddUHfVAY"
      },
      "source": [
        "## Desion Tree with TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KVZVMJsFX2qe",
        "outputId": "04aa0cac-8031-4c02-dfce-126429602e86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.8419199672500256\n",
            "Precision:  0.7584573326546904\n",
            "Recall:  0.7560362779190833\n",
            "F1-Score:  0.7572000098918569\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.87      0.86      0.86     23218\n",
            "           0       0.55      0.54      0.55      3130\n",
            "           1       0.86      0.87      0.86     22507\n",
            "\n",
            "    accuracy                           0.84     48855\n",
            "   macro avg       0.76      0.76      0.76     48855\n",
            "weighted avg       0.84      0.84      0.84     48855\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ML_classical(df , TFIDF = True ,model = tree.DecisionTreeClassifier())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQxm1d8rfizy"
      },
      "source": [
        "## desion tree with BOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oD-JNCXEfqWe",
        "outputId": "30c180ac-8e21-4438-a9ec-f52a1dc07acd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.8510694913519599\n",
            "Precision:  0.7733256858337337\n",
            "Recall:  0.769798956065845\n",
            "F1-Score:  0.771516644049164\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.87      0.86      0.87     23218\n",
            "           0       0.58      0.57      0.58      3130\n",
            "           1       0.87      0.88      0.87     22507\n",
            "\n",
            "    accuracy                           0.85     48855\n",
            "   macro avg       0.77      0.77      0.77     48855\n",
            "weighted avg       0.85      0.85      0.85     48855\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ML_classical(df , bow= True ,model = tree.DecisionTreeClassifier())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnqdG0VinmWM"
      },
      "source": [
        "# Tring to make optimization for XGboasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-paXFMOnv_-"
      },
      "outputs": [],
      "source": [
        "def ML_classical_opptmized(df, bow=False, TFIDF=False,\n",
        "                           model=linear_model.LogisticRegression(solver='liblinear')):\n",
        "    df['folds'] = -1\n",
        "    # Shuffle data and reset index\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    # Initiate StratifiedKFold object\n",
        "    np.random.seed(0)\n",
        "    n_splits = 3\n",
        "    kf = StratifiedKFold(n_splits=n_splits)\n",
        "\n",
        "    # Assign each row to its validation set number\n",
        "    for f, (train, val) in enumerate(kf.split(X=df, y=df.Sentiment)):\n",
        "        df.loc[val, 'folds'] = f\n",
        "\n",
        "    # Initialize vectorizer based on representation choice\n",
        "    if bow:\n",
        "        vectorizer = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
        "    elif TFIDF:\n",
        "        vectorizer = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
        "\n",
        "    true_labels = []  # Store true labels for all folds\n",
        "    predicted_labels = []  # Store predicted labels for all folds\n",
        "\n",
        "    # Perform cross-validation\n",
        "    for fold in range(n_splits):\n",
        "        train_df = df[df.folds != fold].reset_index(drop=True)\n",
        "        test_df = df[df.folds == fold].reset_index(drop=True)\n",
        "\n",
        "        vectorizer.fit(train_df.Sentence)\n",
        "        x_train = vectorizer.transform(train_df.Sentence)\n",
        "        x_test = vectorizer.transform(test_df.Sentence)\n",
        "\n",
        "        # Define the parameter grid\n",
        "        param_grid = {\n",
        "            'learning_rate': [0.1, 0.001],\n",
        "            'n_estimators': [100, 300],\n",
        "            'max_depth': [3, 5,]\n",
        "            # Add other hyperparameters and their ranges as needed\n",
        "        }\n",
        "\n",
        "        # Create the GridSearchCV object\n",
        "        grid_search = GridSearchCV(estimator=model, n_jobs=-1 , param_grid=param_grid, scoring='accuracy', cv=5)\n",
        "\n",
        "        # Fit the data to perform grid search\n",
        "        grid_search.fit(x_train, train_df.Sentiment)\n",
        "\n",
        "        # Use the best hyperparameters for training\n",
        "        best_model = grid_search.best_estimator_\n",
        "        best_model.fit(x_train, train_df.Sentiment)\n",
        "\n",
        "        preds = best_model.predict(x_test)\n",
        "\n",
        "        true_labels.extend(test_df.Sentiment)\n",
        "        predicted_labels.extend(preds)\n",
        "         # Compute evaluation metrics for the current fold\n",
        "        accuracy_precision = precision_score(test_df.Sentiment, preds, average='macro')\n",
        "        accuracy_recall = recall_score(test_df.Sentiment, preds, average='macro')\n",
        "        accuracy_f1_score = f1_score(test_df.Sentiment, preds, average='macro')\n",
        "        accuracy = accuracy_score(test_df.Sentiment, preds)\n",
        "\n",
        "        # Print evaluation metrics for the current fold\n",
        "        print(\"Fold:\", fold + 1)\n",
        "        print(\"Accuracy:\", accuracy)\n",
        "        print(\"Precision:\", accuracy_precision)\n",
        "        print(\"Recall:\", accuracy_recall)\n",
        "        print(\"F1-Score:\", accuracy_f1_score)\n",
        "        print(classification_report(test_df.Sentiment, preds))\n",
        "        print(\"-------------------------\")\n",
        "\n",
        "    # Compute evaluation metrics\n",
        "    accuracy_precision = precision_score(true_labels, predicted_labels, average='macro')\n",
        "    accuracy_recall = recall_score(true_labels, predicted_labels, average='macro')\n",
        "    accuracy_f1_score = f1_score(true_labels, predicted_labels, average='macro')\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "\n",
        "\n",
        "    # Print evaluation metrics\n",
        "    print(\"Accuracy:\" ,accuracy )\n",
        "    print(\"Precision: \", accuracy_precision)\n",
        "    print(\"Recall: \", accuracy_recall)\n",
        "    print(\"F1-Score: \", accuracy_f1_score)\n",
        "    print(classification_report(true_labels, predicted_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fj3KydpMnwDR",
        "outputId": "b5489068-3cdb-49e2-8f25-7a7f0c4d13a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold: 1\n",
            "Accuracy: 0.696611909650924\n",
            "Precision: 0.6071485874734973\n",
            "Recall: 0.5605430936789889\n",
            "F1-Score: 0.5652810189223788\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.38      0.16      0.23       287\n",
            "           0       0.71      0.87      0.79      1043\n",
            "           1       0.72      0.65      0.68       618\n",
            "\n",
            "    accuracy                           0.70      1948\n",
            "   macro avg       0.61      0.56      0.57      1948\n",
            "weighted avg       0.67      0.70      0.67      1948\n",
            "\n",
            "-------------------------\n",
            "Fold: 2\n",
            "Accuracy: 0.7108371854134566\n",
            "Precision: 0.6207992356582063\n",
            "Recall: 0.5686139894908687\n",
            "F1-Score: 0.5732983627790977\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.39      0.15      0.22       286\n",
            "           0       0.72      0.90      0.80      1044\n",
            "           1       0.75      0.66      0.70       617\n",
            "\n",
            "    accuracy                           0.71      1947\n",
            "   macro avg       0.62      0.57      0.57      1947\n",
            "weighted avg       0.68      0.71      0.68      1947\n",
            "\n",
            "-------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-32e5493c2905>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mML_classical_opptmized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mTFIDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-f6dbb853885a>\u001b[0m in \u001b[0;36mML_classical_opptmized\u001b[0;34m(df, bow, TFIDF, model)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Fit the data to perform grid search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Use the best hyperparameters for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    565\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    566\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    451\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "ML_classical_opptmized(df , TFIDF = True , model= GradientBoostingClassifier())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6YhbeijdDBF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2efmRvXdDXn"
      },
      "source": [
        "# Ensemble learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdR_DD9tdOQL"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n",
        "def ML_classical_ensemble(df, bow=False, TFIDF=False):\n",
        "    df['folds'] = -1  # Add a column to split data later\n",
        "    df = df.sample(frac=1).reset_index(drop=True)  # Shuffle data and reset index\n",
        "\n",
        "    # Initiate StratifiedKFold object\n",
        "    np.random.seed(0)\n",
        "    n_splits = 5\n",
        "    kf = StratifiedKFold(n_splits=n_splits)\n",
        "\n",
        "    for f, (train, val) in enumerate(kf.split(X=df, y=df.Sentiment)):\n",
        "        df.loc[val, 'folds'] = f  # Assign each row to its validation set number\n",
        "\n",
        "    if bow:\n",
        "        vectorizer = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
        "    elif TFIDF:\n",
        "        vectorizer = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
        "\n",
        "    models = [\n",
        "    ('logistic_regression', linear_model.LogisticRegression(solver='liblinear')),\n",
        "    ('svm', svm.SVC(kernel='linear', probability=True)),\n",
        "    ('random_forest', RandomForestClassifier()),\n",
        "    ('naive_bayes', GaussianNB()),\n",
        "    ('xgboost', XGBClassifier()),\n",
        "    ('decision_tree', DecisionTreeClassifier())\n",
        "                          ]\n",
        "\n",
        "    voting_model = VotingClassifier(models, voting='soft')\n",
        "\n",
        "    true_labels = []  # Store true labels for all folds\n",
        "    predicted_labels = []  # Store predicted labels for all folds\n",
        "\n",
        "    for fold in range(n_splits):\n",
        "        train_df = df[df.folds != fold].reset_index(drop=True)\n",
        "        test_df = df[df.folds == fold].reset_index(drop=True)\n",
        "\n",
        "        vectorizer.fit(train_df.Sentence)\n",
        "        x_train = vectorizer.transform(train_df.Sentence)\n",
        "        x_test = vectorizer.transform(test_df.Sentence)\n",
        "\n",
        "        voting_model.fit(x_train, train_df.Sentiment)\n",
        "        preds = voting_model.predict(x_test)\n",
        "\n",
        "        true_labels.extend(test_df.Sentiment)\n",
        "        predicted_labels.extend(preds)\n",
        "\n",
        "    accuracy_precision = precision_score(true_labels, predicted_labels, average='macro')\n",
        "    accuracy_recall = recall_score(true_labels, predicted_labels, average='macro')\n",
        "    accuracy_f1_score = f1_score(true_labels, predicted_labels, average='macro')\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "\n",
        "    joblib.dump(voting_model, dir + 'EnsembleModel.h5')\n",
        "    print(\"Accuracy: \", accuracy)\n",
        "    print(\"Precision: \", accuracy_precision)\n",
        "    print(\"Recall: \", accuracy_recall)\n",
        "    print(\"F1-Score: \", accuracy_f1_score)\n",
        "    print(classification_report(true_labels, predicted_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2qteH_YdTox"
      },
      "outputs": [],
      "source": [
        "ML_classical_ensemble(df , TFIDF = True  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TXwKifbg8Au"
      },
      "source": [
        "# perform grid search to models to get best accuracy and best hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdmFtmCfhA8u"
      },
      "outputs": [],
      "source": [
        "\n",
        "def ML_classical_GS(df, bow=False, TFIDF=False):\n",
        "    df['folds'] = -1  # Add a column to split data later\n",
        "    df = df.sample(frac=1).reset_index(drop=True)  # Shuffle data and reset index\n",
        "\n",
        "    # Initiate StratifiedKFold object\n",
        "    np.random.seed(0)\n",
        "    n_splits = 5\n",
        "    kf = StratifiedKFold(n_splits=n_splits)\n",
        "\n",
        "    for f, (train, val) in enumerate(kf.split(X=df, y=df.Sentiment)):\n",
        "        df.loc[val, 'folds'] = f  # Assign each row to its validation set number\n",
        "\n",
        "    if bow:\n",
        "        vectorizer = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
        "    elif TFIDF:\n",
        "        vectorizer = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
        "\n",
        "    models = [\n",
        "        ('logistic_regression', LogisticRegression(solver='liblinear' , max_iter=1000 )),\n",
        "        ('svm', svm.SVC(kernel='linear', probability=True)),\n",
        "        ('random_forest', RandomForestClassifier()),\n",
        "        ('gradient_boosting', GradientBoostingClassifier()),\n",
        "    ]\n",
        "\n",
        "    best_model = None\n",
        "    best_accuracy = 0.0\n",
        "\n",
        "    for name, model in models:\n",
        "        param_grid = get_param_grid(name)  # Get the hyperparameter grid for the specific model\n",
        "\n",
        "        # Perform GridSearchCV with the given model and hyperparameter grid\n",
        "        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=n_splits)\n",
        "\n",
        "        for fold in range(n_splits):\n",
        "            # Fit the vectorizer on the training data\n",
        "            vectorizer.fit(df.loc[df.folds != fold, 'Sentence'])\n",
        "\n",
        "            # Transform the training and test data\n",
        "            x_train = vectorizer.transform(df.loc[df.folds != fold, 'Sentence'])\n",
        "            x_test = vectorizer.transform(df.loc[df.folds == fold, 'Sentence'])\n",
        "\n",
        "            grid_search.fit(x_train, df.loc[df.folds != fold, 'Sentiment'])\n",
        "\n",
        "            # Retrieve the best model and its accuracy\n",
        "            best_estimator = grid_search.best_estimator_\n",
        "            accuracy = grid_search.best_score_\n",
        "\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_model = best_estimator\n",
        "\n",
        "    joblib.dump(best_model, dir + 'BestModel.h5')\n",
        "    print(\"Best Model Accuracy: \", best_accuracy)\n",
        "\n",
        "def get_param_grid(model_name):\n",
        "    if model_name == 'logistic_regression':\n",
        "        param_grid = {\n",
        "            'C': [0.1, 1.0, 10.0],\n",
        "            'penalty': ['l1', 'l2']\n",
        "        }\n",
        "    elif model_name == 'svm':\n",
        "        param_grid = {\n",
        "            'C': [0.1, 1.0, 10.0],\n",
        "            'kernel': ['linear', 'rbf']\n",
        "        }\n",
        "    elif model_name == 'random_forest':\n",
        "        param_grid = {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [None, 5, 10]\n",
        "        }\n",
        "    elif model_name == 'gradient_boosting':\n",
        "        param_grid = {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'learning_rate': [0.1, 0.01, 0.001],\n",
        "            'max_depth': [3, 5, 10]\n",
        "        }\n",
        "    else:\n",
        "        param_grid = {}\n",
        "    return param_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3igf2gddT0P",
        "outputId": "6a499470-bdd0-4d37-bb8a-46c960748d4f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "ML_classical_GS(df , TFIDF = True )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m-n8uqcAhS8"
      },
      "source": [
        "# Sequence Models and Transfer Learnig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJQmWdntAhS9"
      },
      "source": [
        "### text preprosessing for deep learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saGJvnBTAhS9"
      },
      "outputs": [],
      "source": [
        "# loading pretrained word2vec embedding 300D \"GoogleNews Word2Vec embeddings\" =\n",
        "# loading the words (vectors) that have been trained with word2vec\n",
        "\n",
        "word2vec_pretrained = KeyedVectors.load_word2vec_format(\"/content/drive/MyDrive/AI-projects/SentimentAnalysis/GoogleNews-vectors-negative300.bin.gz\",binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1L-xSXoFctAB"
      },
      "outputs": [],
      "source": [
        "word2vec_pretrained_dict = dict(zip(word2vec_pretrained.key_to_index.keys(),word2vec_pretrained.vectors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJddXUmqctD6"
      },
      "outputs": [],
      "source": [
        "Example_word = \"Apple\"\n",
        "Example_vector = word2vec_pretrained[Example_word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WDYDViCctH7",
        "outputId": "9cc08da5-794f-463d-95b2-7213e38f1403"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Apple vector = [-1.74804688e-01  3.00292969e-02 -2.16796875e-01  1.56250000e-01\n",
            " -3.57421875e-01 -6.05468750e-02  1.36718750e-01  9.57031250e-02\n",
            "  3.17382812e-03 -4.29687500e-02 -3.30078125e-01  2.57812500e-01\n",
            "  2.51953125e-01 -2.77343750e-01 -6.98242188e-02 -2.95410156e-02\n",
            "  3.22265625e-01 -7.76367188e-02 -3.06396484e-02 -1.67968750e-01\n",
            " -5.76171875e-02  3.05175781e-02  5.52368164e-03 -1.26953125e-01\n",
            " -1.44042969e-02  1.75781250e-01  9.47265625e-02  3.16406250e-01\n",
            " -7.81250000e-03 -3.40270996e-03  3.63769531e-02  1.11816406e-01\n",
            " -1.24023438e-01  1.29882812e-01 -3.22265625e-02 -1.60156250e-01\n",
            "  7.56835938e-02  6.73828125e-02  4.08203125e-01  2.23632812e-01\n",
            "  1.60156250e-01  3.63769531e-02 -1.64062500e-01 -3.51562500e-01\n",
            "  4.49218750e-02  6.34765625e-02 -1.15234375e-01  3.12500000e-01\n",
            " -2.80761719e-02 -9.22851562e-02  5.98144531e-02  1.57470703e-02\n",
            " -1.15234375e-01  2.18750000e-01 -5.78613281e-02  2.07031250e-01\n",
            " -1.03515625e-01 -2.07031250e-01 -7.08007812e-02 -7.47070312e-02\n",
            "  1.35742188e-01  1.80664062e-01 -2.50000000e-01 -3.27148438e-02\n",
            " -9.76562500e-02 -7.81250000e-02  7.32421875e-03  2.47070312e-01\n",
            " -1.75781250e-01  1.21459961e-02 -2.49023438e-01 -5.61523438e-02\n",
            "  4.10156250e-02 -1.59179688e-01 -3.34472656e-02 -7.29370117e-03\n",
            " -1.11328125e-01 -2.03125000e-01  1.00585938e-01 -1.26953125e-01\n",
            " -6.93359375e-02 -1.02539062e-02 -1.81640625e-01 -1.54296875e-01\n",
            " -7.91015625e-02 -4.08203125e-01  3.22265625e-01  2.91015625e-01\n",
            " -2.69531250e-01 -1.61132812e-01 -2.92968750e-01  1.17675781e-01\n",
            " -1.64062500e-01 -1.21582031e-01  1.26953125e-01 -3.14453125e-01\n",
            " -2.66113281e-02  8.10546875e-02  1.18652344e-01  8.30078125e-02\n",
            "  3.07617188e-02 -7.71484375e-02 -2.08984375e-01  1.27929688e-01\n",
            "  5.88378906e-02 -1.55273438e-01 -6.98242188e-02  1.03027344e-01\n",
            "  4.68750000e-02 -4.57031250e-01 -3.61328125e-01 -4.99725342e-04\n",
            "  2.37304688e-01 -4.79125977e-03  1.39648438e-01 -5.78613281e-02\n",
            " -2.39257812e-01 -4.35546875e-01 -8.44726562e-02  3.44238281e-02\n",
            " -4.93164062e-02 -1.54296875e-01 -3.32031250e-01 -2.16796875e-01\n",
            "  1.65039062e-01 -1.12792969e-01 -1.45507812e-01  1.60156250e-01\n",
            " -3.59375000e-01  8.10546875e-02 -1.20605469e-01 -4.46777344e-02\n",
            " -2.25585938e-01 -5.66406250e-02 -7.91015625e-02  1.11694336e-02\n",
            "  2.20947266e-02 -2.28271484e-02 -5.56640625e-02 -1.66992188e-01\n",
            "  1.75781250e-02 -1.39648438e-01 -2.51953125e-01 -3.59375000e-01\n",
            "  2.20703125e-01 -5.34667969e-02  3.22265625e-01 -1.91406250e-01\n",
            " -5.74218750e-01 -1.58203125e-01 -5.85937500e-02 -2.17773438e-01\n",
            " -1.30859375e-01 -4.61425781e-02 -2.53906250e-01  3.61328125e-02\n",
            " -1.58203125e-01 -1.39648438e-01 -4.71191406e-02  2.44140625e-01\n",
            " -3.30078125e-01 -1.82617188e-01 -8.88671875e-02 -1.11694336e-02\n",
            " -9.71679688e-02 -1.52343750e-01  3.20312500e-01  2.14843750e-02\n",
            " -5.03540039e-03  6.39648438e-02 -9.37500000e-02 -1.69921875e-01\n",
            " -7.91015625e-02 -1.50390625e-01 -1.73828125e-01  1.05468750e-01\n",
            "  2.55859375e-01 -9.61914062e-02 -2.52685547e-02 -1.06933594e-01\n",
            " -2.41210938e-01 -8.20312500e-02  5.88378906e-02 -2.75390625e-01\n",
            "  2.21679688e-01  6.12792969e-02  1.86767578e-02  2.91015625e-01\n",
            " -8.64257812e-02 -6.93359375e-02  1.35498047e-02  1.76757812e-01\n",
            " -5.07812500e-02 -2.08984375e-01 -1.37695312e-01  1.46484375e-01\n",
            " -3.10546875e-01 -2.28515625e-01 -1.54296875e-01  3.73535156e-02\n",
            "  9.46044922e-03 -2.43164062e-01  1.40625000e-01  3.02734375e-01\n",
            " -2.31933594e-03  1.67968750e-01  1.33789062e-01 -1.10839844e-01\n",
            " -2.50000000e-01  2.42919922e-02  4.93164062e-02  1.84570312e-01\n",
            " -1.67236328e-02  9.27734375e-02 -1.72851562e-01 -4.00390625e-02\n",
            "  6.68945312e-02  1.25000000e-01 -2.12890625e-01 -3.78906250e-01\n",
            " -3.65234375e-01  3.67187500e-01  9.03320312e-02  2.31445312e-01\n",
            " -1.35742188e-01  1.17675781e-01 -4.68750000e-02  2.80761719e-02\n",
            "  1.63085938e-01  9.08203125e-02 -4.17968750e-01 -1.88476562e-01\n",
            " -2.29492188e-01 -3.69140625e-01  1.41601562e-01 -1.41601562e-02\n",
            "  1.48437500e-01 -1.83593750e-01  1.08886719e-01  8.00781250e-02\n",
            "  2.38281250e-01 -1.51977539e-02 -1.61132812e-02 -4.41406250e-01\n",
            "  4.41894531e-02 -1.80664062e-01 -1.89453125e-01 -3.44848633e-03\n",
            " -9.96093750e-02 -1.35742188e-01 -4.49218750e-01  2.10937500e-01\n",
            "  3.34472656e-02  1.66015625e-01  1.55273438e-01  2.00195312e-01\n",
            "  1.79687500e-01  5.37109375e-02 -1.93359375e-01  3.10546875e-01\n",
            "  2.94921875e-01  2.70996094e-02  2.51464844e-02  2.50000000e-01\n",
            " -5.78613281e-02  2.08007812e-01 -3.51562500e-01 -1.26953125e-01\n",
            "  1.02050781e-01 -2.87109375e-01  1.17187500e-01  1.41601562e-01\n",
            "  7.22656250e-02 -3.36914062e-02  2.13867188e-01 -3.54003906e-02\n",
            "  3.12500000e-01 -1.07421875e-01 -1.29882812e-01  2.66113281e-02\n",
            " -1.25976562e-01  3.26171875e-01  2.96630859e-02  3.02734375e-01\n",
            "  1.20117188e-01 -1.16210938e-01  5.49316406e-02  1.15356445e-02\n",
            "  6.25000000e-02  3.02734375e-01  1.34765625e-01 -9.22851562e-02\n",
            "  3.36914062e-02 -1.59179688e-01  3.45703125e-01 -6.73828125e-02\n",
            " -2.44140625e-01 -1.79443359e-02 -1.06445312e-01  2.57812500e-01]\n"
          ]
        }
      ],
      "source": [
        "print(Example_word,\"vector =\",Example_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NMJJCacgM4E",
        "outputId": "35f83722-a7f4-4b0d-814b-7336d6b9b456"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Example_vector.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPa4sheQAhS9",
        "outputId": "c1fcf104-7a2d-4f2b-e97d-e711cc6cecef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(word2vec_pretrained_dict.values())[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fw-5RTvTf7EX",
        "outputId": "3b7e92de-6fab-4777-adcd-4c35a173cbb2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.00836182, -0.11669922,  0.11035156,  0.07177734, -0.02087402,\n",
              "        0.09375   ,  0.07910156, -0.22265625, -0.00714111,  0.06445312,\n",
              "       -0.19628906, -0.05200195, -0.16503906,  0.0480957 , -0.18847656,\n",
              "        0.19921875, -0.05224609,  0.33398438, -0.01501465,  0.04125977,\n",
              "       -0.23242188, -0.04858398,  0.0098877 , -0.00338745, -0.15625   ,\n",
              "       -0.23730469, -0.1640625 ,  0.03039551, -0.17285156, -0.07666016,\n",
              "       -0.12988281,  0.04736328,  0.12695312, -0.10742188, -0.22363281,\n",
              "       -0.01330566, -0.09716797,  0.03808594,  0.13574219, -0.08154297,\n",
              "       -0.04882812, -0.01916504, -0.07714844,  0.20214844, -0.06884766,\n",
              "       -0.10498047, -0.00921631, -0.15136719, -0.0703125 ,  0.046875  ,\n",
              "       -0.10742188,  0.17285156, -0.01507568,  0.16796875, -0.0168457 ,\n",
              "        0.21386719, -0.01977539,  0.0859375 , -0.00662231, -0.10009766,\n",
              "       -0.15917969,  0.09082031, -0.08105469, -0.01507568, -0.07910156,\n",
              "       -0.29492188,  0.09667969, -0.13867188,  0.21289062,  0.07568359,\n",
              "        0.03979492,  0.15429688, -0.02880859, -0.00527954, -0.14746094,\n",
              "        0.11035156,  0.06787109, -0.1328125 , -0.05053711, -0.09375   ,\n",
              "       -0.05029297,  0.07861328,  0.06982422, -0.04467773,  0.0300293 ,\n",
              "        0.16699219, -0.13671875,  0.25390625,  0.14453125,  0.01965332,\n",
              "        0.0177002 , -0.06884766, -0.14355469, -0.03930664,  0.00683594,\n",
              "        0.1796875 ,  0.06640625, -0.00248718,  0.18359375,  0.20214844,\n",
              "       -0.234375  ,  0.1171875 , -0.07861328, -0.03735352,  0.18945312,\n",
              "        0.12988281, -0.10351562,  0.12060547, -0.05932617,  0.12988281,\n",
              "        0.03564453, -0.09814453,  0.34179688, -0.1796875 ,  0.05175781,\n",
              "        0.06152344, -0.03637695, -0.14648438,  0.0072937 , -0.19140625,\n",
              "        0.18457031, -0.03564453,  0.078125  ,  0.03369141,  0.10058594,\n",
              "       -0.04125977, -0.2578125 ,  0.00460815,  0.07470703, -0.12304688,\n",
              "       -0.19726562, -0.10302734,  0.00131226, -0.07373047, -0.11865234,\n",
              "        0.21875   , -0.18359375,  0.06738281,  0.20507812, -0.12207031,\n",
              "        0.27929688, -0.12890625, -0.28515625,  0.01708984, -0.02075195,\n",
              "       -0.14550781,  0.03833008,  0.17871094, -0.14257812, -0.1015625 ,\n",
              "        0.16503906, -0.28320312, -0.12109375,  0.08300781, -0.18945312,\n",
              "       -0.1328125 , -0.0612793 ,  0.19042969,  0.10498047, -0.29882812,\n",
              "        0.26757812,  0.11865234,  0.02905273,  0.18945312,  0.12304688,\n",
              "        0.09033203,  0.24804688,  0.20019531, -0.21679688,  0.11523438,\n",
              "       -0.11132812, -0.09960938,  0.13769531, -0.12597656, -0.12011719,\n",
              "        0.15039062,  0.078125  , -0.2578125 ,  0.10205078,  0.00201416,\n",
              "        0.04003906, -0.10449219,  0.13574219,  0.09375   ,  0.07763672,\n",
              "       -0.05053711,  0.02844238, -0.24609375, -0.00534058, -0.06884766,\n",
              "        0.16503906, -0.09521484,  0.05102539,  0.13378906,  0.18652344,\n",
              "        0.06738281,  0.05126953,  0.13378906, -0.07177734, -0.00964355,\n",
              "       -0.07958984,  0.0456543 , -0.2109375 , -0.10253906, -0.08886719,\n",
              "       -0.19824219, -0.10058594, -0.03173828, -0.21191406,  0.09667969,\n",
              "        0.14648438, -0.04638672, -0.1328125 , -0.03833008, -0.00604248,\n",
              "        0.09423828,  0.05029297, -0.06347656,  0.01147461,  0.00915527,\n",
              "        0.12451172,  0.18066406, -0.13476562,  0.10498047,  0.18554688,\n",
              "       -0.16796875,  0.02148438,  0.13671875, -0.00279236, -0.21777344,\n",
              "       -0.17773438, -0.06738281, -0.18261719,  0.0390625 ,  0.16796875,\n",
              "        0.07470703,  0.09912109,  0.125     ,  0.09472656, -0.02038574,\n",
              "        0.10742188,  0.00367737,  0.26171875,  0.19628906, -0.15625   ,\n",
              "        0.00430298, -0.18164062,  0.34375   , -0.08740234, -0.1640625 ,\n",
              "        0.08007812,  0.140625  ,  0.00878906,  0.11914062,  0.10498047,\n",
              "       -0.27148438, -0.12597656, -0.09472656,  0.12695312,  0.06396484,\n",
              "        0.22949219, -0.00854492,  0.20996094, -0.04589844,  0.01989746,\n",
              "       -0.10058594,  0.01623535,  0.12158203, -0.06787109, -0.09228516,\n",
              "       -0.2265625 , -0.09179688, -0.03149414, -0.11572266,  0.41796875,\n",
              "        0.0559082 , -0.41992188, -0.03442383, -0.13476562,  0.24511719,\n",
              "       -0.03710938,  0.06835938,  0.03662109, -0.10546875,  0.11083984,\n",
              "        0.00619507,  0.01153564,  0.13769531, -0.29492188, -0.015625  ,\n",
              "       -0.13671875,  0.15429688, -0.05322266,  0.06103516,  0.18164062,\n",
              "        0.03540039, -0.18066406, -0.07470703, -0.07861328, -0.01953125],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word2vec_pretrained_dict[\"ero\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2cCsbAlAhS9"
      },
      "outputs": [],
      "source": [
        "df['Sentence'] = df['Sentence'].apply(preprocessing_text)\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(df.Sentence, df.Sentiment, test_size = 0.18,\n",
        "                                                 random_state = 48, stratify= df.Sentiment, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cW2YPUIAhS-"
      },
      "outputs": [],
      "source": [
        "# One hot Encodring convert, to pass to word2vec\n",
        "y_train_enc = to_categorical.to_categorical(y_train, 3)\n",
        "y_test_enc = to_categorical.to_categorical(y_test, 3)\n",
        "# output_examples = [1,0,0],[0,1,0],[0,0,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSF-ilqaAhS-"
      },
      "outputs": [],
      "source": [
        "token = tf.keras.preprocessing.text.Tokenizer(num_words=None)\n",
        "\n",
        "token.fit_on_texts(list(X_train) + list(X_test)) # fits tokens on texts\n",
        "xtrain_seq = token.texts_to_sequences(X_train) # text to sequences converts the sentence words to number sequences\n",
        "xtest_seq = token.texts_to_sequences(X_test)\n",
        "\n",
        "#zero pad sequences\n",
        "xtrain_pad = pad_sequences(xtrain_seq,padding='post') # zero padding all sentences to have the same shape as the largest one\n",
        "xtest_pad = pad_sequences(xtest_seq,padding='post')\n",
        "\n",
        "word_index = token.word_index # returns the word index that have been tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oj15v5oaIei",
        "outputId": "be9af20e-4023-46d7-d9d7-082dd124ebb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eur': 1, 'company': 2, 'mn': 3, 'sale': 4, 'profit': 5, 'finnish': 6, 'share': 7, 'said': 8, 'net': 9, 'million': 10, 'year': 11, 'operating': 12, 'mln': 13, 'period': 14, 'group': 15, 'finland': 16, 'quarter': 17, 'market': 18, 'service': 19, 'euro': 20, 'new': 21, 'http': 22, 'business': 23, 'oyj': 24, 'first': 25, 'loss': 26, 'also': 27, 'today': 28, 'compared': 29, 'price': 30, 'product': 31, 'helsinki': 32, 'operation': 33, 'contract': 34, 'corresponding': 35, 'solution': 36, 'percent': 37, 'bank': 38, 'total': 39, 'per': 40, 'stock': 41, 'system': 42, 'result': 43, 'unit': 44, 'plant': 45, 'financial': 46, 'order': 47, 'not': 48, 'decreased': 49, 'investment': 50, 'nokia': 51, 'customer': 52, 'report': 53, 'technology': 54, 'hel': 55, 'corporation': 56, 'capital': 57, 'according': 58, 'pct': 59, 'production': 60, 'project': 61, 'u': 62, 'well': 63, 'increased': 64, 'month': 65, 'mobile': 66, 'increase': 67, 'value': 68, 'last': 69, 'earlier': 70, 'industry': 71, 'third': 72, 'eurm': 73, 'rose': 74, 'deal': 75, 'omx': 76, 'plc': 77, 'agreement': 78, 'usd': 79, 'part': 80, 'building': 81, 'fell': 82, 'area': 83, 'board': 84, 'would': 85, 'second': 86, 'construction': 87, 'ceo': 88, 'network': 89, 'billion': 90, 'buy': 91, 'half': 92, 'aapl': 93, 'expected': 94, 'earnings': 95, 'oy': 96, 'one': 97, 'revenue': 98, 'growth': 99, 'paper': 100, 'maker': 101, 'development': 102, 'long': 103, 'line': 104, 'time': 105, 'end': 106, 'lower': 107, 'medium': 108, 'two': 109, 'cost': 110, 'acquisition': 111, 'sell': 112, 'director': 113, 'news': 114, 'employee': 115, 'day': 116, 'totalled': 117, 'dividend': 118, 'management': 119, 'russia': 120, 'number': 121, 'right': 122, 'short': 123, 'may': 124, 'plan': 125, 'equipment': 126, 'high': 127, 'september': 128, 'exchange': 129, 'country': 130, 'annual': 131, 'however': 132, 'stake': 133, 'global': 134, 'approximately': 135, 'software': 136, 'totaled': 137, 'office': 138, 'no': 139, 'communication': 140, 'option': 141, 'offer': 142, 'march': 143, 'people': 144, 'data': 145, 'manufacturer': 146, 'october': 147, 'tsla': 148, 'volume': 149, 'including': 150, 'cash': 151, 'support': 152, 'eps': 153, 'oil': 154, 'steel': 155, 'based': 156, 'announced': 157, 'design': 158, 'sector': 159, 'good': 160, 'electronics': 161, 'reported': 162, 'transaction': 163, 'ltd': 164, 'target': 165, 'manufacturing': 166, 'make': 167, 'week': 168, 'expects': 169, 'say': 170, 'pretax': 171, 'supply': 172, 'china': 173, 'january': 174, 'start': 175, 'due': 176, 'see': 177, 'phone': 178, 'shareholder': 179, 'information': 180, 'estimated': 181, 'position': 182, 'take': 183, 'cut': 184, 'amounted': 185, 'process': 186, 'level': 187, 'facility': 188, 'subsidiary': 189, 'higher': 190, 'includes': 191, 'use': 192, 'close': 193, 'signed': 194, 'made': 195, 'engineering': 196, 'application': 197, 'term': 198, 'item': 199, 'baltic': 200, 'include': 201, 'nine': 202, 'energy': 203, 'trade': 204, 'april': 205, 'largest': 206, 'release': 207, 'store': 208, 'back': 209, 'next': 210, 'q': 211, 'holding': 212, 'since': 213, 'february': 214, 'delivery': 215, 'investor': 216, 'around': 217, 'cent': 218, 'property': 219, 'corp': 220, 'division': 221, 'move': 222, 'president': 223, 'member': 224, 'food': 225, 'fund': 226, 'provider': 227, 'uk': 228, 'fall': 229, 'negotiation': 230, 'capacity': 231, 'supplier': 232, 'work': 233, 'strong': 234, 'still': 235, 'device': 236, 'world': 237, 'call': 238, 'international': 239, 'nordic': 240, 'sweden': 241, 'security': 242, 'august': 243, 'estimate': 244, 'local': 245, 'cover': 246, 'fb': 247, 'material': 248, 'power': 249, 'model': 250, 'analyst': 251, 'staff': 252, 'major': 253, 'turnover': 254, 'amount': 255, 'inc': 256, 'change': 257, 'point': 258, 'industrial': 259, 'issue': 260, 'added': 261, 'available': 262, 'look': 263, 'city': 264, 'brand': 265, 'swedish': 266, 'tax': 267, 'three': 268, 'nonrecurring': 269, 'stora': 270, 'retail': 271, 'elcoteq': 272, 'consumer': 273, 'currently': 274, 'yearonyear': 275, 'spy': 276, 'ruukki': 277, 'program': 278, 'pharmaceutical': 279, 'chairman': 280, 'machinery': 281, 'significant': 282, 'scanfil': 283, 'general': 284, 'ab': 285, 'distribution': 286, 'excluding': 287, 'passenger': 288, 'low': 289, 'set': 290, 'mill': 291, 'previous': 292, 'margin': 293, 'state': 294, 'following': 295, 'yit': 296, 'basware': 297, 'russian': 298, 'bn': 299, 'nordea': 300, 'rise': 301, 'personnel': 302, 'continue': 303, 'meeting': 304, 'fourth': 305, 'posted': 306, 'metal': 307, 'june': 308, 'head': 309, 'job': 310, 'place': 311, 'aim': 312, 'activity': 313, 'enso': 314, 'whole': 315, 'developed': 316, 'issued': 317, 'traffic': 318, 'update': 319, 'insurance': 320, 'hit': 321, 'partner': 322, 'positive': 323, 'november': 324, 'range': 325, 'structure': 326, 'lead': 327, 'strategy': 328, 'purchase': 329, 'situation': 330, 'mr': 331, 'real': 332, 'like': 333, 'index': 334, 'beer': 335, 'via': 336, 'st': 337, 'performance': 338, 'operator': 339, 'developer': 340, 'already': 341, 'grew': 342, 'chain': 343, 'research': 344, 'demand': 345, 'firm': 346, 'awarded': 347, 'sold': 348, 'current': 349, 'go': 350, 'related': 351, 'asset': 352, 'finnair': 353, 'g': 354, 'america': 355, 'astrazeneca': 356, 'sport': 357, 'chief': 358, 'rate': 359, 'europe': 360, 'tesco': 361, 'ftse': 362, 'north': 363, 'estate': 364, 'fiskars': 365, 'remain': 366, 'looking': 367, 'gain': 368, 'cooperation': 369, 'center': 370, 'july': 371, 'capman': 372, 'hold': 373, 'completed': 374, 'adp': 375, 'focus': 376, 'need': 377, 'closed': 378, 'manufacture': 379, 'went': 380, 'agreed': 381, 'negative': 382, 'life': 383, 'key': 384, 'addition': 385, 'near': 386, 'executive': 387, 'acquired': 388, 'measure': 389, 'flow': 390, 'income': 391, 'chart': 392, 'previously': 393, 'ago': 394, 'joint': 395, 'started': 396, 'handling': 397, 'used': 398, 'subscription': 399, 'daily': 400, 'rating': 401, 'worth': 402, 'interest': 403, 'held': 404, 'forest': 405, 'detail': 406, 'alma': 407, 'estonia': 408, 'sampo': 409, 'kemira': 410, 'petersburg': 411, 'delivered': 412, 'european': 413, 'friday': 414, 'london': 415, 'wednesday': 416, 'sanoma': 417, 'internet': 418, 'test': 419, 'analysis': 420, 'several': 421, 'come': 422, 'maintenance': 423, 'bid': 424, 'processing': 425, 'large': 426, 'germany': 427, 'pm': 428, 'thursday': 429, 'credit': 430, 'aspo': 431, 'voting': 432, 'telecom': 433, 'xac': 434, 'provides': 435, 'provide': 436, 'pay': 437, 'small': 438, 'press': 439, 'name': 440, 'nt': 441, 'site': 442, 'mreal': 443, 'way': 444, 'x': 445, 'owner': 446, 'packaging': 447, 'future': 448, 'commercial': 449, 'received': 450, 'employ': 451, 'metso': 452, 'forecast': 453, 'get': 454, 'region': 455, 'headquartered': 456, 'standard': 457, 'marketing': 458, 'versus': 459, 'another': 460, 'monday': 461, 'loan': 462, 'published': 463, 'yesterday': 464, 'trading': 465, 'four': 466, 'cargotec': 467, 'teliasonera': 468, 'e': 469, 'stockmann': 470, 'takeover': 471, 'decided': 472, 'could': 473, 'dropped': 474, 'saving': 475, 'statement': 476, 'water': 477, 'fixed': 478, 'put': 479, 'continuing': 480, 'drop': 481, 'breakout': 482, 'afx': 483, 'tuesday': 484, 'segment': 485, 'content': 486, 'longterm': 487, 'machine': 488, 'barclays': 489, 'poyry': 490, 'okmetic': 491, 'reduction': 492, 'export': 493, 'declined': 494, 'liter': 495, 'using': 496, 'base': 497, 'outotec': 498, 'land': 499, 'outlook': 500, 'drug': 501, 'teleste': 502, 'transfer': 503, 'going': 504, 'rapala': 505, 'venture': 506, 'disclosed': 507, 'give': 508, 'public': 509, 'control': 510, 'le': 511, 'pulp': 512, 'tonne': 513, 'fiscal': 514, 'rautaruukki': 515, 'aspocomp': 516, 'warning': 517, 'december': 518, 'technical': 519, 'home': 520, 'combined': 521, 'factory': 522, 'corporate': 523, 'neste': 524, 'respectively': 525, 'slightly': 526, 'launch': 527, 'break': 528, 'l': 529, 'componenta': 530, 'component': 531, 'recall': 532, 'handset': 533, 'potential': 534, 'scheduled': 535, 'managing': 536, 'maximum': 537, 'b': 538, 'sek': 539, 'main': 540, 'bullish': 541, 'announcement': 542, 'early': 543, 'sabmiller': 544, 'online': 545, 'glass': 546, 'shipping': 547, 'run': 548, 'incap': 549, 'feb': 550, 'full': 551, 'effect': 552, 'government': 553, 'konecranes': 554, 'offering': 555, 'ahlstrom': 556, 'printing': 557, 'within': 558, 'built': 559, 'aldata': 560, 'operates': 561, 'build': 562, 'resource': 563, 'decision': 564, 'top': 565, 'weak': 566, 'bought': 567, 'central': 568, 'merger': 569, 'poland': 570, 'gas': 571, 'client': 572, 'square': 573, 'player': 574, 'almost': 575, 'deliver': 576, 'chemical': 577, 'restructuring': 578, 'making': 579, 'big': 580, 'digital': 581, 'strategic': 582, 'person': 583, 'layoff': 584, 'provided': 585, 'marimekko': 586, 'resistance': 587, 'reporting': 588, 'existing': 589, 'india': 590, 'planned': 591, 'infrastructure': 592, 'starting': 593, 'fair': 594, 'expand': 595, 'house': 596, 'tikkurila': 597, 'video': 598, 'continued': 599, 'magazine': 600, 'included': 601, 'producer': 602, 'representing': 603, 'rental': 604, 'approval': 605, 'book': 606, 'nasdaq': 607, 'siemens': 608, 'generated': 609, 'glaston': 610, 'told': 611, 'airline': 612, 'patent': 613, 'manager': 614, 'record': 615, 'quality': 616, 'bridge': 617, 'consolidated': 618, 'team': 619, 'location': 620, 'six': 621, 'open': 622, 'electronic': 623, 'administration': 624, 'far': 625, 'meat': 626, 'kesko': 627, 'finlandbased': 628, 'slipped': 629, 'improvement': 630, 'party': 631, 'return': 632, 'national': 633, 'competition': 634, 'established': 635, 'car': 636, 'portfolio': 637, 'paid': 638, 'improved': 639, 'signal': 640, 'broadband': 641, 'bond': 642, 'improve': 643, 'pohjola': 644, 'cargo': 645, 'port': 646, 'kone': 647, 'ready': 648, 'r': 649, 'different': 650, 'become': 651, 'tool': 652, 'pension': 653, 'user': 654, 'date': 655, 'va': 656, 'providing': 657, 'cramo': 658, 'continues': 659, 'beginning': 660, 'talvivaara': 661, 'raw': 662, 'outokumpu': 663, 'upmkymmene': 664, 'tesla': 665, 'apple': 666, 'jan': 667, 'logistics': 668, 'raute': 669, 'ship': 670, 'sq': 671, 'worker': 672, 'adpnews': 673, 'much': 674, 'outside': 675, 'nice': 676, 'figure': 677, 'atrium': 678, 'upm': 679, 'comment': 680, 'subscribed': 681, 'given': 682, 'interim': 683, 'bounce': 684, 'stood': 685, 'action': 686, 'espoo': 687, 'nyse': 688, 'invest': 689, 'co': 690, 'januaryjune': 691, 'mainly': 692, 'comparable': 693, 'ebit': 694, 'metre': 695, 'located': 696, 'currency': 697, 'many': 698, 'got': 699, 'inbev': 700, 'comptel': 701, 'recently': 702, 'great': 703, 'expansion': 704, 'diluted': 705, 'fortum': 706, 'came': 707, 'carried': 708, 'cencorp': 709, 'condition': 710, 'listed': 711, 'rival': 712, 'vessel': 713, 'c': 714, 'wood': 715, 'united': 716, 'route': 717, 'norway': 718, 'n': 719, 'drive': 720, 'vice': 721, 'register': 722, 'operational': 723, 'side': 724, 'biohit': 725, 'enables': 726, 'space': 727, 'present': 728, 'debt': 729, 'enable': 730, 'method': 731, 'crane': 732, 'france': 733, 'show': 734, 'study': 735, 'latest': 736, 'natural': 737, 'responsible': 738, 'raised': 739, 'union': 740, 'owns': 741, 'stockholm': 742, 'getting': 743, 'narrowed': 744, 'phase': 745, 'rt': 746, 'owned': 747, 'working': 748, 'vaisala': 749, 'growing': 750, 'huhtamaki': 751, 'bln': 752, 'upgrade': 753, 'januaryseptember': 754, 'planning': 755, 'royal': 756, 'elisa': 757, 'want': 758, 'tiimari': 759, 'american': 760, 'ton': 761, 'temporary': 762, 'morning': 763, 'street': 764, 'equity': 765, 'fullyear': 766, 'developing': 767, 'treatment': 768, 'japan': 769, 'music': 770, 'newspaper': 771, 'money': 772, 'forward': 773, 'alexandria': 774, 'tekla': 775, 'access': 776, 'electricity': 777, 'green': 778, 'lay': 779, 'worldwide': 780, 'latvia': 781, 'complete': 782, 'ramirent': 783, 'nearly': 784, 'nflx': 785, 'double': 786, 'authority': 787, 'talentum': 788, 'think': 789, 'p': 790, 'dec': 791, 'summer': 792, 'efficiency': 793, 'bottom': 794, 'automation': 795, 'norwegian': 796, 'especially': 797, 'five': 798, 'amzn': 799, 'decrease': 800, 'type': 801, 'ferry': 802, 'installation': 803, 'department': 804, 'park': 805, 'biggest': 806, 'telecommunication': 807, 'able': 808, 'clothing': 809, 'raisio': 810, 'organization': 811, 'economic': 812, 'additional': 813, 'coming': 814, 'vehicle': 815, 'produce': 816, 'olvi': 817, 'grow': 818, 'bring': 819, 'experience': 820, 'asia': 821, 'eu': 822, 'shell': 823, 'entire': 824, 'account': 825, 'later': 826, 'moscow': 827, 'profile': 828, 'watch': 829, 'domestic': 830, 'private': 831, 'raise': 832, 'enterprise': 833, 'impact': 834, 'sbux': 835, 'without': 836, 'terminal': 837, 'efore': 838, 'conference': 839, 'retailer': 840, 'invoice': 841, 'transportation': 842, 'financing': 843, 'flight': 844, 'hotel': 845, 'oct': 846, 'thomson': 847, 'cap': 848, 'temporarily': 849, 'talk': 850, 'yet': 851, 'ground': 852, 'remaining': 853, 'various': 854, 'acquire': 855, 'seat': 856, 'carrier': 857, 'trend': 858, 'czech': 859, 'h': 860, 'setup': 861, 'form': 862, 'environmental': 863, 'develop': 864, 'environment': 865, 'orion': 866, 'court': 867, 'liquid': 868, 'lot': 869, 'britain': 870, 'concerning': 871, 'oulu': 872, 'suominen': 873, 'bp': 874, 'important': 875, 'function': 876, 'south': 877, 'shopping': 878, 'web': 879, 'science': 880, 'lift': 881, 'chartered': 882, 'cutting': 883, 'leading': 884, 'field': 885, 'recent': 886, 'usa': 887, 'digia': 888, 'reduce': 889, 'email': 890, 'leader': 891, 'transport': 892, 'better': 893, 'face': 894, 'smaller': 895, 'viking': 896, 'platform': 897, 'uponor': 898, 'released': 899, 'clear': 900, 'payment': 901, 'best': 902, 'size': 903, 'add': 904, 'mining': 905, 'gave': 906, 'charging': 907, 'together': 908, 'sea': 909, 'bull': 910, 'ended': 911, 'light': 912, 'even': 913, 'northern': 914, 'hk': 915, 'suomen': 916, 'composite': 917, 'core': 918, 'yhoo': 919, 'glencore': 920, 'italy': 921, 'ice': 922, 'deutsche': 923, 'mean': 924, 'buying': 925, 'silicon': 926, 'saw': 927, 'computer': 928, 'consumption': 929, 'managed': 930, 'amer': 931, 'fine': 932, 'officer': 933, 'reduced': 934, 'profitability': 935, 'expense': 936, 'majority': 937, 'tel': 938, 'autumn': 939, 'find': 940, 'affecto': 941, 'banking': 942, 'finance': 943, 'free': 944, 'jump': 945, 'selling': 946, 'taken': 947, 'broker': 948, 'recommendation': 949, 'wireless': 950, 'travel': 951, 'station': 952, 'aviva': 953, 'salcomp': 954, 'laid': 955, 'actual': 956, 'brazil': 957, 'expertise': 958, 'commission': 959, 'post': 960, 'keep': 961, 'lithuania': 962, 'vaahto': 963, 'kroon': 964, 'strike': 965, 'proposed': 966, 'republic': 967, 'seen': 968, 'hand': 969, 'operate': 970, 'plus': 971, 'southern': 972, 'ag': 973, 'among': 974, 'brewery': 975, 'bg': 976, 'ore': 977, 'flexible': 978, 'k': 979, 'wafer': 980, 'pleased': 981, 'tampere': 982, 'recorded': 983, 'tomorrow': 984, 'accordance': 985, 'africa': 986, 'win': 987, 'cancer': 988, 'hkscan': 989, 'fsecure': 990, 'panostaja': 991, 'disclose': 992, 'abp': 993, 'technopolis': 994, 'finnlines': 995, 'nok': 996, 'act': 997, 'microsoft': 998, 'association': 999, 'mail': 1000, 'lemminkainen': 1001, 'heating': 1002, 'rally': 1003, 'centre': 1004, 'likely': 1005, 'sponda': 1006, 'window': 1007, 'breaking': 1008, 'shire': 1009, 'play': 1010, 'took': 1011, 'arm': 1012, 'voice': 1013, 'stainless': 1014, 'spring': 1015, 'selected': 1016, 'rb': 1017, 'charger': 1018, 'turned': 1019, 'safety': 1020, 'special': 1021, 'nickel': 1022, 'produced': 1023, 'estonian': 1024, 'fuel': 1025, 'output': 1026, 'dip': 1027, 'fishing': 1028, 'note': 1029, 'indian': 1030, 'reached': 1031, 'balance': 1032, 'sheet': 1033, 'section': 1034, 'senior': 1035, 'ruokatalo': 1036, 'asian': 1037, 'accounting': 1038, 'soon': 1039, 'tyre': 1040, 'boost': 1041, 'ownership': 1042, 'elevator': 1043, 'efficient': 1044, 'forestry': 1045, 'vantaa': 1046, 'navigation': 1047, 'game': 1048, 'branch': 1049, 'ahead': 1050, 'arrangement': 1051, 'rtrks': 1052, 'netherlands': 1053, 'presence': 1054, 'notice': 1055, 'source': 1056, 'moving': 1057, 'significantly': 1058, 'ensure': 1059, 'reach': 1060, 'found': 1061, 'approach': 1062, 'upon': 1063, 'completion': 1064, 'basis': 1065, 'hod': 1066, 'former': 1067, 'zone': 1068, 'publish': 1069, 'bbry': 1070, 'least': 1071, 'review': 1072, 'air': 1073, 'citycon': 1074, 'propose': 1075, 'shall': 1076, 'rsi': 1077, 'old': 1078, 'vote': 1079, 'cement': 1080, 'partnership': 1081, 'fully': 1082, 'container': 1083, 'establish': 1084, 'dollar': 1085, 'east': 1086, 'personal': 1087, 'average': 1088, 'pattern': 1089, 'art': 1090, 'apriljune': 1091, 'secure': 1092, 'redundant': 1093, 'upside': 1094, 'proposal': 1095, 'controlled': 1096, 'mw': 1097, 'wartsila': 1098, 'buyer': 1099, 'known': 1100, 'heat': 1101, 'accounted': 1102, 'ixonos': 1103, 'limited': 1104, 'tallinn': 1105, 'morrison': 1106, 'road': 1107, 'upgraded': 1108, 'mid': 1109, 'vmc': 1110, 'ratio': 1111, 'goog': 1112, 'taking': 1113, 'consulting': 1114, 'channel': 1115, 'housing': 1116, 'overall': 1117, 'sum': 1118, 'fiber': 1119, 'connection': 1120, 'might': 1121, 'begin': 1122, 'increasing': 1123, 'little': 1124, 'lost': 1125, 'insurer': 1126, 'sawmill': 1127, 'registered': 1128, 'mark': 1129, 'ponsse': 1130, 'dealer': 1131, 'laboratory': 1132, 'accessory': 1133, 'institution': 1134, 'transferred': 1135, 'boat': 1136, 'exel': 1137, 'danske': 1138, 'jumped': 1139, 'confirmed': 1140, 'consists': 1141, 'honkarakenne': 1142, 'wipe': 1143, 'nonwovens': 1144, 'server': 1145, 'welcome': 1146, 'representative': 1147, 'fda': 1148, 'import': 1149, 'concern': 1150, 'track': 1151, 'guidance': 1152, 'measurement': 1153, 'goldman': 1154, 'sachs': 1155, 'electrical': 1156, 'aker': 1157, 'committee': 1158, 'district': 1159, 'wind': 1160, 'ordered': 1161, 'join': 1162, 'specialty': 1163, 'foreign': 1164, 'benefon': 1165, 'danish': 1166, 'coating': 1167, 'problem': 1168, 'invested': 1169, 'british': 1170, 'launched': 1171, 'across': 1172, 'gross': 1173, 'trademark': 1174, 'strengthen': 1175, 'closing': 1176, 'appointed': 1177, 'tecnomen': 1178, 'competitive': 1179, 'star': 1180, 'active': 1181, 'secondquarter': 1182, 'weekly': 1183, 'similar': 1184, 'let': 1185, 'direct': 1186, 'eastern': 1187, 'tough': 1188, 'engine': 1189, 'gold': 1190, 'matter': 1191, 'generation': 1192, 'complex': 1193, 'kaupthing': 1194, 'denmark': 1195, 'commitment': 1196, 'buyout': 1197, 'divestment': 1198, 'tieto': 1199, 'required': 1200, 'bhp': 1201, 'social': 1202, 'lnnen': 1203, 'tehtaat': 1204, 'website': 1205, 'red': 1206, 'residential': 1207, 'factor': 1208, 'lassila': 1209, 'tikanoja': 1210, 'iphone': 1211, 'economy': 1212, 'designed': 1213, 'pt': 1214, 'steady': 1215, 'waste': 1216, 'particular': 1217, 'preliminary': 1218, 'eet': 1219, 'event': 1220, 'nokian': 1221, 'expectation': 1222, 'university': 1223, 'founded': 1224, 'every': 1225, 'fish': 1226, 'canada': 1227, 'prospect': 1228, 'ragutis': 1229, 'installed': 1230, 'twtr': 1231, 'slide': 1232, 'fear': 1233, 'opportunity': 1234, 'wide': 1235, 'hong': 1236, 'strength': 1237, 'mine': 1238, 'climb': 1239, 'se': 1240, 'bearish': 1241, 'trader': 1242, 'iwm': 1243, 'benchmark': 1244, 'kyro': 1245, 'approved': 1246, 'list': 1247, 'falling': 1248, 'aktia': 1249, 'charge': 1250, 'lse': 1251, 'letter': 1252, 'advanced': 1253, 'subject': 1254, 'final': 1255, 'easyjet': 1256, 'matti': 1257, 'ebitda': 1258, 'risk': 1259, 'implementation': 1260, 'reorganisation': 1261, 'directed': 1262, 'implement': 1263, 'etteplan': 1264, 'latin': 1265, 'premise': 1266, 'grimaldi': 1267, 'publisher': 1268, 'gap': 1269, 'ssh': 1270, 'em': 1271, 'w': 1272, 'participant': 1273, 'informed': 1274, 'sign': 1275, 'licensing': 1276, 'quite': 1277, 'macd': 1278, 'refining': 1279, 'pop': 1280, 'dutch': 1281, 'late': 1282, 'receive': 1283, 'policy': 1284, 'julyseptember': 1285, 'tietoenator': 1286, 'nov': 1287, 'solid': 1288, 'suv': 1289, 'trouble': 1290, 'ending': 1291, 'seller': 1292, 'fleet': 1293, 'ac': 1294, 'affected': 1295, 'capability': 1296, 'turn': 1297, 'claim': 1298, 'underlying': 1299, 'acquiring': 1300, 'decline': 1301, 'highquality': 1302, 'meur': 1303, 'vendor': 1304, 'involved': 1305, 'category': 1306, 'employment': 1307, 'scheme': 1308, 'synergy': 1309, 'consolidation': 1310, 'entered': 1311, 'qqq': 1312, 'affect': 1313, 'series': 1314, 'running': 1315, 'poor': 1316, 'know': 1317, 'mixed': 1318, 'textile': 1319, 'circuit': 1320, 'vacon': 1321, 'moment': 1322, 'furniture': 1323, 'meet': 1324, 'consultancy': 1325, 'possible': 1326, 'shop': 1327, 'serve': 1328, 'extension': 1329, 'satama': 1330, 'extremely': 1331, 'swung': 1332, 'googl': 1333, 'warehouse': 1334, 'thus': 1335, 'single': 1336, 'litre': 1337, 'downside': 1338, 'led': 1339, 'modern': 1340, 'lithuanian': 1341, 'speed': 1342, 'solidium': 1343, 'underground': 1344, 'mobility': 1345, 'chinese': 1346, 'truck': 1347, 'enough': 1348, 'remained': 1349, 'wholesale': 1350, 'producing': 1351, 'seven': 1352, 'tv': 1353, 'registration': 1354, 'sustainable': 1355, 'recovery': 1356, 'widened': 1357, 'scope': 1358, 'training': 1359, 'morgan': 1360, 'showed': 1361, 'live': 1362, 'webcast': 1363, 'adjustment': 1364, 'effective': 1365, 'srv': 1366, 'parent': 1367, 'clearly': 1368, 'saying': 1369, 'contact': 1370, 'develops': 1371, 'certain': 1372, 'solteq': 1373, 'kazakhstan': 1374, 'quarterly': 1375, 'coast': 1376, 'comprising': 1377, 'outperform': 1378, 'testing': 1379, 'jyvaskyla': 1380, 'lease': 1381, 'farm': 1382, 'bad': 1383, 'min': 1384, 'version': 1385, 'conducted': 1386, 'ten': 1387, 'interested': 1388, 'paint': 1389, 'entry': 1390, 'hall': 1391, 'rule': 1392, 'samsung': 1393, 'sanomat': 1394, 'floor': 1395, 'apartment': 1396, 'refinery': 1397, 'aluminium': 1398, 'iron': 1399, 'step': 1400, 'noted': 1401, 'cfo': 1402, 'expert': 1403, 'lol': 1404, 'tm': 1405, 'friend': 1406, 'family': 1407, 'offshore': 1408, 'middle': 1409, 'profitable': 1410, 'spain': 1411, 'force': 1412, 'motor': 1413, 'annually': 1414, 'eight': 1415, 'juha': 1416, 'status': 1417, 'deputy': 1418, 'pc': 1419, 'away': 1420, 'percentage': 1421, 'belgium': 1422, 'brandnew': 1423, 'overview': 1424, 'expenditure': 1425, 'card': 1426, 'longer': 1427, 'eb': 1428, 'eek': 1429, 'berkshire': 1430, 'community': 1431, 'stage': 1432, 'along': 1433, 'ipo': 1434, 'thanks': 1435, 'cable': 1436, 'closure': 1437, 'filter': 1438, 'format': 1439, 'medical': 1440, 'hope': 1441, 'turku': 1442, 'cross': 1443, 'achieve': 1444, 'tower': 1445, 'mexico': 1446, 'sto': 1447, 'yard': 1448, 'eye': 1449, 'handle': 1450, 'railway': 1451, 'turkey': 1452, 'follow': 1453, 'endp': 1454, 'stop': 1455, 'healthcare': 1456, 'needed': 1457, 'reason': 1458, 'enter': 1459, 'citigroup': 1460, 'australia': 1461, 'load': 1462, 'coated': 1463, 'sainsbury': 1464, 'kalmar': 1465, 'google': 1466, 'startup': 1467, 'equipped': 1468, 'feed': 1469, 'xa': 1470, 'th': 1471, 'followed': 1472, 'ml': 1473, 'changed': 1474, 'responsibility': 1475, 'metsaliitto': 1476, 'pricing': 1477, 'block': 1478, 'specialist': 1479, 'mechanical': 1480, 'tip': 1481, 'patient': 1482, 'coq': 1483, 'yearago': 1484, 'slump': 1485, 'kong': 1486, 'comprises': 1487, 'oriolakd': 1488, 'headquarters': 1489, 'therefore': 1490, 'official': 1491, 'subscriber': 1492, 'black': 1493, 'quick': 1494, 'kari': 1495, 'applied': 1496, 'zinc': 1497, 'body': 1498, 'municipality': 1499, 'downgraded': 1500, 'initial': 1501, 'dma': 1502, 'thirdquarter': 1503, 'repurchase': 1504, 'hsbc': 1505, 'emission': 1506, 'fee': 1507, 'furthermore': 1508, 'initiated': 1509, 'traded': 1510, 'agency': 1511, 'januarymarch': 1512, 'ukraine': 1513, 'advertising': 1514, 'license': 1515, 'huge': 1516, 'goal': 1517, 'agricultural': 1518, 'idea': 1519, 'macgregor': 1520, 'abroad': 1521, 'allows': 1522, 'brought': 1523, 'marine': 1524, 'neomarkka': 1525, 'sysopen': 1526, 'implemented': 1527, 'create': 1528, 'stone': 1529, 'divested': 1530, 'map': 1531, 'integration': 1532, 'msft': 1533, 'emerging': 1534, 'legal': 1535, 'ii': 1536, 'indicator': 1537, 'created': 1538, 'covered': 1539, 'qpr': 1540, 'prepaid': 1541, 'panel': 1542, 'considerably': 1543, 'hike': 1544, 'instrument': 1545, 'fast': 1546, 'aimed': 1547, 'ilkkayhtyma': 1548, 'hour': 1549, 'creating': 1550, 'professional': 1551, 'york': 1552, 'afternoon': 1553, 'distributor': 1554, 'rest': 1555, 'totalling': 1556, 'stonesoft': 1557, 'sixmonth': 1558, 'award': 1559, 'threemonth': 1560, 'town': 1561, 'sole': 1562, 'surged': 1563, 'christmas': 1564, 'requirement': 1565, 'gild': 1566, 'primarily': 1567, 'ski': 1568, 'cocacola': 1569, 'outstanding': 1570, 'past': 1571, 'utility': 1572, 'id': 1573, 'printed': 1574, 'radio': 1575, 'frequency': 1576, 'john': 1577, 'yahoo': 1578, 'excess': 1579, 'llc': 1580, 'role': 1581, 'exit': 1582, 'represents': 1583, 'divided': 1584, 'initiative': 1585, 'territory': 1586, 'send': 1587, 'history': 1588, 'expands': 1589, 'prior': 1590, 'hungary': 1591, 'uncertainty': 1592, 'considering': 1593, 'rumor': 1594, 'monthly': 1595, 'billiton': 1596, 'regional': 1597, 'independent': 1598, 'happy': 1599, 'tltv': 1600, 'serving': 1601, 'valued': 1602, 'failed': 1603, 'sept': 1604, 'tallink': 1605, 'touch': 1606, 'tender': 1607, 'facebook': 1608, 'deployment': 1609, 'save': 1610, 'human': 1611, 'chosen': 1612, 'kind': 1613, 'mhz': 1614, 'weakness': 1615, 'became': 1616, 'boosted': 1617, 'tire': 1618, 'shanghai': 1619, 'federal': 1620, 'baxalta': 1621, 'law': 1622, 'course': 1623, 'vat': 1624, 'nuclear': 1625, 'focusing': 1626, 'connectivity': 1627, 'carry': 1628, 'stay': 1629, 'canceled': 1630, 'veneer': 1631, 'wall': 1632, 'plywood': 1633, 'german': 1634, 'billing': 1635, 'supermarket': 1636, 'smartphone': 1637, 'intent': 1638, 'original': 1639, 'announces': 1640, 'picked': 1641, 'frame': 1642, 'garden': 1643, 'favourable': 1644, 'color': 1645, 'oversold': 1646, 'reader': 1647, 'unchanged': 1648, 'knowhow': 1649, 'stream': 1650, 'install': 1651, 'example': 1652, 'couple': 1653, 'undisclosed': 1654, 'benefit': 1655, 'print': 1656, 'fresh': 1657, 'negotiating': 1658, 'alliance': 1659, 'transferring': 1660, 'specialising': 1661, 'meanwhile': 1662, 'altogether': 1663, 'strengthened': 1664, 'glaxosmithkline': 1665, 'viewed': 1666, 'log': 1667, 'copper': 1668, 'mineral': 1669, 'vol': 1670, 'lte': 1671, 'brazilian': 1672, 'quoted': 1673, 'ireland': 1674, 'nonlife': 1675, 'please': 1676, 'salomon': 1677, 'competitiveness': 1678, 'lowered': 1679, 'cell': 1680, 'doubled': 1681, 'separate': 1682, 'threat': 1683, 'rising': 1684, 'de': 1685, 'necessary': 1686, 'tecnotree': 1687, 'concept': 1688, 'challenging': 1689, 'granted': 1690, 'seems': 1691, 'eesti': 1692, 'seek': 1693, 'oko': 1694, 'merged': 1695, 'lem': 1696, 'qualcomm': 1697, 'amid': 1698, 'snow': 1699, 'interesting': 1700, 'integrated': 1701, 'gps': 1702, 'jointly': 1703, 'participated': 1704, 'ibm': 1705, 'hobby': 1706, 'demanding': 1707, 'hardware': 1708, 'licence': 1709, 'relative': 1710, 'giant': 1711, 'plot': 1712, 'premium': 1713, 'beverage': 1714, 'lemminkinen': 1715, 'opening': 1716, 'finally': 1717, 'protalix': 1718, 'implementing': 1719, 'stated': 1720, 'km': 1721, 'interface': 1722, 'merchant': 1723, 'named': 1724, 'raahe': 1725, 'expanded': 1726, 'cause': 1727, 'shot': 1728, 'bag': 1729, 'hook': 1730, 'associated': 1731, 'manages': 1732, 'care': 1733, 'relatively': 1734, 'manage': 1735, 'diagnostic': 1736, 'rapid': 1737, 'opened': 1738, 'madison': 1739, 'finnishswedish': 1740, 'portion': 1741, 'sappi': 1742, 'surprise': 1743, 'gypsii': 1744, 'attack': 1745, 'partly': 1746, 'intraday': 1747, 'meter': 1748, 'float': 1749, 'entertainment': 1750, 'arena': 1751, 'catalyst': 1752, 'contains': 1753, 'civil': 1754, 'broad': 1755, 'consideration': 1756, 'nomination': 1757, 'scale': 1758, 'counter': 1759, 'serf': 1760, 'island': 1761, 'principal': 1762, 'relevant': 1763, 'module': 1764, 'swedbank': 1765, 'otherwise': 1766, 'lee': 1767, 'swing': 1768, 'slid': 1769, 'shift': 1770, 'reserve': 1771, 'extensive': 1772, 'duration': 1773, 'love': 1774, 'check': 1775, 'ninemonth': 1776, 'pressure': 1777, 'surge': 1778, 'litas': 1779, 'bear': 1780, 'class': 1781, 'biotie': 1782, 'liquidity': 1783, 'crisis': 1784, 'explained': 1785, 'texas': 1786, 'specialises': 1787, 'cruise': 1788, 'isrg': 1789, 'thousand': 1790, 'reuters': 1791, 'inventor': 1792, 'manufactured': 1793, 'met': 1794, 'engaged': 1795, 'metro': 1796, 'consensus': 1797, 'lng': 1798, 'hard': 1799, 'larger': 1800, 'scandinavian': 1801, 'comprise': 1802, 'intel': 1803, 'kingfisher': 1804, 'shortterm': 1805, 'internal': 1806, 'stronger': 1807, 'recycled': 1808, 'convertible': 1809, 'lifted': 1810, 'scissors': 1811, 'enabling': 1812, 'concluded': 1813, 'view': 1814, 'asa': 1815, 'gmbh': 1816, 'plastic': 1817, 'must': 1818, 'latvian': 1819, 'grade': 1820, 'called': 1821, 'variety': 1822, 'downgrade': 1823, 'bac': 1824, 'messaging': 1825, 'receives': 1826, 'jarmo': 1827, 'advisory': 1828, 'ft': 1829, 'individual': 1830, 'palm': 1831, 'strengthening': 1832, 'telenor': 1833, 'competitor': 1834, 'mall': 1835, 'pearl': 1836, 'fat': 1837, 'shale': 1838, 'motorola': 1839, 'ongoing': 1840, 'savcor': 1841, 'code': 1842, 'tectia': 1843, 'cooperative': 1844, 'saint': 1845, 'relation': 1846, 'council': 1847, 'file': 1848, 'starbucks': 1849, 'hot': 1850, 'workforce': 1851, 'romania': 1852, 'paikallissanomat': 1853, 'despite': 1854, 'purchased': 1855, 'holder': 1856, 'lvl': 1857, 'case': 1858, 'document': 1859, 'functionality': 1860, 'done': 1861, 'popular': 1862, 'plate': 1863, 'fact': 1864, 'indicated': 1865, 'pretty': 1866, 'wo': 1867, 'goodwill': 1868, 'sami': 1869, 'scan': 1870, 'introduced': 1871, 'clean': 1872, 'presentation': 1873, 'carphone': 1874, 'kaukotelko': 1875, 'fiveyear': 1876, 'obtained': 1877, 'axp': 1878, 'auto': 1879, 'lifting': 1880, 'timo': 1881, 'inventory': 1882, 'fined': 1883, 'asphalt': 1884, 'cartel': 1885, 'broken': 1886, 'okmv': 1887, 'throughout': 1888, 'yearearlier': 1889, 'ict': 1890, 'leave': 1891, 'crosscountry': 1892, 'jp': 1893, 'represented': 1894, 'reiterated': 1895, 'asthma': 1896, 'dynamic': 1897, 'cthv': 1898, 'understanding': 1899, 'giving': 1900, 'remainder': 1901, 'aug': 1902, 'newstex': 1903, 'spread': 1904, 'z': 1905, 'pharma': 1906, 'changing': 1907, 'advance': 1908, 'martela': 1909, 'comprehensive': 1910, 'pro': 1911, 'brexit': 1912, 'finalized': 1913, 'axe': 1914, 'publication': 1915, 'statutory': 1916, 'streamlining': 1917, 'eero': 1918, 'marketwatch': 1919, 'rutav': 1920, 'opinion': 1921, 'announce': 1922, 'oppohjola': 1923, 'tumble': 1924, 'towards': 1925, 'grain': 1926, 'brief': 1927, 'tube': 1928, 'po': 1929, 'watching': 1930, 'johnson': 1931, 'matthey': 1932, 'firstquarter': 1933, 'lloyd': 1934, 'poised': 1935, 'triangle': 1936, 'minute': 1937, 'approves': 1938, 'losing': 1939, 'beyond': 1940, 'tree': 1941, 'household': 1942, 'orange': 1943, 'placed': 1944, 'unnamed': 1945, 'stand': 1946, 'supported': 1947, 'loyal': 1948, 'fireplace': 1949, 'alternative': 1950, 'aircraft': 1951, 'wet': 1952, 'japanese': 1953, 'besides': 1954, 'hannu': 1955, 'extends': 1956, 'front': 1957, 'kai': 1958, 'master': 1959, 'subcontractor': 1960, 'faster': 1961, 'proha': 1962, 'mutual': 1963, 'turning': 1964, 'commercially': 1965, 'amazon': 1966, 'threeyear': 1967, 'antenna': 1968, 'pilot': 1969, 'distribute': 1970, 'regulator': 1971, 'participate': 1972, 'formed': 1973, 'peter': 1974, 'furnace': 1975, 'hitting': 1976, 'really': 1977, 'trial': 1978, 'search': 1979, 'plumbing': 1980, 'targeting': 1981, 'commissioned': 1982, 'roof': 1983, 'jun': 1984, 'pursue': 1985, 'asked': 1986, 'visitor': 1987, 'san': 1988, 'expanding': 1989, 'storage': 1990, 'someone': 1991, 'promise': 1992, 'recalling': 1993, 'rail': 1994, 'approve': 1995, 'itv': 1996, 'thing': 1997, 'tobacco': 1998, 'oneoff': 1999, 'actually': 2000, 'employed': 2001, 'remuneration': 2002, 'kilometre': 2003, 'pushed': 2004, 'identity': 2005, 'tulikivi': 2006, 'measuring': 2007, 'fargo': 2008, 'band': 2009, 'norske': 2010, 'acquires': 2011, 'audio': 2012, 'blue': 2013, 'worked': 2014, 'deposit': 2015, 'gift': 2016, 'interior': 2017, 'combine': 2018, 'unless': 2019, 'verizon': 2020, 'library': 2021, 'primary': 2022, 'ollipekka': 2023, 'acting': 2024, 'nastola': 2025, 'resulted': 2026, 'cancellation': 2027, 'frozen': 2028, 'vegetable': 2029, 'payroll': 2030, 'bringing': 2031, 'twoday': 2032, 'frankfurt': 2033, 'protection': 2034, 'jukka': 2035, 'jussi': 2036, 'journey': 2037, 'elektrobit': 2038, 'fifth': 2039, 'lure': 2040, 'interrupted': 2041, 'suite': 2042, 'success': 2043, 'dedicated': 2044, 'cloud': 2045, 'whose': 2046, 'lending': 2047, 'neutral': 2048, 'recognized': 2049, 'initially': 2050, 'salv': 2051, 'address': 2052, 'poultry': 2053, 'pcln': 2054, 'amazing': 2055, 'investing': 2056, 'reporter': 2057, 'gulf': 2058, 'bothnia': 2059, 'returned': 2060, 'bidding': 2061, 'pick': 2062, 'combining': 2063, 'momentum': 2064, 'sma': 2065, 'pvt': 2066, 'limit': 2067, 'soft': 2068, 'maybe': 2069, 'link': 2070, 'demonstration': 2071, 'procurement': 2072, 'broke': 2073, 'convergent': 2074, 'reversal': 2075, 'traditional': 2076, 'box': 2077, 'page': 2078, 'supporting': 2079, 'guide': 2080, 'mika': 2081, 'moved': 2082, 'shipment': 2083, 'proportion': 2084, 'juhani': 2085, 'certification': 2086, 'lindex': 2087, 'whereby': 2088, 'powered': 2089, 'osl': 2090, 'correspond': 2091, 'saudi': 2092, 'intends': 2093, 'feature': 2094, 'ever': 2095, 'invention': 2096, 'multiple': 2097, 'trying': 2098, 'story': 2099, 'diageo': 2100, 'tackle': 2101, 'networking': 2102, 'described': 2103, 'sverige': 2104, 'crown': 2105, 'solar': 2106, 'recycling': 2107, 'becoming': 2108, 'operative': 2109, 'fallen': 2110, 'introduce': 2111, 'lahti': 2112, 'automotive': 2113, 'gone': 2114, 'leased': 2115, 'alert': 2116, 'hamina': 2117, 'apply': 2118, 'ipad': 2119, 'wholly': 2120, 'rejected': 2121, 'shareholding': 2122, 'safran': 2123, 'publishing': 2124, 'baby': 2125, 'korea': 2126, 'innovation': 2127, 'substantial': 2128, 'nordstjernan': 2129, 'cutlery': 2130, 'fisa': 2131, 'tank': 2132, 'attempt': 2133, 'shoe': 2134, 'rapidly': 2135, 'flat': 2136, 'collection': 2137, 'delighted': 2138, 'continuation': 2139, 'authentication': 2140, 'secured': 2141, 'dongguan': 2142, 'compensation': 2143, 'winner': 2144, 'immediate': 2145, 'read': 2146, 'escalator': 2147, 'par': 2148, 'geosentric': 2149, 'animal': 2150, 'narrow': 2151, 'gallerix': 2152, 'airport': 2153, 'malaysia': 2154, 'budget': 2155, 'realized': 2156, 'austria': 2157, 'forssa': 2158, 'gsk': 2159, 'loudeac': 2160, 'marcel': 2161, 'morvillars': 2162, 'summary': 2163, 'dropping': 2164, 'seppala': 2165, 'ministry': 2166, 'allow': 2167, 'outlet': 2168, 'matching': 2169, 'learning': 2170, 'man': 2171, 'ino': 2172, 'combination': 2173, 'article': 2174, 'irish': 2175, 'ethanol': 2176, 'micro': 2177, 'leap': 2178, 'estimation': 2179, 'metsabotnia': 2180, 'beat': 2181, 'vietnam': 2182, 'sue': 2183, 'depending': 2184, 'pipette': 2185, 'answer': 2186, 'dixons': 2187, 'apart': 2188, 'regular': 2189, 'mu': 2190, 'znga': 2191, 'refused': 2192, 'exercising': 2193, 'prudential': 2194, 'builder': 2195, 'wrote': 2196, 'suitable': 2197, 'help': 2198, 'western': 2199, 'novel': 2200, 'hiab': 2201, 'bae': 2202, 'tablet': 2203, 'simultaneously': 2204, 'message': 2205, 'calendar': 2206, 'meadville': 2207, 'lion': 2208, 'collaboration': 2209, 'effort': 2210, 'risto': 2211, 'documentation': 2212, 'largerthanexpected': 2213, 'smartphones': 2214, 'slight': 2215, 'modular': 2216, 'db': 2217, 'alfa': 2218, 'megafon': 2219, 'copenhagen': 2220, 'oslo': 2221, 'niche': 2222, 'ta': 2223, 'corresponds': 2224, 'skier': 2225, 'chase': 2226, 'amendment': 2227, 'lung': 2228, 'suomi': 2229, 'whether': 2230, 'therapy': 2231, 'o': 2232, 'memorandum': 2233, 'regulatory': 2234, 'symbian': 2235, 'acknowledged': 2236, 'royalty': 2237, 'decisionmaking': 2238, 'prefer': 2239, 'caused': 2240, 'crew': 2241, 'heading': 2242, 'flag': 2243, 'defense': 2244, 'progress': 2245, 'advantage': 2246, 'shape': 2247, 'turnaround': 2248, 'shalkiya': 2249, 'offered': 2250, 'others': 2251, 'forced': 2252, 'hampshire': 2253, 'regarding': 2254, 'inspection': 2255, 'kauppalehti': 2256, 'amrn': 2257, 'wpp': 2258, 'enhance': 2259, 'brewer': 2260, 'afev': 2261, 'ascending': 2262, 'meltdown': 2263, 'twig': 2264, 'kuitu': 2265, 'institutional': 2266, 'penny': 2267, 'powder': 2268, 'bye': 2269, 'clarify': 2270, 'certified': 2271, 'lining': 2272, 'slow': 2273, 'breakup': 2274, 'eila': 2275, 'elected': 2276, 'objective': 2277, 'navteq': 2278, 'overbought': 2279, 'vz': 2280, 'bunge': 2281, 'telephone': 2282, 'weakened': 2283, 'nevertheless': 2284, 'ornamental': 2285, 'signaling': 2286, 'imported': 2287, 'lulu': 2288, 'camera': 2289, 'glisten': 2290, 'disappointment': 2291, 'tanker': 2292, 'philippine': 2293, 'er': 2294, 'commerce': 2295, 'premier': 2296, 'alone': 2297, 'grown': 2298, 'appointment': 2299, 'task': 2300, 'slowing': 2301, 'ilk': 2302, 'minority': 2303, 'resident': 2304, 'caixabank': 2305, 'bpi': 2306, 'acerta': 2307, 'axa': 2308, 'qcom': 2309, 'remains': 2310, 'communicated': 2311, 'concrete': 2312, 'finish': 2313, 'vaias': 2314, 'turkish': 2315, 'package': 2316, 'locally': 2317, 'clinical': 2318, 'attract': 2319, 'quickly': 2320, 'supplying': 2321, 'ebrd': 2322, 'syndicated': 2323, 'sharply': 2324, 'although': 2325, 'abc': 2326, 'proprietary': 2327, 'pound': 2328, 'laying': 2329, 'pori': 2330, 'faller': 2331, 'skr': 2332, 'oneoffs': 2333, 'contracting': 2334, 'generates': 2335, 'listing': 2336, 'determining': 2337, 'sa': 2338, 'liquidated': 2339, 'loose': 2340, 'difficult': 2341, 'harvester': 2342, 'ceramic': 2343, 'custom': 2344, 'rsa': 2345, 'skog': 2346, 'display': 2347, 'completes': 2348, 'preparing': 2349, 'favorite': 2350, 'improving': 2351, 'consolidating': 2352, 'associate': 2353, 'agenda': 2354, 'extra': 2355, 'internationally': 2356, 'believe': 2357, 'repeat': 2358, 'kallasvuo': 2359, 'cybercom': 2360, 'nd': 2361, 'michael': 2362, 'concentrating': 2363, 'carrying': 2364, 'restaurant': 2365, 'graphic': 2366, 'payable': 2367, 'tamglass': 2368, 'expecting': 2369, 'intelligence': 2370, 'freenet': 2371, 'wooden': 2372, 'surface': 2373, 'winter': 2374, 'en': 2375, 'stuck': 2376, 'though': 2377, 'bns': 2378, 'seal': 2379, 'engineer': 2380, 'privacy': 2381, 'rts': 2382, 'bos': 2383, 'burdened': 2384, 'train': 2385, 'consolidate': 2386, 'heikki': 2387, 'foundation': 2388, 'explains': 2389, 'crh': 2390, 'hire': 2391, 'customised': 2392, 'descending': 2393, 'left': 2394, 'fire': 2395, 'volatility': 2396, 'reception': 2397, 'rolling': 2398, 'normal': 2399, 'abb': 2400, 'capable': 2401, 'assigned': 2402, 'lender': 2403, 'usdm': 2404, 'spot': 2405, 'erkki': 2406, 'drink': 2407, 'endtoend': 2408, 'meego': 2409, 'sievi': 2410, 'cc': 2411, 'wonderware': 2412, 'invite': 2413, 'room': 2414, 'brse': 2415, 'vxx': 2416, 'bo': 2417, 'wait': 2418, 'valeant': 2419, 'pearson': 2420, 'wahlroos': 2421, 'lifecycle': 2422, 'mediation': 2423, 'whitbread': 2424, 'measured': 2425, 'driving': 2426, 'foundry': 2427, 'persimmon': 2428, 'framework': 2429, 'relationship': 2430, 'duty': 2431, 'sep': 2432, 'delayed': 2433, 'permanent': 2434, 'border': 2435, 'discussion': 2436, 'considered': 2437, 'latv': 2438, 'ctrp': 2439, 'eg': 2440, 'ilkka': 2441, 'mortgage': 2442, 'strengthens': 2443, 'selection': 2444, 'intended': 2445, 'targeted': 2446, 'putting': 2447, 'skf': 2448, 'landsbanken': 2449, 'shipyard': 2450, 'trust': 2451, 'construct': 2452, 'minister': 2453, 'nevsky': 2454, 'bavelloni': 2455, 'anticipates': 2456, 'atvi': 2457, 'hbos': 2458, 'foot': 2459, 'switzerland': 2460, 'therapeutic': 2461, 'extended': 2462, 'antibody': 2463, 'alus': 2464, 'energia': 2465, 'jones': 2466, 'ashtead': 2467, 'commodity': 2468, 'liability': 2469, 'french': 2470, 'grid': 2471, 'biodiesel': 2472, 'ea': 2473, 'airbus': 2474, 'appoints': 2475, 'trailer': 2476, 'everyone': 2477, 'pacific': 2478, 'ivrcl': 2479, 'cold': 2480, 'adding': 2481, 'tyrvaan': 2482, 'brings': 2483, 'layer': 2484, 'sound': 2485, 'historic': 2486, 'maritime': 2487, 'successful': 2488, 'peeling': 2489, 'institute': 2490, 'rmrv': 2491, 'realtime': 2492, 'loyalty': 2493, 'assembly': 2494, 'newpage': 2495, 'innovative': 2496, 'productivity': 2497, 'kci': 2498, 'cinema': 2499, 'stationery': 2500, 'tiiv': 2501, 'celg': 2502, 'slashed': 2503, 'stx': 2504, 'iittala': 2505, 'imo': 2506, 'challenge': 2507, 'ya': 2508, 'eurochem': 2509, 'margarine': 2510, 'grinding': 2511, 'fewer': 2512, 'image': 2513, 'miss': 2514, 'martin': 2515, 'trent': 2516, 'postpaid': 2517, 'arcelormittal': 2518, 'euronext': 2519, 'paris': 2520, 'amanda': 2521, 'aviation': 2522, 'setting': 2523, 'beef': 2524, 'replace': 2525, 'calculated': 2526, 'projection': 2527, 'socalled': 2528, 'relocate': 2529, 'precision': 2530, 'interactive': 2531, 'kcrv': 2532, 'either': 2533, 'warrant': 2534, 'consecutive': 2535, 'karppinen': 2536, 'attached': 2537, 'establishes': 2538, 'push': 2539, 'extraordinary': 2540, 'booked': 2541, 'suisse': 2542, 'alandsbanken': 2543, 'guess': 2544, 'pkc': 2545, 'amplifier': 2546, 'sent': 2547, 'incur': 2548, 'stopped': 2549, 'involves': 2550, 'riga': 2551, 'trip': 2552, 'choice': 2553, 'mti': 2554, 'vr': 2555, 'hollola': 2556, 'casing': 2557, 'attention': 2558, 'administrator': 2559, 'accumulation': 2560, 'correlation': 2561, 'launching': 2562, 'slaughterhouse': 2563, 'finding': 2564, 'karvinen': 2565, 'supervision': 2566, 'essential': 2567, 'geographical': 2568, 'served': 2569, 'fourthquarter': 2570, 'seminar': 2571, 'helasto': 2572, 'inside': 2573, 'leaving': 2574, 'door': 2575, 'disposable': 2576, 'question': 2577, 'appropriate': 2578, 'silva': 2579, 'oldest': 2580, 'visit': 2581, 'subordinated': 2582, 'folding': 2583, 'dc': 2584, 'tech': 2585, 'recognize': 2586, 'disposal': 2587, 'merging': 2588, 'ball': 2589, 'organized': 2590, 'follows': 2591, 'gothenburg': 2592, 'delhi': 2593, 'wish': 2594, 'salonen': 2595, 'declining': 2596, 'engulfing': 2597, 'ko': 2598, 'ba': 2599, 'qt': 2600, 'packed': 2601, 'statoil': 2602, 'investigation': 2603, 'tap': 2604, 'kyroskoski': 2605, 'protocol': 2606, 'workflow': 2607, 'desktop': 2608, 'remote': 2609, 'chance': 2610, 'altimo': 2611, 'outdoor': 2612, 'scott': 2613, 'threatening': 2614, 'exploration': 2615, 'understand': 2616, 'nyberg': 2617, 'exercise': 2618, 'poaches': 2619, 'paul': 2620, 'delivers': 2621, 'presently': 2622, 'financially': 2623, 'ups': 2624, 'globally': 2625, 'sure': 2626, 'annum': 2627, 'concentrate': 2628, 'ervio': 2629, 'effectively': 2630, 'reject': 2631, 'jet': 2632, 'holiday': 2633, 'smelter': 2634, 'covering': 2635, 'contrast': 2636, 'aer': 2637, 'lingus': 2638, 'cycle': 2639, 'nv': 2640, 'bofa': 2641, 'sterv': 2642, 'fujitsu': 2643, 'walter': 2644, 'vladimir': 2645, 'mnmn': 2646, 'exceed': 2647, 'removal': 2648, 'loading': 2649, 'exposure': 2650, 'cider': 2651, 'bottle': 2652, 'grant': 2653, 'tulonen': 2654, 'holmen': 2655, 'determined': 2656, 'repurchased': 2657, 'carbon': 2658, 'preparation': 2659, 'agm': 2660, 'bearing': 2661, 'streamline': 2662, 'omxn': 2663, 'headbox': 2664, 'impressive': 2665, 'intertek': 2666, 'hosting': 2667, 'italian': 2668, 'asda': 2669, 'dismissed': 2670, 'rejoicing': 2671, 'timetable': 2672, 'feasibility': 2673, 'clarity': 2674, 'financed': 2675, 'circulation': 2676, 'wholesaler': 2677, 'tieup': 2678, 'cncv': 2679, 'productional': 2680, 'shoulder': 2681, 'trigger': 2682, 'hatch': 2683, 'quarteronquarter': 2684, 'adac': 2685, 'munich': 2686, 'expect': 2687, 'promising': 2688, 'survey': 2689, 'prepared': 2690, 'child': 2691, 'storm': 2692, 'detailed': 2693, 'creates': 2694, 'esl': 2695, 'wmt': 2696, 'successfully': 2697, 'dt': 2698, 'prosecutor': 2699, 'abn': 2700, 'represent': 2701, 'cabin': 2702, 'generate': 2703, 'nicely': 2704, 'hurt': 2705, 'yield': 2706, 'shrank': 2707, 'tim': 2708, 'centrica': 2709, 'intc': 2710, 'valueadded': 2711, 'fibrebased': 2712, 'tata': 2713, 'bhushan': 2714, 'allowed': 2715, 'satisfied': 2716, 'basic': 2717, 'antti': 2718, 'growhow': 2719, 'phosphate': 2720, 'uup': 2721, 'slovakia': 2722, 'statistic': 2723, 'bounced': 2724, 'proven': 2725, 'malware': 2726, 'wwwahlstromcom': 2727, 'newly': 2728, 'barrel': 2729, 'walking': 2730, 'aria': 2731, 'purpose': 2732, 'yara': 2733, 'incurred': 2734, 'buyback': 2735, 'latter': 2736, 'glad': 2737, 'telemig': 2738, 'weather': 2739, 'browser': 2740, 'teus': 2741, 'california': 2742, 'blast': 2743, 'village': 2744, 'costco': 2745, 'smith': 2746, 'forex': 2747, 'rollsroyce': 2748, 'sky': 2749, 'tweet': 2750, 'surprised': 2751, 'happened': 2752, 'resolution': 2753, 'try': 2754, 'icis': 2755, 'commence': 2756, 'apr': 2757, 'ride': 2758, 'biobv': 2759, 'converter': 2760, 'transmission': 2761, 'predefined': 2762, 'frost': 2763, 'extending': 2764, 'mandatory': 2765, 'casting': 2766, 'doktas': 2767, 'uptodate': 2768, 'directly': 2769, 'hammer': 2770, 'revolving': 2771, 'struggling': 2772, 'dispute': 2773, 'petrol': 2774, 'kim': 2775, 'avenue': 2776, 'los': 2777, 'relocation': 2778, 'discount': 2779, 'anticipated': 2780, 'stacker': 2781, 'leasing': 2782, 'highly': 2783, 'kndi': 2784, 'hilton': 2785, 'rantanen': 2786, 'confidence': 2787, 'contracted': 2788, 'termination': 2789, 'nominal': 2790, 'scrap': 2791, 'treasury': 2792, 'sensor': 2793, 'sinter': 2794, 'ashley': 2795, 'ifrs': 2796, 'proud': 2797, 'lt': 2798, 'adjusted': 2799, 'tenant': 2800, 'ericsson': 2801, 'cosmetic': 2802, 'decoration': 2803, 'leisure': 2804, 'alnv': 2805, 'slowly': 2806, 'jon': 2807, 'risfelt': 2808, 'express': 2809, 'lodged': 2810, 'totaling': 2811, 'resort': 2812, 'controlling': 2813, 'outsourced': 2814, 'soapstone': 2815, 'conglomerate': 2816, 'lg': 2817, 'always': 2818, 'posting': 2819, 'citing': 2820, 'preferred': 2821, 'kept': 2822, 'ubs': 2823, 'leverage': 2824, 'multimedia': 2825, 'blinkbox': 2826, 'write': 2827, 'dnb': 2828, 'seb': 2829, 'ge': 2830, 'ndx': 2831, 'sustainability': 2832, 'soar': 2833, 'gradually': 2834, 'milestone': 2835, 'kraft': 2836, 'tripled': 2837, 'payout': 2838, 'vision': 2839, 'fouryear': 2840, 'scaling': 2841, 'reportedly': 2842, 'healthy': 2843, 'focused': 2844, 'breadth': 2845, 'processor': 2846, 'signing': 2847, 'agn': 2848, 'optimization': 2849, 'unveil': 2850, 'split': 2851, 'simply': 2852, 'buffett': 2853, 'ultra': 2854, 'southeastern': 2855, 'northwestern': 2856, 'technopark': 2857, 'ebgv': 2858, 'row': 2859, 'behind': 2860, 'stonegate': 2861, 'nokv': 2862, 'contribute': 2863, 'competence': 2864, 'employing': 2865, 'lnkd': 2866, 'bertrand': 2867, 'sciard': 2868, 'slowed': 2869, 'adbe': 2870, 'conduct': 2871, 'sekm': 2872, 'acgv': 2873, 'resulting': 2874, 'bloomberg': 2875, 'talking': 2876, 'nda': 2877, 'tgk': 2878, 'updated': 2879, 'congrats': 2880, 'fit': 2881, 'style': 2882, 'circulating': 2883, 'expires': 2884, 'uptrend': 2885, 'caller': 2886, 'controller': 2887, 'belarus': 2888, 'label': 2889, 'hanging': 2890, 'basing': 2891, 'sanofi': 2892, 'boerse': 2893, 'radar': 2894, 'valuation': 2895, 'trendline': 2896, 'tunnel': 2897, 'authorisation': 2898, 'rihko': 2899, 'benecol': 2900, 'marathon': 2901, 'fertilizer': 2902, 'spokeswoman': 2903, 'beating': 2904, 'municipal': 2905, 'proving': 2906, 'truly': 2907, 'pohs': 2908, 'subscribe': 2909, 'screen': 2910, 'uvxy': 2911, 'bangalore': 2912, 'crore': 2913, 'sending': 2914, 'stanchart': 2915, 'valley': 2916, 'strait': 2917, 'mount': 2918, 'aluminum': 2919, 'keeping': 2920, 'congress': 2921, 'arranged': 2922, 'involve': 2923, 'biomass': 2924, 'coalfired': 2925, 'voima': 2926, 'tvo': 2927, 'intercontinental': 2928, 'bus': 2929, 'lag': 2930, 'longstanding': 2931, 'ethernet': 2932, 'gdx': 2933, 'fill': 2934, 'mara': 2935, 'attractive': 2936, 'beijing': 2937, 'slash': 2938, 'intake': 2939, 'analytics': 2940, 'respective': 2941, 'glav': 2942, 'ecb': 2943, 'fitness': 2944, 'precor': 2945, 'wilson': 2946, 'leed': 2947, 'wave': 2948, 'liquefied': 2949, 'reward': 2950, 'arabia': 2951, 'stochastic': 2952, 'lattelecom': 2953, 'protein': 2954, 'turun': 2955, 'careful': 2956, 'compare': 2957, 'manchester': 2958, 'twelve': 2959, 'pursuit': 2960, 'telanne': 2961, 'mat': 2962, 'possibility': 2963, 'dwt': 2964, 'parttime': 2965, 'tape': 2966, 'particularly': 2967, 'continent': 2968, 'rev': 2969, 'otev': 2970, 'perspective': 2971, 'mode': 2972, 'tournament': 2973, 'fun': 2974, 'compass': 2975, 'asahi': 2976, 'assignment': 2977, 'commented': 2978, 'ntsb': 2979, 'bot': 2980, 'campaign': 2981, 'featuring': 2982, 'career': 2983, 'common': 2984, 'collect': 2985, 'calculates': 2986, 'complement': 2987, 'xlf': 2988, 'peab': 2989, 'english': 2990, 'language': 2991, 'varma': 2992, 'kilo': 2993, 'maturity': 2994, 'dish': 2995, 'unsecured': 2996, 'collective': 2997, 'affair': 2998, 'adjust': 2999, 'jvc': 3000, 'sarantel': 3001, 'summit': 3002, 'kilometer': 3003, 'ebay': 3004, 'bay': 3005, 'scandinavia': 3006, 'inaugurated': 3007, 'tlsn': 3008, 'md': 3009, 'chp': 3010, 'importance': 3011, 'np': 3012, 'repayment': 3013, 'brokerage': 3014, 'cv': 3015, 'scfv': 3016, 'merge': 3017, 'f': 3018, 'unilever': 3019, 'mostly': 3020, 'bigger': 3021, 'toxic': 3022, 'gld': 3023, 'vwap': 3024, 'vary': 3025, 'plaza': 3026, 'affectogenimap': 3027, 'lkab': 3028, 'hightech': 3029, 'austrian': 3030, 'excellent': 3031, 'superior': 3032, 'paying': 3033, 'painful': 3034, 'jpmorgan': 3035, 'jpm': 3036, 'singapore': 3037, 'cad': 3038, 'bulk': 3039, 'reflects': 3040, 'marked': 3041, 'hearing': 3042, 'driven': 3043, 'environmentally': 3044, 'distributed': 3045, 'filed': 3046, 'korean': 3047, 'pack': 3048, 'costcutting': 3049, 'executed': 3050, 'spending': 3051, 'v': 3052, 'proposes': 3053, 'copyright': 3054, 'januaryaugust': 3055, 'hathaway': 3056, 'reading': 3057, 'rnn': 3058, 'outcome': 3059, 'spanning': 3060, 'settle': 3061, 'reiterates': 3062, 'proceeds': 3063, 'gruppen': 3064, 'auditor': 3065, 'knowledge': 3066, 'capture': 3067, 'spent': 3068, 'lp': 3069, 'involving': 3070, 'fio': 3071, 'exhibition': 3072, 'revealed': 3073, 'lsedeutsche': 3074, 'wrong': 3075, 'reservation': 3076, 'consistency': 3077, 'pushing': 3078, 'kronor': 3079, 'lowest': 3080, 'coke': 3081, 'bb': 3082, 'fullyowned': 3083, 'repair': 3084, 'appoint': 3085, 'written': 3086, 'museum': 3087, 'separated': 3088, 'helsinkibased': 3089, 'rosen': 3090, 'cautious': 3091, 'optimistic': 3092, 'hectare': 3093, 'gasoline': 3094, 'yearly': 3095, 'secondlargest': 3096, 'usbased': 3097, 'brian': 3098, 'contractor': 3099, 'backlog': 3100, 'mumbai': 3101, 'suffering': 3102, 'strip': 3103, 'indicate': 3104, 'rigid': 3105, 'bluechip': 3106, 'chapter': 3107, 'differ': 3108, 'repeated': 3109, 'dragged': 3110, 'smart': 3111, 'reference': 3112, 'climate': 3113, 'earned': 3114, 'probably': 3115, 'schedule': 3116, 'rio': 3117, 'tinto': 3118, 'length': 3119, 'sufficient': 3120, 'immediately': 3121, 'outsourcing': 3122, 'depot': 3123, 'infra': 3124, 'integrate': 3125, 'vwr': 3126, 'closer': 3127, 'plunge': 3128, 'gmcr': 3129, 'cu': 3130, 'dart': 3131, 'rebound': 3132, 'severn': 3133, 'notably': 3134, 'cet': 3135, 'procedure': 3136, 'radiation': 3137, 'molybdenum': 3138, 'copying': 3139, 'prohibited': 3140, 'consent': 3141, 'piece': 3142, 'choose': 3143, 'cooperate': 3144, 'relates': 3145, 'qcor': 3146, 'operated': 3147, 'upper': 3148, 'fitch': 3149, 'emirate': 3150, 'crazy': 3151, 'kaukomarkkinat': 3152, 'renovation': 3153, 'nembv': 3154, 'belgian': 3155, 'seikku': 3156, 'pietarsaari': 3157, 'marketplace': 3158, 'nurminen': 3159, 'pda': 3160, 'catch': 3161, 'pa': 3162, 'enjoy': 3163, 'worry': 3164, 'flagship': 3165, 'fixedterm': 3166, 'smh': 3167, 'jyvaeskylae': 3168, 'loses': 3169, 'novartis': 3170, 'downtown': 3171, 'expensive': 3172, 'wynn': 3173, 'smoking': 3174, 'ban': 3175, 'globe': 3176, 'coverage': 3177, 'financialwire': 3178, 'mmov': 3179, 'edge': 3180, 'gsm': 3181, 'actively': 3182, 'forma': 3183, 'pull': 3184, 'founder': 3185, 'authorized': 3186, 'crossover': 3187, 'larox': 3188, 'mo': 3189, 'quote': 3190, 'uae': 3191, 'automatically': 3192, 'identical': 3193, 'pipeline': 3194, 'concludes': 3195, 'csco': 3196, 'nke': 3197, 'supervisory': 3198, 'ragot': 3199, 'waterqueen': 3200, 'tortue': 3201, 'literature': 3202, 'defends': 3203, 'heart': 3204, 'novator': 3205, 'exported': 3206, 'spokesperson': 3207, 'jrvisuomen': 3208, 'portti': 3209, 'valve': 3210, 'rated': 3211, 'rent': 3212, 'dairy': 3213, 'separating': 3214, 'weaker': 3215, 'david': 3216, 'pekka': 3217, 'lember': 3218, 'warmly': 3219, 'consistent': 3220, 'owing': 3221, 'maintains': 3222, 'streaming': 3223, 'dopplr': 3224, 'booking': 3225, 'defined': 3226, 'platen': 3227, 'funding': 3228, 'ntap': 3229, 'birch': 3230, 'ing': 3231, 'roll': 3232, 'budapest': 3233, 'element': 3234, 'fly': 3235, 'organic': 3236, 'donation': 3237, 'jouko': 3238, 'tele': 3239, 'consortium': 3240, 'distance': 3241, 'cleaning': 3242, 'insight': 3243, 'aggregate': 3244, 'strongest': 3245, 'provisioning': 3246, 'semiconductor': 3247, 'behalf': 3248, 'alexander': 3249, 'wagon': 3250, 'backup': 3251, 'dress': 3252, 'sriperumbudur': 3253, 'chennai': 3254, 'diesel': 3255, 'renewable': 3256, 'apiece': 3257, 'waiting': 3258, 'comparison': 3259, 'meal': 3260, 'forever': 3261, 'proline': 3262, 'loadbearing': 3263, 'opensource': 3264, 'tyrvinen': 3265, 'divide': 3266, 'coffee': 3267, 'denied': 3268, 'judge': 3269, 'dozen': 3270, 'hospital': 3271, 'centralized': 3272, 'ticket': 3273, 'lippupiste': 3274, 'excellence': 3275, 'doctor': 3276, 'exas': 3277, 'competing': 3278, 'wire': 3279, 'usually': 3280, 'ak': 3281, 'implies': 3282, 'parking': 3283, 'clientele': 3284, 'television': 3285, 'vauramo': 3286, 'volatile': 3287, 'stockmarket': 3288, 'nikkei': 3289, 'hang': 3290, 'lie': 3291, 'platinum': 3292, 'superstructure': 3293, 'kallio': 3294, 'tuomas': 3295, 'loader': 3296, 'viewing': 3297, 'priit': 3298, 'valmet': 3299, 'delegation': 3300, 'sitra': 3301, 'jsc': 3302, 'cnp': 3303, 'boerselse': 3304, 'retained': 3305, 'warsaw': 3306, 'popping': 3307, 'cheap': 3308, 'robot': 3309, 'tapio': 3310, 'wellness': 3311, 'featured': 3312, 'pioneer': 3313, 'runner': 3314, 'rentalrelated': 3315, 'administrative': 3316, 'wealth': 3317, 'decker': 3318, 'husqvarna': 3319, 'ryobi': 3320, 'miraclegro': 3321, 'van': 3322, 'crushed': 3323, 'anything': 3324, 'recover': 3325, 'otto': 3326, 'henrik': 3327, 'bernhard': 3328, 'compton': 3329, 'covenant': 3330, 'destia': 3331, 'inflation': 3332, 'examining': 3333, 'renewed': 3334, 'slip': 3335, 'adult': 3336, 'nyrstar': 3337, 'sulphuric': 3338, 'acid': 3339, 'flexibly': 3340, 'wwwcargoteccom': 3341, 'asx': 3342, 'cheapest': 3343, 'acinvestorblog': 3344, 'table': 3345, 'straddle': 3346, 'swedishfinnish': 3347, 'upmv': 3348, 'underperf': 3349, 'ntt': 3350, 'perkonoja': 3351, 'chf': 3352, 'stocktwits': 3353, 'upward': 3354, 'cgcbv': 3355, 'jordan': 3356, 'allocation': 3357, 'grapevine': 3358, 'costefficiency': 3359, 'meiklejohn': 3360, 'meteorology': 3361, 'zinclead': 3362, 'palmberg': 3363, 'lemcon': 3364, 'dioxide': 3365, 'telecominvest': 3366, 'argument': 3367, 'aftersales': 3368, 'volkswagen': 3369, 'audi': 3370, 'lang': 3371, 'efov': 3372, 'reel': 3373, 'baird': 3374, 'surmise': 3375, 'comfort': 3376, 'modelling': 3377, 'tailored': 3378, 'peroni': 3379, 'grolsch': 3380, 'bssoss': 3381, 'mertano': 3382, 'bnamericas': 3383, 'invitation': 3384, 'bakery': 3385, 'discounter': 3386, 'kantar': 3387, 'privately': 3388, 'session': 3389, 'meggitt': 3390, 'agro': 3391, 'warehousing': 3392, 'impairment': 3393, 'mkt': 3394, 'wkly': 3395, 'forming': 3396, 'automobile': 3397, 'polish': 3398, 'teva': 3399, 'taloustutkimus': 3400, 'badly': 3401, 'young': 3402, 'dia': 3403, 'capped': 3404, 'elsewhere': 3405, 'retreat': 3406, 'thought': 3407, 'webbased': 3408, 'tracking': 3409, 'cholesterol': 3410, 'finished': 3411, 'nausea': 3412, 'failing': 3413, 'halted': 3414, 'portal': 3415, 'weakest': 3416, 'accountant': 3417, 'columbia': 3418, 'washington': 3419, 'issuer': 3420, 'solely': 3421, 'innovator': 3422, 'exceeded': 3423, 'introduction': 3424, 'touchscreen': 3425, 'wac': 3426, 'outoteccom': 3427, 'paatela': 3428, 'moreover': 3429, 'kito': 3430, 'hoist': 3431, 'cellular': 3432, 'chicago': 3433, 'deadline': 3434, 'bbby': 3435, 'sql': 3436, 'extensively': 3437, 'orkola': 3438, 'depressed': 3439, 'adequacy': 3440, 'mformation': 3441, 'surveillance': 3442, 'infected': 3443, 'seppo': 3444, 'renovate': 3445, 'largely': 3446, 'pole': 3447, 'awardwinning': 3448, 'proved': 3449, 'succeed': 3450, 'rautalinko': 3451, 'sterling': 3452, 'peer': 3453, 'peugeot': 3454, 'sodra': 3455, 'sandberg': 3456, 'maaseudun': 3457, 'tulevaisuus': 3458, 'celular': 3459, 'pine': 3460, 'visibility': 3461, 'white': 3462, 'compatible': 3463, 'ranging': 3464, 'settlement': 3465, 'kirkkonummi': 3466, 'nivala': 3467, 'dog': 3468, 'charter': 3469, 'technically': 3470, 'megawatt': 3471, 'onetime': 3472, 'rautakirja': 3473, 'probe': 3474, 'slammed': 3475, 'pnc': 3476, 'gained': 3477, 'cm': 3478, 'deep': 3479, 'tried': 3480, 'possibly': 3481, 'easter': 3482, 'tradition': 3483, 'banco': 3484, 'movement': 3485, 'lehman': 3486, 'governmentowned': 3487, 'autotank': 3488, 'admits': 3489, 'authorization': 3490, 'click': 3491, 'projected': 3492, 'qihu': 3493, 'expire': 3494, 'pivot': 3495, 'switch': 3496, 'asbuilt': 3497, 'accommodate': 3498, 'grc': 3499, 'fabrication': 3500, 'typically': 3501, 'agent': 3502, 'awareness': 3503, 'prepare': 3504, 'urban': 3505, 'dokumculuk': 3506, 'bouncing': 3507, 'stakeholder': 3508, 'everything': 3509, 'committed': 3510, 'uranium': 3511, 'sense': 3512, 'extract': 3513, 'ford': 3514, 'surfeit': 3515, 'dolce': 3516, 'gabbana': 3517, 'declare': 3518, 'unikko': 3519, 'floral': 3520, 'invalid': 3521, 'wiio': 3522, 'silver': 3523, 'shopinshops': 3524, 'angeles': 3525, 'francisco': 3526, 'thomas': 3527, 'hoyer': 3528, 'bricolage': 3529, 'hp': 3530, 'tailormade': 3531, 'iso': 3532, 'empty': 3533, 'handler': 3534, 'favourably': 3535, 'nekoski': 3536, 'overshadowed': 3537, 'protect': 3538, 'gran': 3539, 'intellibis': 3540, 'external': 3541, 'iprint': 3542, 'csx': 3543, 'nielsen': 3544, 'closedown': 3545, 'stqv': 3546, 'peri': 3547, 'accumulating': 3548, 'unaudited': 3549, 'concerned': 3550, 'experiencing': 3551, 'consultant': 3552, 'borlnge': 3553, 'carl': 3554, 'bmo': 3555, 'kausta': 3556, 'airbaltic': 3557, 'headcount': 3558, 'bow': 3559, 'tie': 3560, 'till': 3561, 'upset': 3562, 'hence': 3563, 'balanced': 3564, 'etc': 3565, 'happens': 3566, 'roughly': 3567, 'specified': 3568, 'delight': 3569, 'recruitment': 3570, 'monster': 3571, 'advertisement': 3572, 'scenario': 3573, 'everyday': 3574, 'convergence': 3575, 'anne': 3576, 'ibb': 3577, 'bib': 3578, 'downward': 3579, 'vodafone': 3580, 'prolongation': 3581, 'volunteer': 3582, 'digicel': 3583, 'supplied': 3584, 'generating': 3585, 'papua': 3586, 'lihir': 3587, 'grappling': 3588, 'metropolitan': 3589, 'thirdparty': 3590, 'struggled': 3591, 'archipelago': 3592, 'votorantim': 3593, 'kaleva': 3594, 'kustannus': 3595, 'takeda': 3596, 'fabric': 3597, 'favor': 3598, 'flew': 3599, 'applies': 3600, 'geosolutions': 3601, 'powerful': 3602, 'ghz': 3603, 'modem': 3604, 'renesas': 3605, 'federation': 3606, 'inovio': 3607, 'whitecollar': 3608, 'terminated': 3609, 'talktalk': 3610, 'netflix': 3611, 'daiichi': 3612, 'sankyo': 3613, 'movantik': 3614, 'publ': 3615, 'sizable': 3616, 'backdrop': 3617, 'exploit': 3618, 'fancy': 3619, 'dans': 3620, 'piper': 3621, 'symantec': 3622, 'highlight': 3623, 'pure': 3624, 'strand': 3625, 'reliability': 3626, 'resolve': 3627, 'timing': 3628, 'appear': 3629, 'hack': 3630, 'nefarious': 3631, 'activated': 3632, 'clicked': 3633, 'surfer': 3634, 'mouse': 3635, 'cursor': 3636, 'rebuild': 3637, 'saav': 3638, 'uso': 3639, 'strongly': 3640, 'yy': 3641, 'newsroom': 3642, 'keywords': 3643, 'tecnomenresults': 3644, 'sevenmonth': 3645, 'ip': 3646, 'pace': 3647, 'batch': 3648, 'technological': 3649, 'pullback': 3650, 'calif': 3651, 'rad': 3652, 'damage': 3653, 'permission': 3654, 'mt': 3655, 'discus': 3656, 'scala': 3657, 'nobel': 3658, 'biocare': 3659, 'fidelity': 3660, 'ghana': 3661, 'airspace': 3662, 'recommencing': 3663, 'stranded': 3664, 'accommodation': 3665, 'observed': 3666, 'odell': 3667, 'consolidates': 3668, 'processed': 3669, 'deducted': 3670, 'finton': 3671, 'eligible': 3672, 'pypl': 3673, 'perkins': 3674, 'hoping': 3675, 'gbx': 3676, 'drillisch': 3677, 'motorcyclist': 3678, 'neteller': 3679, 'cheaper': 3680, 'tikv': 3681, 'waterborne': 3682, 'marjo': 3683, 'uruguayan': 3684, 'mercator': 3685, 'slumped': 3686, 'snowfall': 3687, 'westpac': 3688, 'frn': 3689, 'seawind': 3690, 'regal': 3691, 'kapellskar': 3692, 'paldiski': 3693, 'hat': 3694, 'die': 3695, 'neudorf': 3696, 'cadbury': 3697, 'egg': 3698, 'plug': 3699, 'aiming': 3700, 'faulty': 3701, 'rd': 3702, 'degree': 3703, 'lean': 3704, 'leaning': 3705, 'bim': 3706, 'modeling': 3707, 'shenzhen': 3708, 'kapthing': 3709, 'selloff': 3710, 'usual': 3711, 'lose': 3712, 'micron': 3713, 'suspend': 3714, 'twitter': 3715, 'linkedin': 3716, 'grpn': 3717, 'upstream': 3718, 'grocery': 3719, 'allocated': 3720, 'orcl': 3721, 'crm': 3722, 'permit': 3723, 'steelmaker': 3724, 'drought': 3725, 'increasingly': 3726, 'happen': 3727, 'merrill': 3728, 'lynch': 3729, 'lowcost': 3730, 'honored': 3731, 'ctxs': 3732, 'wider': 3733, 'rao': 3734, 'kalnapiliotauro': 3735, 'grupe': 3736, 'kalnapilistauras': 3737, 'unibrew': 3738, 'totally': 3739, 'missed': 3740, 'embroiled': 3741, 'disagreement': 3742, 'chip': 3743, 'qualcommpatented': 3744, 'vrtx': 3745, 'lo': 3746, 'anyone': 3747, 'lafarge': 3748, 'holcim': 3749, 'oao': 3750, 'lidskoe': 3751, 'pivo': 3752, 'grodno': 3753, 'decaliter': 3754, 'boycotting': 3755, 'wage': 3756, 'electrification': 3757, 'gmo': 3758, 'soy': 3759, 'genesis': 3760, 'sony': 3761, 'toy': 3762, 'salary': 3763, 'bavaria': 3764, 'industriekapital': 3765, 'priced': 3766, 'eurbn': 3767, 'powerless': 3768, 'trending': 3769, 'periodend': 3770, 'equivalent': 3771, 'turbine': 3772, 'tougher': 3773, 'substantially': 3774, 'bread': 3775, 'kauhava': 3776, 'avc': 3777, 'systemhaus': 3778, 'squeeze': 3779, 'resolved': 3780, 'secondary': 3781, 'slv': 3782, 'awful': 3783, 'gettin': 3784, 'mikkonen': 3785, 'unicom': 3786, 'nation': 3787, 'blog': 3788, 'max': 3789, 'extend': 3790, 'execute': 3791, 'midoctober': 3792, 'filmiteollisuus': 3793, 'seasonal': 3794, 'fluctuation': 3795, 'voiced': 3796, 'tj': 3797, 'war': 3798, 'skogster': 3799, 'host': 3800, 'gaining': 3801, 'brcm': 3802, 'pfizer': 3803, 'uv': 3804, 'wastewater': 3805, 'trojan': 3806, 'wrtsil': 3807, 'foodservice': 3808, 'utilisation': 3809, 'vaasa': 3810, 'cellphone': 3811, 'colorful': 3812, 'sentiment': 3813, 'hurdle': 3814, 'samas': 3815, 'handelsbanken': 3816, 'turnkey': 3817, 'roro': 3818, 'handheld': 3819, 'atom': 3820, 'cameco': 3821, 'sharp': 3822, 'seeing': 3823, 'minor': 3824, 'tumkur': 3825, 'petrofac': 3826, 'delay': 3827, 'mirabela': 3828, 'brasil': 3829, 'bulgaria': 3830, 'stress': 3831, 'physical': 3832, 'sunrise': 3833, 'kyronsalmi': 3834, 'savonlinna': 3835, 'intact': 3836, 'sd': 3837, 'lowering': 3838, 'conviction': 3839, 'peat': 3840, 'belchatow': 3841, 'intention': 3842, 'teollisuuden': 3843, 'poorest': 3844, 'shelter': 3845, 'viiv': 3846, 'chile': 3847, 'gprs': 3848, 'mm': 3849, 'chose': 3850, 'miner': 3851, 'jul': 3852, 'employer': 3853, 'health': 3854, 'nicotine': 3855, 'explore': 3856, 'evaluated': 3857, 'ca': 3858, 'sophos': 3859, 'rostelecom': 3860, 'onwards': 3861, 'lasse': 3862, 'structural': 3863, 'loaded': 3864, 'advise': 3865, 'skeptical': 3866, 'cheer': 3867, 'rocketed': 3868, 'ability': 3869, 'orangehandled': 3870, 'compete': 3871, 'stable': 3872, 'storengy': 3873, 'konglisted': 3874, 'notable': 3875, 'angler': 3876, 'constitute': 3877, 'ackman': 3878, 'bjorn': 3879, 'geographic': 3880, 'enhanced': 3881, 'imputed': 3882, 'population': 3883, 'yahoofinance': 3884, 'resume': 3885, 'tullow': 3886, 'westerlund': 3887, 'conciliator': 3888, 'salonius': 3889, 'rpk': 3890, 'ask': 3891, 'sight': 3892, 'hospitalized': 3893, 'ace': 3894, 'streak': 3895, 'discontinue': 3896, 'constructionreal': 3897, 'pohjolan': 3898, 'whollyowned': 3899, 'sab': 3900, 'wwwdailystockplayscom': 3901, 'nugt': 3902, 'hedging': 3903, 'viii': 3904, 'blackstone': 3905, 'meantime': 3906, 'diabetes': 3907, 'bar': 3908, 'youtube': 3909, 'ovi': 3910, 'cyber': 3911, 'clerical': 3912, 'chamber': 3913, 'coowner': 3914, 'bullying': 3915, 'fight': 3916, 'consisting': 3917, 'thyssenkrupp': 3918, 'otis': 3919, 'schindler': 3920, 'alleged': 3921, 'davy': 3922, 'vtmrakennuskonevuokraamo': 3923, 'whilst': 3924, 'permanently': 3925, 'guard': 3926, 'banned': 3927, 'mooring': 3928, 'carshipping': 3929, 'railroadcar': 3930, 'checkpoint': 3931, 'erdenet': 3932, 'notification': 3933, 'shedding': 3934, 'trim': 3935, 'remotely': 3936, 'fennia': 3937, 'achieved': 3938, 'tornio': 3939, 'fixedperiod': 3940, 'apriloctober': 3941, 'issuance': 3942, 'processguide': 3943, 'xpress': 3944, 'flooring': 3945, 'bias': 3946, 'ra': 3947, 'detroit': 3948, 'mkel': 3949, 'ray': 3950, 'ostrom': 3951, 'lake': 3952, 'sporting': 3953, 'steadily': 3954, 'ruble': 3955, 'eaten': 3956, 'snap': 3957, 'bochum': 3958, 'helping': 3959, 'critical': 3960, 'button': 3961, 'subsequent': 3962, 'detection': 3963, 'intermittently': 3964, 'spirit': 3965, 'sink': 3966, 'evening': 3967, 'solo': 3968, 'brother': 3969, 'pulse': 3970, 'flexi': 3971, 'variant': 3972, 'finishing': 3973, 'niam': 3974, 'orhangazi': 3975, 'hedge': 3976, 'shimano': 3977, 'exclusive': 3978, 'veidekke': 3979, 'recipient': 3980, 'sel': 3981, 'hasten': 3982, 'bargaining': 3983, 'castle': 3984, 'entitled': 3985, 'flagging': 3986, 'parnu': 3987, 'nothing': 3988, 'else': 3989, 'innofactor': 3990, 'loviisa': 3991, 'trainer': 3992, 'downtrend': 3993, 'stiff': 3994, 'dvd': 3995, 'bristol': 3996, 'sealed': 3997, 'cooper': 3998, 'african': 3999, 'thin': 4000, 'vanhanen': 4001, 'damaging': 4002, 'arrive': 4003, 'oneday': 4004, 'putin': 4005, 'mile': 4006, 'rare': 4007, 'yelp': 4008, 'freight': 4009, 'disappoint': 4010, 'comparative': 4011, 'taste': 4012, 'opus': 4013, 'ddd': 4014, 'vartan': 4015, 'entirety': 4016, 'dagens': 4017, 'industri': 4018, 'affarsvarlden': 4019, 'activision': 4020, 'plummeted': 4021, 'validating': 4022, 'noncore': 4023, 'mountain': 4024, 'stifel': 4025, 'gilligan': 4026, 'vianor': 4027, 'participating': 4028, 'allotted': 4029, 'cjsc': 4030, 'geberit': 4031, 'blaming': 4032, 'respiratory': 4033, 'select': 4034, 'icoa': 4035, 'ojalayhtyma': 4036, 'snag': 4037, 'keyboard': 4038, 'scene': 4039, 'effected': 4040, 'pesonen': 4041, 'trace': 4042, 'osmium': 4043, 'tetroxide': 4044, 'permeri': 4045, 'northernmost': 4046, 'golden': 4047, 'rinkuskiai': 4048, 'kauno': 4049, 'halved': 4050, 'throughput': 4051, 'inaugural': 4052, 'iconic': 4053, 'spend': 4054, 'letting': 4055, 'excited': 4056, 'maxi': 4057, 'joe': 4058, 'uncommonly': 4059, 'roger': 4060, 'talermo': 4061, 'availability': 4062, 'appears': 4063, 'entity': 4064, 'undertaking': 4065, 'aleksandri': 4066, 'roundtable': 4067, 'rspo': 4068, 'lietuva': 4069, 'inclusive': 4070, 'ofcom': 4071, 'sourcing': 4072, 'mentioned': 4073, 'anode': 4074, 'visual': 4075, 'purchasing': 4076, 'paperboard': 4077, 'ecommerce': 4078, 'heavy': 4079, 'reactor': 4080, 'hydro': 4081, 'diversified': 4082, 'gts': 4083, 'friendly': 4084, 'hunt': 4085, 'kesbv': 4086, 'yearonyea': 4087, 'krav': 4088, 'bwld': 4089, 'delivering': 4090, 'enefit': 4091, 'workshop': 4092, 'dow': 4093, 'interview': 4094, 'hardest': 4095, 'separately': 4096, 'jim': 4097, 'worsethanexpected': 4098, 'altona': 4099, 'kevin': 4100, 'entitlement': 4101, 'labu': 4102, 'fluid': 4103, 'bed': 4104, 'calcination': 4105, 'rock': 4106, 'climbed': 4107, 'criticised': 4108, 'lorry': 4109, 'alpina': 4110, 'peltonen': 4111, 'impacted': 4112, 'aberdeen': 4113, 'mastered': 4114, 'bestseller': 4115, 'treat': 4116, 'perfect': 4117, 'coordinate': 4118, 'recruit': 4119, 'settop': 4120, 'citi': 4121, 'agree': 4122, 'hr': 4123, 'inorganic': 4124, 'coagulant': 4125, 'andhra': 4126, 'pradesh': 4127, 'ameas': 4128, 'highest': 4129, 'sparebank': 4130, 'reimbursed': 4131, 'contribution': 4132, 'tell': 4133, 'branded': 4134, 'stateowned': 4135, 'kauhajoki': 4136, 'hostile': 4137, 'syndicate': 4138, 'underperform': 4139, 'tza': 4140, 'iran': 4141, 'hybrid': 4142, 'ftth': 4143, 'sastamala': 4144, 'belief': 4145, 'rerouting': 4146, 'midmarket': 4147, 'headboxes': 4148, 'plasterboard': 4149, 'bilfinger': 4150, 'neighbouring': 4151, 'acando': 4152, 'acanb': 4153, 'gerber': 4154, 'buster': 4155, 'knife': 4156, 'staying': 4157, 'approx': 4158, 'sanomawsoy': 4159, 'culture': 4160, 'addon': 4161, 'considers': 4162, 'canadian': 4163, 'belvedere': 4164, 'lumene': 4165, 'speeded': 4166, 'sevenyear': 4167, 'ignored': 4168, 'icy': 4169, 'teks': 4170, 'amro': 4171, 'alphabet': 4172, 'courthouse': 4173, 'saltonstall': 4174, 'drying': 4175, 'elect': 4176, 'aho': 4177, 'conjunction': 4178, 'pirkka': 4179, 'hoped': 4180, 'patricia': 4181, 'distinctive': 4182, 'premarket': 4183, 'saarelainen': 4184, 'shorting': 4185, 'depositary': 4186, 'smelt': 4187, 'spout': 4188, 'atomic': 4189, 'supercalendered': 4190, 'newsprint': 4191, 'era': 4192, 'expo': 4193, 'signature': 4194, 'surfing': 4195, 'reducing': 4196, 'marketfinanced': 4197, 'yndx': 4198, 'showing': 4199, 'tecv': 4200, 'living': 4201, 'maintain': 4202, 'thief': 4203, 'stealing': 4204, 'oneworld': 4205, 'amr': 4206, 'salo': 4207, 'pivotal': 4208, 'pile': 4209, 'cerberus': 4210, 'lpbacked': 4211, 'cat': 4212, 'manyears': 4213, 'vdw': 4214, 'perform': 4215, 'scanning': 4216, 'ladle': 4217, 'aftermarket': 4218, 'bulletin': 4219, 'retirement': 4220, 'festive': 4221, 'tightened': 4222, 'thicker': 4223, 'aaland': 4224, 'innova': 4225, 'boomeranger': 4226, 'wig': 4227, 'siri': 4228, 'speaker': 4229, 'millercoors': 4230, 'assa': 4231, 'abloy': 4232, 'lock': 4233, 'umts': 4234, 'do': 4235, 'administered': 4236, 'psoriasis': 4237, 'season': 4238, 'von': 4239, 'koskull': 4240, 'kiosk': 4241, 'suffered': 4242, 'retest': 4243, 'ooo': 4244, 'constructor': 4245, 'profiling': 4246, 'ostrava': 4247, 'relocated': 4248, 'rost': 4249, 'exxon': 4250, 'santos': 4251, 'angola': 4252, 'kaunas': 4253, 'mezzanine': 4254, 'steve': 4255, 'indepth': 4256, 'studying': 4257, 'competency': 4258, 'hargreaves': 4259, 'lansdown': 4260, 'isolationism': 4261, 'raivv': 4262, 'felt': 4263, 'kemijrvi': 4264, 'direction': 4265, 'rout': 4266, 'whitehall': 4267, 'productionrelated': 4268, 'seppl': 4269, 'sdav': 4270, 'dimension': 4271, 'vuosaari': 4272, 'eliv': 4273, 'tallinnbased': 4274, 'aero': 4275, 'rocket': 4276, 'restriction': 4277, 'reenter': 4278, 'bps': 4279, 'connect': 4280, 'bbl': 4281, 'dark': 4282, 'horizon': 4283, 'bottled': 4284, 'lawsuit': 4285, 'wysockiego': 4286, 'mcd': 4287, 'encourage': 4288, 'latvijas': 4289, 'finieris': 4290, 'successor': 4291, 'resignation': 4292, 'pakistan': 4293, 'taiwan': 4294, 'inch': 4295, 'unveils': 4296, 'becomes': 4297, 'continuous': 4298, 'liechtenstein': 4299, 'sna': 4300, 'gmt': 4301, 'republication': 4302, 'redistribution': 4303, 'framing': 4304, 'expressly': 4305, 'consider': 4306, 'citadele': 4307, 'genetically': 4308, 'engineered': 4309, 'barc': 4310, 'bcs': 4311, 'confident': 4312, 'eqt': 4313, 'nem': 4314, 'folded': 4315, 'newswire': 4316, 'hydrocarbon': 4317, 'mix': 4318, 'ema': 4319, 'injectable': 4320, 'lakshmi': 4321, 'mittal': 4322, 'weakening': 4323, 'rough': 4324, 'pankki': 4325, 'occupied': 4326, 'ovum': 4327, 'kauniskangas': 4328, 'januarynovember': 4329, 'yliopistonrinne': 4330, 'grows': 4331, 'foresees': 4332, 'breakeven': 4333, 'mylan': 4334, 'boosting': 4335, 'apetit': 4336, 'crystal': 4337, 'saved': 4338, 'winning': 4339, 'offtake': 4340, 'extraction': 4341, 'virala': 4342, 'leakage': 4343, 'gypsum': 4344, 'pond': 4345, 'detected': 4346, 'tapiola': 4347, 'weighing': 4348, 'dosing': 4349, 'ease': 4350, 'alholma': 4351, 'midmay': 4352, 'januarydecember': 4353, 'honka': 4354, 'lmt': 4355, 'activate': 4356, 'bc': 4357, 'fh': 4358, 'carolina': 4359, 'completely': 4360, 'matrix': 4361, 'complexity': 4362, 'routine': 4363, 'leaf': 4364, 'normally': 4365, 'thirdrow': 4366, 'belt': 4367, 'sdm': 4368, 'scaffolding': 4369, 'generator': 4370, 'squarefoot': 4371, 'owning': 4372, 'confirmation': 4373, 'incentive': 4374, 'easier': 4375, 'scania': 4376, 'pellet': 4377, 'kiruna': 4378, 'metallurgical': 4379, 'dubai': 4380, 'roi': 4381, 'discontinued': 4382, 'ax': 4383, 'disappointing': 4384, 'pcb': 4385, 'reflected': 4386, 'spx': 4387, 'headed': 4388, 'gray': 4389, 'reshuffled': 4390, 'rim': 4391, 'blackberry': 4392, 'swap': 4393, 'risen': 4394, 'franchising': 4395, 'nike': 4396, 'leadership': 4397, 'toolonlahti': 4398, 'disclosing': 4399, 'kostroma': 4400, 'exporter': 4401, 'stability': 4402, 'stateapproved': 4403, 'sentry': 4404, 'immersive': 4405, 'mediterranean': 4406, 'replumbing': 4407, 'bathroom': 4408, 'biscuit': 4409, 'voluntary': 4410, 'ilmn': 4411, 'uruguay': 4412, 'egm': 4413, 'koskinen': 4414, 'offset': 4415, 'broader': 4416, 'actor': 4417, 'stamp': 4418, 'recovered': 4419, 'newest': 4420, 'salaried': 4421, 'doring': 4422, 'eaton': 4423, 'acted': 4424, 'midstrength': 4425, 'amounting': 4426, 'interconnection': 4427, 'demonstrates': 4428, 'diagnostics': 4429, 'liking': 4430, 'rumour': 4431, 'ornav': 4432, 'bluecollar': 4433, 'shock': 4434, 'blame': 4435, 'degerfors': 4436, 'parliament': 4437, 'pershare': 4438, 'diapol': 4439, 'onto': 4440, 'vicinity': 4441, 'slam': 4442, 'advice': 4443, 'raising': 4444, 'eod': 4445, 'appealing': 4446, 'sihvo': 4447, 'succeeded': 4448, 'dibba': 4449, 'altia': 4450, 'additionally': 4451, 'diminished': 4452, 'vsevolozhsk': 4453, 'leningrad': 4454, 'sister': 4455, 'wcdma': 4456, 'noop': 4457, 'mikko': 4458, 'facing': 4459, 'juniper': 4460, 'cisco': 4461, 'jnpr': 4462, 'sabah': 4463, 'twist': 4464, 'sixyear': 4465, 'lesprom': 4466, 'feel': 4467, 'sponsoring': 4468, 'admiral': 4469, 'sued': 4470, 'settled': 4471, 'supposed': 4472, 'calling': 4473, 'specifically': 4474, 'hacking': 4475, 'jv': 4476, 'inked': 4477, 'mature': 4478, 'cxe': 4479, 'dinh': 4480, 'cellulose': 4481, 'text': 4482, 'gainer': 4483, 'wdc': 4484, 'morris': 4485, 'bell': 4486, 'debenture': 4487, 'flsmidth': 4488, 'violated': 4489, 'tenth': 4490, 'rapv': 4491, 'evaluation': 4492, 'runup': 4493, 'valio': 4494, 'worried': 4495, 'sludge': 4496, 'ink': 4497, 'norvestia': 4498, 'jwn': 4499, 'departure': 4500, 'shipbuilder': 4501, 'dpz': 4502, 'borrower': 4503, 'galvan': 4504, 'longs': 4505, 'height': 4506, 'investigate': 4507, 'walden': 4508, 'weighs': 4509, 'horizontal': 4510, 'rig': 4511, 'sveza': 4512, 'assumption': 4513, 'pipettors': 4514, 'investigator': 4515, 'fesco': 4516, 'maturing': 4517, 'varpaisj': 4518, 'rvi': 4519, 'emsa': 4520, 'juri': 4521, 'heard': 4522, 'rehu': 4523, 'earliest': 4524, 'telko': 4525, 'leipurin': 4526, 'generally': 4527, 'baltimore': 4528, 'police': 4529, 'madoff': 4530, 'ponzi': 4531, 'outsource': 4532, 'cuttolength': 4533, 'zao': 4534, 'surrounding': 4535, 'fxe': 4536, 'varesvuo': 4537, 'canal': 4538, 'inclusion': 4539, 'enclosed': 4540, 'basketball': 4541, 'hockey': 4542, 'decide': 4543, 'cba': 4544, 'amsterdam': 4545, 'converting': 4546, 'marubeni': 4547, 'appeal': 4548, 'judgement': 4549, 'silja': 4550, 'attribute': 4551, 'helsinkistockholm': 4552, 'title': 4553, 'governance': 4554, 'fork': 4555, 'unfortunately': 4556, 'somaxon': 4557, 'nalmefene': 4558, 'cessation': 4559, 'toward': 4560, 'reshape': 4561, 'sir': 4562, 'watermark': 4563, 'slovak': 4564, 'boot': 4565, 'architecture': 4566, 'petrochemical': 4567, 'catering': 4568, 'lahtinen': 4569, 'petition': 4570, 'url': 4571, 'tomtom': 4572, 'garmin': 4573, 'netapp': 4574, 'ratingsnetwork': 4575, 'reliable': 4576, 'midnighttrader': 4577, 'arto': 4578, 'born': 4579, 'vologda': 4580, 'overlaid': 4581, 'bodily': 4582, 'injury': 4583, 'quadrupled': 4584, 'affecting': 4585, 'ingredient': 4586, 'slaughtering': 4587, 'philip': 4588, 'pecs': 4589, 'zoltan': 4590, 'krippl': 4591, 'refinance': 4592, 'carriage': 4593, 'shared': 4594, 'logging': 4595, 'gantry': 4596, 'companiesandmarketscom': 4597, 'beta': 4598, 'crash': 4599, 'progressed': 4600, 'ietf': 4601, 'rfc': 4602, 'bioc': 4603, 'tariff': 4604, 'lidl': 4605, 'kraftliner': 4606, 'originally': 4607, 'triggered': 4608, 'uncertain': 4609, 'bk': 4610, 'grocer': 4611, 'residue': 4612, 'feedftse': 4613, 'mover': 4614, 'northwest': 4615, 'accused': 4616, 'bancorp': 4617, 'favorable': 4618, 'request': 4619, 'lye': 4620, 'leak': 4621, 'occurred': 4622, 'sun': 4623, 'suitor': 4624, 'ruled': 4625, 'distributable': 4626, 'cabot': 4627, 'michelin': 4628, 'upgrading': 4629, 'mind': 4630, 'candle': 4631, 'student': 4632, 'thru': 4633, 'occupies': 4634, 'laine': 4635, 'pulkovo': 4636, 'emphasis': 4637, 'reserved': 4638, 'shipbuilding': 4639, 'race': 4640, 'accelerate': 4641, 'papermaking': 4642, 'activation': 4643, 'intuitive': 4644, 'deere': 4645, 'invoicing': 4646, 'djia': 4647, 'conversion': 4648, 'burrill': 4649, 'blow': 4650, 'speaking': 4651, 'tiev': 4652, 'plunged': 4653, 'entirely': 4654, 'labor': 4655, 'mike': 4656, 'kpi': 4657, 'smithfield': 4658, 'crossing': 4659, 'mediumterm': 4660, 'amex': 4661, 'plx': 4662, 'evraz': 4663, 'bahia': 4664, 'harvesting': 4665, 'efi': 4666, 'simmons': 4667, 'accumulate': 4668, 'damaged': 4669, 'criticising': 4670, 'cry': 4671, 'disclosure': 4672, 'runway': 4673, 'edging': 4674, 'seinajoki': 4675, 'southwestern': 4676, 'welding': 4677, 'svyturysutenos': 4678, 'bbh': 4679, 'school': 4680, 'gpro': 4681, 'swiss': 4682, 'astana': 4683, 'fl': 4684, 'jari': 4685, 'oversupply': 4686, 'pharmacy': 4687, 'interaction': 4688, 'cohen': 4689, 'steer': 4690, 'roce': 4691, 'magnetite': 4692, 'metric': 4693, 'arokarhu': 4694, 'scanned': 4695, 'disappeared': 4696, 'pressed': 4697, 'tyrvn': 4698, 'replaced': 4699, 'sav': 4700, 'sentera': 4701, 'guangdong': 4702, 'qingyuan': 4703, 'polytechnic': 4704, 'victoria': 4705, 'osuuskunta': 4706, 'commissioning': 4707, 'fluctuated': 4708, 'unstable': 4709, 'vvus': 4710, 'raid': 4711, 'ky': 4712, 'automated': 4713, 'spokesman': 4714, 'apparent': 4715, 'ev': 4716, 'tacked': 4717, 'laasanen': 4718, 'electric': 4719, 'classic': 4720, 'navigator': 4721, 'adam': 4722, 'pcbs': 4723, 'facade': 4724, 'norsun': 4725, 'denominated': 4726, 'coscom': 4727, 'lat': 4728, 'tanzania': 4729, 'sanitation': 4730, 'cien': 4731, 'pertti': 4732, 'copy': 4733, 'stoppage': 4734, 'stkscojqt': 4735, 'samporosenlew': 4736, 'ordabasy': 4737, 'realistic': 4738, 'lemiste': 4739, 'marko': 4740, 'djsi': 4741, 'continental': 4742, 'peigs': 4743, 'wwwpeigsse': 4744, 'sardus': 4745, 'latta': 4746, 'maltider': 4747, 'purdy': 4748, 'agu': 4749, 'teflon': 4750, 'adjustable': 4751, 'multichannel': 4752, 'singlechannel': 4753, 'eronen': 4754, 'assume': 4755, 'ugglarp': 4756, 'skne': 4757, 'residentialconstruction': 4758, 'communal': 4759, 'spencer': 4760, 'acom': 4761, 'stkscogx': 4762, 'sanyo': 4763, 'siga': 4764, 'reargument': 4765, 'sponsorship': 4766, 'lukkoexpert': 4767, 'kimmo': 4768, 'uusimaki': 4769, 'sukhraj': 4770, 'dulai': 4771, 'boni': 4772, 'culdesac': 4773, 'garage': 4774, 'chaim': 4775, 'katzman': 4776, 'dissolved': 4777, 'interavanti': 4778, 'ct': 4779, 'eventim': 4780, 'pietinalho': 4781, 'motivate': 4782, 'smoke': 4783, 'exact': 4784, 'flagged': 4785, 'stkscorra': 4786, 'ergo': 4787, 'nugget': 4788, 'doubledeck': 4789, 'width': 4790, 'mmin': 4791, 'boxboard': 4792, 'soullor': 4793, 'scrappy': 4794, 'swy': 4795, 'vale': 4796, 'nbl': 4797, 'tcovttzpis': 4798, 'krone': 4799, 'rebounding': 4800, 'worst': 4801, 'dj': 4802, 'inspires': 4803, 'quatrocon': 4804, 'rakvere': 4805, 'concentrated': 4806, 'meatpacking': 4807, 'imaging': 4808, 'tcotckqznmiqx': 4809, 'tcofemhjtamqj': 4810, 'pickup': 4811, 'kitd': 4812, 'seng': 4813, 'relief': 4814, 'lewis': 4815, 'treating': 4816, 'pilanesberg': 4817, 'refilehikma': 4818, 'partihallsforbindelsen': 4819, 'sahlberg': 4820, 'teppo': 4821, 'mustonen': 4822, 'marking': 4823, 'demonstrated': 4824, 'aava': 4825, 'multiwindow': 4826, 'kasak': 4827, 'balti': 4828, 'metsamasina': 4829, 'rakverebased': 4830, 'slice': 4831, 'kazakh': 4832, 'coca': 4833, 'cola': 4834, 'stkscofyco': 4835, 'stkscoghjc': 4836, 'gatecrash': 4837, 'obligated': 4838, 'isk': 4839, 'boeing': 4840, 'bet': 4841, 'expiry': 4842, 'stkscotcse': 4843, 'lgpl': 4844, 'plccompany': 4845, 'pursuant': 4846, 'eli': 4847, 'lilly': 4848, 'lly': 4849, 'productive': 4850, 'pocketable': 4851, 'netbooks': 4852, 'mediaphones': 4853, 'connected': 4854, 'invehicle': 4855, 'infotainment': 4856, 'powerplant': 4857, 'patrik': 4858, 'flykt': 4859, 'alakoski': 4860, 'suihko': 4861, 'nadarajah': 4862, 'asokan': 4863, 'internettype': 4864, 'raty': 4865, 'archicad': 4866, 'popularity': 4867, 'ethibel': 4868, 'innovest': 4869, 'avena': 4870, 'optimising': 4871, 'indosat': 4872, 'tentative': 4873, 'julius': 4874, 'baer': 4875, 'emerge': 4876, 'bidder': 4877, 'tapeks': 4878, 'noma': 4879, 'zanadvorov': 4880, 'utah': 4881, 'wanted': 4882, 'skiequipment': 4883, 'turkcell': 4884, 'cukurova': 4885, 'luxembourg': 4886, 'mastercard': 4887, 'kior': 4888, 'god': 4889, 'imagine': 4890, 'onestopshop': 4891, 'originated': 4892, 'uncoated': 4893, 'mcc': 4894, 'yinhe': 4895, 'provisional': 4896, 'establishing': 4897, 'pill': 4898, 'aulasmaa': 4899, 'slo': 4900, 'kiilto': 4901, 'toptronics': 4902, 'normark': 4903, 'pellonpaja': 4904, 'mansner': 4905, 'feasible': 4906, 'battle': 4907, 'okay': 4908, 'happening': 4909, 'leaching': 4910, 'stkscosiyn': 4911, 'nod': 4912, 'indication': 4913, 'handoff': 4914, 'wife': 4915, 'paycheck': 4916, 'quipped': 4917, 'sixknot': 4918, 'swirled': 4919, 'anchored': 4920, 'heavily': 4921, 'sponsored': 4922, 'sled': 4923, 'halonen': 4924, 'receiving': 4925, 'luxury': 4926, 'onefamily': 4927, 'nsr': 4928, 'viscaria': 4929, 'adak': 4930, 'avalon': 4931, 'avi': 4932, 'curbed': 4933, 'racked': 4934, 'sidewalk': 4935, 'pip': 4936, 'actelion': 4937, 'premedia': 4938, 'nasdboy': 4939, 'dnf': 4940, 'movewill': 4941, 'guaranteed': 4942, 'aeronautic': 4943, 'eads': 4944, 'docomo': 4945, 'juergen': 4946, 'converged': 4947, 'reseller': 4948, 'brando': 4949, 'kbkiwi': 4950, 'amended': 4951, 'stkscojqw': 4952, 'linkspans': 4953, 'morocco': 4954, 'retailing': 4955, 'vvauto': 4956, 'brkb': 4957, 'rendered': 4958, 'strawberryrhubarb': 4959, 'applepear': 4960, 'maijaliisa': 4961, 'friman': 4962, 'rebate': 4963, 'feedback': 4964, 'rpm': 4965, 'forum': 4966, 'userexperience': 4967, 'barronsonline': 4968, 'tcobdneknl': 4969, 'doom': 4970, 'previousyear': 4971, 'maritim': 4972, 'tekmanni': 4973, 'forssan': 4974, 'betoni': 4975, 'suonenjoen': 4976, 'betonituote': 4977, 'disappear': 4978, 'downplay': 4979, 'conceptual': 4980, 'convincing': 4981, 'organisational': 4982, 'quarterlyannual': 4983, 'fruit': 4984, 'designated': 4985, 'bollore': 4986, 'societe': 4987, 'dexploitation': 4988, 'du': 4989, 'vridi': 4990, 'setv': 4991, 'abidjan': 4992, 'ivory': 4993, 'maglan': 4994, 'fairpoint': 4995, 'stkscoseqc': 4996, 'frp': 4997, 'crecent': 4998, 'qtrs': 4999, 'cant': 5000, 'theyre': 5001, 'natlabs': 5002, 'lcs': 5003, 'respecta': 5004, 'samuel': 5005, 'koivisto': 5006, 'milan': 5007, 'ansa': 5008, 'hutchison': 5009, 'essar': 5010, 'jortikka': 5011, 'fi': 5012, 'keyword': 5013, 'instance': 5014, 'repeating': 5015, 'homepage': 5016, 'wwworionfi': 5017, 'wwwkauppalehtifilive': 5018, 'negotiate': 5019, 'definitive': 5020, 'charting': 5021, 'karhinen': 5022, 'exciting': 5023, 'ad': 5024, 'andrew': 5025, 'marketbased': 5026, 'evli': 5027, 'epi': 5028, 'interchange': 5029, 'editorsinchief': 5030, 'onetenth': 5031, 'preceding': 5032, 'raiffeisen': 5033, 'mflx': 5034, 'pre': 5035, 'filled': 5036, 'lookin': 5037, 'opt': 5038, 'tomi': 5039, 'laamanen': 5040, 'prediction': 5041, 'faith': 5042, 'dandyduct': 5043, 'harmongreg': 5044, 'inverse': 5045, 'stkscoipvj': 5046, 'einvoices': 5047, 'shipowner': 5048, 'grieg': 5049, 'vehvilainen': 5050, 'requisition': 5051, 'conditioning': 5052, 'cooling': 5053, 'heineken': 5054, 'hartwall': 5055, 'hundred': 5056, 'chroark': 5057, 'unxl': 5058, 'stkscoatglooks': 5059, 'shifted': 5060, 'simonson': 5061, 'lowend': 5062, 'generic': 5063, 'heartburn': 5064, 'igrid': 5065, 'synonymous': 5066, 'tendency': 5067, 'fishy': 5068, 'dailymail': 5069, 'optimize': 5070, 'julia': 5071, 'prohaska': 5072, 'forthcoming': 5073, 'restarts': 5074, 'mildly': 5075, 'elevated': 5076, 'fourmonth': 5077, 'intervention': 5078, 'chartlynospq': 5079, 'ia': 5080, 'dax': 5081, 'yr': 5082, 'xle': 5083, 'watchlist': 5084, 'applicable': 5085, 'alberta': 5086, 'companiesdixons': 5087, 'countervalue': 5088, 'yhteishyv': 5089, 'helsingin': 5090, 'brisker': 5091, 'commissioner': 5092, 'kurt': 5093, 'svensson': 5094, 'rockwell': 5095, 'collins': 5096, 'deck': 5097, 'tonight': 5098, 'schwalm': 5099, 'unite': 5100, 'intend': 5101, 'mhs': 5102, 'aforementioned': 5103, 'lassi': 5104, 'noponen': 5105, 'ahola': 5106, 'byline': 5107, 'moran': 5108, 'loom': 5109, 'gazprom': 5110, 'fivestorey': 5111, 'ecoefficient': 5112, 'passed': 5113, 'delta': 5114, 'dji': 5115, 'cvx': 5116, 'tcowmypnomjf': 5117, 'sunnuntai': 5118, 'carlshamn': 5119, 'mejeri': 5120, 'tcooaloirijl': 5121, 'thestreet': 5122, 'hkan': 5123, 'dahlstrm': 5124, 'eightfold': 5125, 'tb': 5126, 'doubledigit': 5127, 'stkscoirf': 5128, 'anrak': 5129, 'pelletizing': 5130, 'sintering': 5131, 'prosition': 5132, 'itg': 5133, 'saku': 5134, 'ruin': 5135, 'phonebook': 5136, 'merisatama': 5137, 'corner': 5138, 'asfaltti': 5139, 'osakeyhti': 5140, 'emergency': 5141, 'assurance': 5142, 'aaron': 5143, 'moss': 5144, 'cashcow': 5145, 'tlt': 5146, 'basel': 5147, 'buoyed': 5148, 'stkscooi': 5149, 'smi': 5150, 'interoperability': 5151, 'standardsbased': 5152, 'tcotohleqfk': 5153, 'tcoprcbtvvy': 5154, 'parvi': 5155, 'execution': 5156, 'electrowattekono': 5157, 'fr': 5158, 'fixing': 5159, 'homebase': 5160, 'nonprofitable': 5161, 'alpine': 5162, 'floorball': 5163, 'stick': 5164, 'radome': 5165, 'zip': 5166, 'warned': 5167, 'compagnie': 5168, 'financement': 5169, 'foncier': 5170, 'binkster': 5171, 'downloadable': 5172, 'instruction': 5173, 'instructional': 5174, 'everywhere': 5175, 'punch': 5176, 'wwwfiskarscom': 5177, 'rifd': 5178, 'ideal': 5179, 'olli': 5180, 'saarinen': 5181, 'supervisor': 5182, 'southfield': 5183, 'mich': 5184, 'patronage': 5185, 'waiving': 5186, 'tune': 5187, 'tcohfklqwobv': 5188, 'juhapekka': 5189, 'weckstrm': 5190, 'esa': 5191, 'wore': 5192, 'beanietype': 5193, 'jacket': 5194, 'nonprofit': 5195, 'interbank': 5196, 'camara': 5197, 'interbancaria': 5198, 'pagamentos': 5199, 'cip': 5200, 'stiggoran': 5201, 'army': 5202, 'unify': 5203, 'kymi': 5204, 'detector': 5205, 'precipitation': 5206, 'intensity': 5207, 'locatrix': 5208, 'locate': 5209, 'consist': 5210, 'liftaway': 5211, 'weatherdeck': 5212, 'ocwen': 5213, 'dial': 5214, 'companiesactelion': 5215, 'unbelievably': 5216, 'thorwoste': 5217, 'forging': 5218, 'tomhend': 5219, 'flush': 5220, 'tcojuhtsm': 5221, 'confirms': 5222, 'millilitre': 5223, 'screw': 5224, 'joined': 5225, 'subdued': 5226, 'stkscopa': 5227, 'piloting': 5228, 'compliance': 5229, 'petrobras': 5230, 'bribery': 5231, 'gww': 5232, 'timely': 5233, 'coincident': 5234, 'tcombkhbkzk': 5235, 'kipa': 5236, 'giraffe': 5237, 'handpainted': 5238, 'resin': 5239, 'treble': 5240, 'constructive': 5241, 'manner': 5242, 'stole': 5243, 'pinned': 5244, 'totalguru': 5245, 'dnn': 5246, 'portland': 5247, 'dunelm': 5248, 'preempt': 5249, 'mohammed': 5250, 'zainalabedin': 5251, 'zain': 5252, 'bahrain': 5253, 'exactly': 5254, 'weekday': 5255, 'baker': 5256, 'remind': 5257, 'wheat': 5258, 'rice': 5259, 'pie': 5260, 'explores': 5261, 'refileaviva': 5262, 'tcoyfajcusqa': 5263, 'palfinger': 5264, 'settling': 5265, 'bros': 5266, 'trustee': 5267, 'espoon': 5268, 'kaupunki': 5269, 'karputer': 5270, 'satellite': 5271, 'wwwiciscom': 5272, 'anheuserbusch': 5273, 'multistorey': 5274, 'carpark': 5275, 'vegastrader': 5276, 'vetr': 5277, 'indicating': 5278, 'tcovvpzqcxsnj': 5279, 'winded': 5280, 'roving': 5281, 'oi': 5282, 'moteltan': 5283, 'bto': 5284, 'ish': 5285, 'reinforced': 5286, 'interference': 5287, 'noise': 5288, 'tour': 5289, 'empresa': 5290, 'desarrollo': 5291, 'urbano': 5292, 'quito': 5293, 'innovaruio': 5294, 'ecuador': 5295, 'mass': 5296, 'transit': 5297, 'frontedge': 5298, 'maximizes': 5299, 'resilience': 5300, 'seamless': 5301, 'tcovtppzyuy': 5302, 'mouth': 5303, 'yrend': 5304, 'hun': 5305, 'herelong': 5306, 'switchboard': 5307, 'directory': 5308, 'colleague': 5309, 'zsl': 5310, 'lanxess': 5311, 'cw': 5312, 'hcp': 5313, 'gon': 5314, 'na': 5315, 'hpq': 5316, 'stkscophz': 5317, 'je': 5318, 'finger': 5319, 'crossed': 5320, 'propeller': 5321, 'gear': 5322, 'outv': 5323, 'naturally': 5324, 'occurring': 5325, 'candidate': 5326, 'readership': 5327, 'companiesfresnillo': 5328, 'predict': 5329, 'skanssi': 5330, 'companieskingfisher': 5331, 'rptold': 5332, 'talvik': 5333, 'hu': 5334, 'omena': 5335, 'matinkyla': 5336, 'inkinen': 5337, 'galerie': 5338, 'podcast': 5339, 'harple': 5340, 'lowdown': 5341, 'demographic': 5342, 'contextual': 5343, 'loudeye': 5344, 'hacker': 5345, 'virus': 5346, 'spyware': 5347, 'kalinisky': 5348, 'difficulty': 5349, 'brewing': 5350, 'expressed': 5351, 'markedly': 5352, 'timberrrr': 5353, 'tcomrbgfdhhva': 5354, 'kgc': 5355, 'stkscoqta': 5356, 'wedge': 5357, 'divergence': 5358, 'pad': 5359, 'passengerrelated': 5360, 'ipv': 5361, 'orphan': 5362, 'cnnbrk': 5363, 'complemented': 5364, 'gdp': 5365, 'tcoeafbvuqgp': 5366, 'klo': 5367, 'grate': 5368, 'cision': 5369, 'peace': 5370, 'todaytomorrow': 5371, 'multiyear': 5372, 'supreme': 5373, 'upholds': 5374, 'jury': 5375, 'exxonmobil': 5376, 'dmasia': 5377, 'asmobile': 5378, 'digitalmediaasiacom': 5379, 'mull': 5380, 'tamil': 5381, 'nadu': 5382, 'mitk': 5383, 'dive': 5384, 'stkscoffn': 5385, 'marchapril': 5386, 'kirst': 5387, 'wmb': 5388, 'buyprice': 5389, 'stkscotsr': 5390, 'guder': 5391, 'finncomm': 5392, 'evasion': 5393, 'altr': 5394, 'tripolibased': 5395, 'noc': 5396, 'libyan': 5397, 'rcon': 5398, 'severe': 5399, 'guy': 5400, 'advised': 5401, 'rabbit': 5402, 'formadehyde': 5403, 'dispose': 5404, 'firsthalf': 5405, 'vnh': 5406, 'beaten': 5407, 'hated': 5408, 'hhhmm': 5409, 'opposite': 5410, 'leasable': 5411, 'vacancy': 5412, 'composition': 5413, 'lehtiyhtyma': 5414, 'streamlined': 5415, 'comply': 5416, 'traceability': 5417, 'parameter': 5418, 'probable': 5419, 'fixedtomobile': 5420, 'pirila': 5421, 'fiberbased': 5422, 'tcogprzujfc': 5423, 'tcogioofzbkv': 5424, 'europolitan': 5425, 'nyman': 5426, 'schultz': 5427, 'gambro': 5428, 'renal': 5429, 'newfound': 5430, 'finlan': 5431, 'tahko': 5432, 'spa': 5433, 'tent': 5434, 'haiti': 5435, 'maarten': 5436, 'boute': 5437, 'surfacetreatment': 5438, 'businesstobusiness': 5439, 'guinea': 5440, 'forwarders': 5441, 'perrigo': 5442, 'heatretaining': 5443, 'celulose': 5444, 'papel': 5445, 'vcp': 5446, 'hautaniemi': 5447, 'volvo': 5448, 'retaining': 5449, 'governemtn': 5450, 'commiting': 5451, 'reaffirmed': 5452, 'ism': 5453, 'dental': 5454, 'oral': 5455, 'hammaslaakarit': 5456, 'tdd': 5457, 'condominium': 5458, 'middleeast': 5459, 'southafrica': 5460, 'rf': 5461, 'elektros': 5462, 'pavara': 5463, 'fki': 5464, 'jiang': 5465, 'tcoczhixtuag': 5466, 'tcozqbxald': 5467, 'commercialise': 5468, 'nasdaqlisted': 5469, 'malaysian': 5470, 'sacanfil': 5471, 'skandinaviska': 5472, 'enskilda': 5473, 'banken': 5474, 'fulfilled': 5475, 'invenergy': 5476, 'northland': 5477, 'kaunisvaara': 5478, 'tapuli': 5479, 'sahavaara': 5480, 'renewing': 5481, 'fab': 5482, 'glasgow': 5483, 'rcp': 5484, 'wwwcomponentacom': 5485, 'renewal': 5486, 'symc': 5487, 'bctmp': 5488, 'sgyp': 5489, 'stkscojawg': 5490, 'theodosopoulos': 5491, 'tellabs': 5492, 'nortel': 5493, 'highgrowth': 5494, 'respondent': 5495, 'praised': 5496, 'finnishness': 5497, 'stkscokm': 5498, 'bankruptcy': 5499, 'ubnt': 5500, 'cheri': 5501, 'strattonite': 5502, 'stkscocsu': 5503, 'fusioniq': 5504, 'tcoimlxzyna': 5505, 'retweet': 5506, 'thetrainlinecom': 5507, 'arrival': 5508, 'argentine': 5509, 'calafate': 5510, 'tapped': 5511, 'compatriot': 5512, 'finnegans': 5513, 'definitely': 5514, 'privatisation': 5515, 'salvor': 5516, 'ponv': 5517, 'bleaching': 5518, 'liner': 5519, 'washing': 5520, 'klabin': 5521, 'telmaco': 5522, 'borba': 5523, 'paran': 5524, 'county': 5525, 'hosted': 5526, 'stkscojuu': 5527, 'bioheapleaching': 5528, 'opec': 5529, 'stkscoaix': 5530, 'wti': 5531, 'brent': 5532, 'stkscosiln': 5533, 'searching': 5534, 'replacement': 5535, 'compensated': 5536, 'osur': 5537, 'abov': 5538, 'williams': 5539, 'newbury': 5540, 'flashing': 5541, 'pursues': 5542, 'consumercustomers': 5543, 'inspected': 5544, 'norwegianregistered': 5545, 'sten': 5546, 'manned': 5547, 'filipino': 5548, 'evidence': 5549, 'brussels': 5550, 'discussed': 5551, 'embarked': 5552, 'unbanked': 5553, 'underbanked': 5554, 'stabilisation': 5555, 'stkscocqkr': 5556, 'seafood': 5557, 'contacted': 5558, 'bph': 5559, 'alior': 5560, 'fallondpicks': 5561, 'slowe': 5562, 'stkscotrg': 5563, 'kicking': 5564, 'preprocess': 5565, 'packet': 5566, 'determine': 5567, 'readychex': 5568, 'distributes': 5569, 'debenhams': 5570, 'tcoxgwnont': 5571, 'companiestravis': 5572, 'breath': 5573, 'miracle': 5574, 'examines': 5575, 'apollo': 5576, 'barleycorn': 5577, 'ambulance': 5578, 'efut': 5579, 'crus': 5580, 'alpha': 5581, 'nwc': 5582, 'domino': 5583, 'pizza': 5584, 'asos': 5585, 'musthave': 5586, 'prohouse': 5587, 'korteniemi': 5588, 'anneli': 5589, 'helokunnas': 5590, 'tuija': 5591, 'peltola': 5592, 'keskinen': 5593, 'salminen': 5594, 'tusa': 5595, 'kia': 5596, 'janhonen': 5597, 'logistic': 5598, 'slovenia': 5599, 'jeder': 5600, 'betatester': 5601, 'erhlt': 5602, 'kostenlos': 5603, 'sechs': 5604, 'monate': 5605, 'und': 5606, 'laut': 5607, 'eigener': 5608, 'aussage': 5609, 'mglichkeit': 5610, 'finale': 5611, 'zu': 5612, 'beeinflussen': 5613, 'strelny': 5614, 'suburb': 5615, 'remodeled': 5616, 'cannon': 5617, 'trap': 5618, 'creme': 5619, 'bunny': 5620, 'expiring': 5621, 'worthless': 5622, 'modelx': 5623, 'tcohsqdovry': 5624, 'tcocscnzebb': 5625, 'envisaged': 5626, 'storey': 5627, 'luna': 5628, 'sixunder': 5629, 'beatriz': 5630, 'recari': 5631, 'martina': 5632, 'eberl': 5633, 'welsh': 5634, 'becky': 5635, 'brewerton': 5636, 'highestplaced': 5637, 'finisher': 5638, 'sharing': 5639, 'seventh': 5640, 'threeunder': 5641, 'shooting': 5642, 'westward': 5643, 'inclination': 5644, 'pisa': 5645, 'diagrid': 5646, 'aligned': 5647, 'geometrically': 5648, 'vostok': 5649, 'mosenergosbytbody': 5650, 'mssb': 5651, 'altaienergosbyt': 5652, 'saratovenergo': 5653, 'sare': 5654, 'tambov': 5655, 'tasb': 5656, 'ofbodywhich': 5657, 'terminator': 5658, 'subcontract': 5659, 'digv': 5660, 'mobilephone': 5661, 'brutal': 5662, 'taneli': 5663, 'hassinen': 5664, 'tcoeqqltfuiou': 5665, 'sarkamies': 5666, 'panic': 5667, 'beatendown': 5668, 'ripe': 5669, 'groupon': 5670, 'marketsbp': 5671, 'promotes': 5672, 'retailwholesale': 5673, 'prague': 5674, 'celebrated': 5675, 'sap': 5676, 'disappoints': 5677, 'trail': 5678, 'tcojndphllzq': 5679, 'round': 5680, 'kuula': 5681, 'parallel': 5682, 'compulsory': 5683, 'kotikokki': 5684, 'straight': 5685, 'reaching': 5686, 'vostochny': 5687, 'hearst': 5688, 'overnight': 5689, 'eighteen': 5690, 'campbell': 5691, 'paperlinx': 5692, 'jalo': 5693, 'dormus': 5694, 'malinen': 5695, 'tier': 5696, 'gartner': 5697, 'volotinen': 5698, 'myllykoski': 5699, 'creditor': 5700, 'cph': 5701, 'fourthlargest': 5702, 'citrix': 5703, 'mizuho': 5704, 'tcovlkqflp': 5705, 'sharestocknews': 5706, 'dragonfly': 5707, 'finalize': 5708, 'stateheld': 5709, 'ues': 5710, 'reform': 5711, 'resting': 5712, 'comfortably': 5713, 'noora': 5714, 'niiininoski': 5715, 'timeless': 5716, 'vertex': 5717, 'hilower': 5718, 'intract': 5719, 'jd': 5720, 'afterhour': 5721, 'explain': 5722, 'trlpc': 5723, 'pricey': 5724, 'preferably': 5725, 'appliance': 5726, 'fragile': 5727, 'fy': 5728, 'warn': 5729, 'eijkens': 5730, 'colourful': 5731, 'individualising': 5732, 'towergate': 5733, 'egan': 5734, 'eightday': 5735, 'barley': 5736, 'specialize': 5737, 'formation': 5738, 'technoparks': 5739, 'hitech': 5740, 'nongmo': 5741, 'chessnwine': 5742, 'breakdown': 5743, 'stkscorkkm': 5744, 'stkscofvvu': 5745, 'stkscocqrm': 5746, 'universal': 5747, 'warner': 5748, 'emi': 5749, 'rotana': 5750, 'mazzika': 5751, 'melody': 5752, 'aggregator': 5753, 'orchard': 5754, 'megamart': 5755, 'eataly': 5756, 'meaning': 5757, 'kid': 5758, 'carnival': 5759, 'memo': 5760, 'substitute': 5761, 'substation': 5762, 'pensionable': 5763, 'sonc': 5764, 'obvious': 5765, 'qualitytastiness': 5766, 'burger': 5767, 'factored': 5768, 'mako': 5769, 'geneva': 5770, 'tero': 5771, 'aaltonen': 5772, 'kahonen': 5773, 'juuka': 5774, 'pit': 5775, 'marktomarket': 5776, 'caprelsa': 5777, 'neeeeds': 5778, 'kapiteeli': 5779, 'cableway': 5780, 'paychex': 5781, 'nationwide': 5782, 'refiner': 5783, 'forge': 5784, 'calibrate': 5785, 'dead': 5786, 'bloated': 5787, 'tcoczcfkbrlt': 5788, 'pride': 5789, 'kroksberg': 5790, 'harnosand': 5791, 'veda': 5792, 'picture': 5793, 'standardised': 5794, 'itella': 5795, 'intellisync': 5796, 'simdax': 5797, 'submitted': 5798, 'altadis': 5799, 'rubin': 5800, 'chrome': 5801, 'fim': 5802, 'alpro': 5803, 'tcoodpranxq': 5804, 'tcoxvnbliwpy': 5805, 'tcothfvwtnrps': 5806, 'interdigital': 5807, 'telecomworldwire': 5808, 'edita': 5809, 'tcojdnzxdcmkl': 5810, 'tcohmoegfhgvn': 5811, 'niklas': 5812, 'eubased': 5813, 'mcp': 5814, 'picker': 5815, 'destroyed': 5816, 'coh': 5817, 'stkscottah': 5818, 'fbr': 5819, 'sit': 5820, 'tcojgwzzc': 5821, 'ogden': 5822, 'word': 5823, 'allergen': 5824, 'endo': 5825, 'drinking': 5826, 'logical': 5827, 'ballast': 5828, 'shs': 5829, 'shortlived': 5830, 'redesigned': 5831, 'crushing': 5832, 'helmet': 5833, 'propertos': 5834, 'kivimeister': 5835, 'deer': 5836, 'timberjack': 5837, 'canvas': 5838, 'expression': 5839, 'mobileinspired': 5840, 'tcoimkygocyc': 5841, 'svenska': 5842, 'shb': 5843, 'rorocruise': 5844, 'karachi': 5845, 'zxx': 5846, 'steaming': 5847, 'relentless': 5848, 'jrvinen': 5849, 'fedday': 5850, 'hd': 5851, 'tvix': 5852, 'hammered': 5853, 'payphones': 5854, 'peterservis': 5855, 'shetland': 5856, 'mineracao': 5857, 'ltda': 5858, 'cumerio': 5859, 'med': 5860, 'jsco': 5861, 'shuts': 5862, 'stricter': 5863, 'struggle': 5864, 'england': 5865, 'andrius': 5866, 'bagdonas': 5867, 'acnielsen': 5868, 'scantrack': 5869, 'chartlybeyt': 5870, 'bioethanol': 5871, 'nuan': 5872, 'undervalued': 5873, 'rbi': 5874, 'sensex': 5875, 'pares': 5876, 'costanza': 5877, 'washable': 5878, 'shade': 5879, 'pistachio': 5880, 'guided': 5881, 'raiso': 5882, 'showcase': 5883, 'entice': 5884, 'stkscojrmw': 5885, 'sullivan': 5886, 'bakman': 5887, 'tallinnhelisnki': 5888, 'claiming': 5889, 'rosberg': 5890, 'sourced': 5891, 'tcoreufpzhbm': 5892, 'okhta': 5893, 'coatingsrelated': 5894, 'pge': 5895, 'gw': 5896, 'meripori': 5897, 'seventythree': 5898, 'archestra': 5899, 'bench': 5900, 'litter': 5901, 'receptacle': 5902, 'toilet': 5903, 'chilled': 5904, 'ymi': 5905, 'stkscoxyf': 5906, 'fitzstock': 5907, 'iyt': 5908, 'stksconbf': 5909, 'postel': 5910, 'glaxo': 5911, 'desano': 5912, 'argentina': 5913, 'adoption': 5914, 'adopter': 5915, 'backhaul': 5916, 'weir': 5917, 'incorporates': 5918, 'sm': 5919, 'wap': 5920, 'age': 5921, 'microwaveable': 5922, 'gdxj': 5923, 'jnug': 5924, 'junior': 5925, 'expe': 5926, 'leg': 5927, 'tcodsjysyr': 5928, 'phillip': 5929, 'tase': 5930, 'quitting': 5931, 'independence': 5932, 'individually': 5933, 'aladdin': 5934, 'kaspersky': 5935, 'marshal': 5936, 'mcafee': 5937, 'panda': 5938, 'proofpoint': 5939, 'tumbleweed': 5940, 'websense': 5941, 'usmanov': 5942, 'statecontrolled': 5943, 'svyazinvest': 5944, 'highrises': 5945, 'virsuliskes': 5946, 'scrapped': 5947, 'kurkilahti': 5948, 'dna': 5949, 'mantyharju': 5950, 'virgin': 5951, 'requires': 5952, 'realprojekti': 5953, 'dwa': 5954, 'lightweight': 5955, 'broadsheet': 5956, 'tabloid': 5957, 'yearold': 5958, 'derive': 5959, 'housewares': 5960, 'annvik': 5961, 'engagement': 5962, 'tcotqmmtnx': 5963, 'tcosedqrwgdkt': 5964, 'inverted': 5965, 'jacob': 5966, 'xfnasia': 5967, 'lr': 5968, 'steam': 5969, 'tcohzwdpza': 5970, 'maggie': 5971, 'ramsey': 5972, 'oregon': 5973, 'flock': 5974, 'frequent': 5975, 'stkscojkuf': 5976, 'applicant': 5977, 'jot': 5978, 'oyname': 5979, 'mammila': 5980, 'tuomo': 5981, 'piirainen': 5982, 'kellokoski': 5983, 'mikaapplication': 5984, 'kolnp': 5985, 'adate': 5986, 'filing': 5987, 'derived': 5988, 'scopi': 5989, 'eng': 5990, 'lankapaja': 5991, 'sheetmetal': 5992, 'accepted': 5993, 'karttakeskus': 5994, 'bedroom': 5995, 'cool': 5996, 'makeover': 5997, 'zero': 5998, 'configuration': 5999, 'allan': 6000, 'tcocxahjixd': 6001, 'reijo': 6002, 'maihaniemi': 6003, 'zte': 6004, 'acad': 6005, 'amgn': 6006, 'ucb': 6007, 'topline': 6008, 'ph': 6009, 'evaluating': 6010, 'romosozumab': 6011, 'men': 6012, 'tcohboudrb': 6013, 'yang': 6014, 'schroders': 6015, 'depresses': 6016, 'wellknown': 6017, 'proceeded': 6018, 'skepticism': 6019, 'companiesmeggitt': 6020, 'military': 6021, 'ahms': 6022, 'franchise': 6023, 'panindia': 6024, 'companiescar': 6025, 'hastings': 6026, 'tcofdxaegi': 6027, 'blockchain': 6028, 'repo': 6029, 'chambersburg': 6030, 'stkscobue': 6031, 'tekes': 6032, 'designtalo': 6033, 'cib': 6034, 'egyptian': 6035, 'katajavuori': 6036, 'yearlong': 6037, 'sabbatical': 6038, 'dig': 6039, 'wrtv': 6040, 'psv': 6041, 'eidesvik': 6042, 'eiof': 6043, 'earning': 6044, 'tenyear': 6045, 'ise': 6046, 'vuokatti': 6047, 'prototype': 6048, 'rampup': 6049, 'jdst': 6050, 'blrx': 6051, 'lpg': 6052, 'afsi': 6053, 'dvax': 6054, 'hdge': 6055, 'aht': 6056, 'alr': 6057, 'asmb': 6058, 'tspt': 6059, 'wow': 6060, 'medvezhyegorsk': 6061, 'indexlinked': 6062, 'tutkimuksen': 6063, 'tahdet': 6064, 'tcobizyabue': 6065, 'nil': 6066, 'melngailis': 6067, 'assumed': 6068, 'rantakari': 6069, 'unlisted': 6070, 'biologicals': 6071, 'diseasespecific': 6072, 'supplement': 6073, 'hepatitis': 6074, 'cardiovascular': 6075, 'ailment': 6076, 'photo': 6077, 'kaupunkin': 6078, 'talotekniikka': 6079, 'declares': 6080, 'kindle': 6081, 'stkscocetw': 6082, 'engadget': 6083, 'bolster': 6084, 'kallioranta': 6085, 'chum': 6086, 'churned': 6087, 'courtship': 6088, 'agreeing': 6089, 'mapping': 6090, 'pressurized': 6091, 'cps': 6092, 'telco': 6093, 'teaming': 6094, 'teenager': 6095, 'birmingham': 6096, 'hampden': 6097, 'europeanwide': 6098, 'casualty': 6099, 'companieshoward': 6100, 'argus': 6101, 'projektivuokraus': 6102, 'adjusting': 6103, 'biggies': 6104, 'excuse': 6105, 'highway': 6106, 'ocn': 6107, 'stuff': 6108, 'utsi': 6109, 'stkscour': 6110, 'nitrogen': 6111, 'nitrate': 6112, 'mongolia': 6113, 'hydrocopper': 6114, 'learn': 6115, 'voicedirected': 6116, 'respect': 6117, 'ameriprice': 6118, 'stockoptionexpert': 6119, 'stkscocols': 6120, 'companieshome': 6121, 'bts': 6122, 'configured': 6123, 'diagnosed': 6124, 'hlf': 6125, 'killing': 6126, 'extracted': 6127, 'filtrate': 6128, 'dried': 6129, 'cake': 6130, 'moisture': 6131, 'tmls': 6132, 'fo': 6133, 'sho': 6134, 'sixstep': 6135, 'timefocused': 6136, 'collaborative': 6137, 'absolut': 6138, 'exploring': 6139, 'borrowing': 6140, 'pmax': 6141, 'secondgeneration': 6142, 'moorestown': 6143, 'tcogyadbjsq': 6144, 'tcokpbbcgqxy': 6145, 'tcozjertkch': 6146, 'standalone': 6147, 'relating': 6148, 'pkt': 6149, 'stkscoexvu': 6150, 'qprv': 6151, 'basement': 6152, 'jumbo': 6153, 'yielder': 6154, 'gdf': 6155, 'suez': 6156, 'rejecting': 6157, 'shortable': 6158, 'adsk': 6159, 'cr': 6160, 'laurence': 6161, 'tally': 6162, 'evident': 6163, 'stellar': 6164, 'gwr': 6165, 'watco': 6166, 'eyeing': 6167, 'stkscooxy': 6168, 'cruising': 6169, 'diversify': 6170, 'aspokem': 6171, 'concentrator': 6172, 'siilinjarvi': 6173, 'minera': 6174, 'slavery': 6175, 'deramus': 6176, 'formerly': 6177, 'writer': 6178, 'saarioinen': 6179, 'weber': 6180, 'convinced': 6181, 'reuse': 6182, 'pet': 6183, 'strapping': 6184, 'petcore': 6185, 'coop': 6186, 'splitscreen': 6187, 'multiplayer': 6188, 'vtb': 6189, 'subdivision': 6190, 'stkscocivx': 6191, 'tag': 6192, 'honkamaa': 6193, 'viewpoint': 6194, 'ulm': 6195, 'dusseldorf': 6196, 'usp': 6197, 'roadmap': 6198, 'simon': 6199, 'kcg': 6200, 'travis': 6201, 'creep': 6202, 'sits': 6203, 'redtogreen': 6204, 'gate': 6205, 'tcovxfuuit': 6206, 'mcdonald': 6207, 'doobie': 6208, 'song': 6209, 'oxyview': 6210, 'oximeter': 6211, 'bloodoxygen': 6212, 'saturation': 6213, 'emailed': 6214, 'offenburg': 6215, 'bidirectional': 6216, 'auction': 6217, 'bloc': 6218, 'rvr': 6219, 'lteready': 6220, 'multiradio': 6221, 'daxtum': 6222, 'similarsized': 6223, 'karkkila': 6224, 'weert': 6225, 'stkscokj': 6226, 'slope': 6227, 'cliffton': 6228, 'entreprenad': 6229, 'rod': 6230, 'depends': 6231, 'feeder': 6232, 'wenchong': 6233, 'intriguing': 6234, 'mobilebased': 6235, 'internetbased': 6236, 'accelerated': 6237, 'earningsrelated': 6238, 'tendering': 6239, 'bullet': 6240, 'bunzl': 6241, 'lack': 6242, 'tcosxzuno': 6243, 'metalszinc': 6244, 'fuelling': 6245, 'eos': 6246, 'innovationsmanagement': 6247, 'novotel': 6248, 'calibration': 6249, 'alltime': 6250, 'ucell': 6251, 'asks': 6252, 'herttaassa': 6253, 'dara': 6254, 'mp': 6255, 'snapper': 6256, 'sanction': 6257, 'gazpromshell': 6258, 'jeopardy': 6259, 'tcovlzlgvihzv': 6260, 'reisholz': 6261, 'specialised': 6262, 'costly': 6263, 'thinly': 6264, 'populated': 6265, 'balloon': 6266, 'withdrawing': 6267, 'wellingborough': 6268, 'highperformance': 6269, 'portable': 6270, 'augustoctober': 6271, 'heathrow': 6272, 'osaka': 6273, 'la': 6274, 'vega': 6275, 'highend': 6276, 'lunchtime': 6277, 'aa': 6278, 'tecnomens': 6279, 'workforse': 6280, 'dojis': 6281, 'tcolguybxeze': 6282, 'toast': 6283, 'bitter': 6284, 'powersupply': 6285, 'dino': 6286, 'retired': 6287, 'jefferies': 6288, 'stkscogcqd': 6289, 'cornerstone': 6290, 'biofuelfired': 6291, 'harbor': 6292, 'jarvinen': 6293, 'surpass': 6294, 'nationally': 6295, 'regionally': 6296, 'och': 6297, 'inrikes': 6298, 'tidningar': 6299, 'isr': 6300, 'isoray': 6301, 'stkscoifpo': 6302, 'biotech': 6303, 'lahden': 6304, 'lampokasittely': 6305, 'heatmasters': 6306, 'stabilised': 6307, 'sataflexo': 6308, 'blizzard': 6309, 'esports': 6310, 'viewership': 6311, 'tcobiqlbtk': 6312, 'partnered': 6313, 'hdsi': 6314, 'ehitus': 6315, 'sauk': 6316, 'toll': 6317, 'intu': 6318, 'tcodrbanunu': 6319, 'trow': 6320, 'stkscogls': 6321, 'demanded': 6322, 'overcharging': 6323, 'inspectorate': 6324, 'everest': 6325, 'climber': 6326, 'surf': 6327, 'synosia': 6328, 'allstock': 6329, 'dgi': 6330, 'lizard': 6331, 'candlestick': 6332, 'wid': 6333, 'stkscocnu': 6334, 'cdp': 6335, 'dash': 6336, 'shelved': 6337, 'info': 6338, 'recoil': 6339, 'regeneration': 6340, 'ryder': 6341, 'cup': 6342, 'venue': 6343, 'gleneagles': 6344, 'ennismore': 6345, 'politician': 6346, 'failure': 6347, 'differentiate': 6348, 'beleaguered': 6349, 'skandinavian': 6350, 'sailed': 6351, 'hallbergivarsson': 6352, 'renatta': 6353, 'frvan': 6354, 'kjepeneter': 6355, 'rptl': 6356, 'plnr': 6357, 'attach': 6358, 'stksconfei': 6359, 'brooklynbandit': 6360, 'simo': 6361, 'sausage': 6362, 'grill': 6363, 'shish': 6364, 'kebab': 6365, 'paavel': 6366, 'retrieving': 6367, 'typed': 6368, 'keyloggers': 6369, 'mechanism': 6370, 'arrived': 6371, 'phishing': 6372, 'pharming': 6373, 'femsa': 6374, 'ade': 6375, 'soybased': 6376, 'annualised': 6377, 'selfservice': 6378, 'exploited': 6379, 'stkscojr': 6380, 'stoch': 6381, 'louiebeene': 6382, 'navb': 6383, 'coker': 6384, 'tesoro': 6385, 'eagle': 6386, 'martinez': 6387, 'castell': 6388, 'ahlv': 6389, 'spunlace': 6390, 'chirnside': 6391, 'vap': 6392, 'simple': 6393, 'interestbearing': 6394, 'hitk': 6395, 'resuming': 6396, 'historical': 6397, 'commencing': 6398, 'karlstad': 6399, 'governed': 6400, 'legislative': 6401, 'xli': 6402, 'intermediate': 6403, 'stkscojtdf': 6404, 'lulea': 6405, 'edgar': 6406, 'edmonds': 6407, 'christian': 6408, 'fischer': 6409, 'speech': 6410, 'kyrolainen': 6411, 'ambassador': 6412, 'symbol': 6413, 'ergonomic': 6414, 'kveyt': 6415, 'suomalainen': 6416, 'pekoni': 6417, 'marketed': 6418, 'breather': 6419, 'stkscogtc': 6420, 'underpins': 6421, 'maya': 6422, 'doering': 6423, 'correctedshire': 6424, 'dyax': 6425, 'downloading': 6426, 'toppled': 6427, 'semitrailer': 6428, 'barstow': 6429, 'bancosthey': 6430, 'backburner': 6431, 'geov': 6432, 'download': 6433, 'tlvv': 6434, 'bookrunners': 6435, 'organised': 6436, 'finnishmade': 6437, 'dug': 6438, 'stochastics': 6439, 'bailout': 6440, 'cooperationand': 6441, 'tied': 6442, 'ourmachinery': 6443, 'fanfare': 6444, 'machineryengineering': 6445, 'caverion': 6446, 'lookout': 6447, 'restoration': 6448, 'winder': 6449, 'salt': 6450, 'soosalu': 6451, 'chkp': 6452, 'tcouvmmrsrzy': 6453, 'ocbc': 6454, 'barclay': 6455, 'tcotajfchqh': 6456, 'breached': 6457, 'kariniemi': 6458, 'kasei': 6459, 'buckeye': 6460, 'ei': 6461, 'bosniaherzegovina': 6462, 'aluminij': 6463, 'dd': 6464, 'mostar': 6465, 'syrjnen': 6466, 'internationalised': 6467, 'ahs': 6468, 'ige': 6469, 'irdm': 6470, 'ewbc': 6471, 'phys': 6472, 'ulta': 6473, 'vnet': 6474, 'biv': 6475, 'tcokjzuzmu': 6476, 'outright': 6477, 'midsized': 6478, 'indebtedness': 6479, 'seitovirta': 6480, 'patja': 6481, 'reclaimed': 6482, 'recession': 6483, 'lazada': 6484, 'alibaba': 6485, 'croda': 6486, 'sage': 6487, 'shortlisted': 6488, 'mitsubishi': 6489, 'euapwr': 6490, 'areva': 6491, 'toshiba': 6492, 'hitachi': 6493, 'evidentiary': 6494, 'sciencebased': 6495, 'rory': 6496, 'fitzgerald': 6497, 'servicing': 6498, 'sensing': 6499, 'hydraulics': 6500, 'mandate': 6501, 'employeremployee': 6502, 'neon': 6503, 'electrically': 6504, 'fulfillment': 6505, 'baba': 6506, 'wks': 6507, 'mrkt': 6508, 'washout': 6509, 'stalk': 6510, 'liljeholmen': 6511, 'aramco': 6512, 'motiva': 6513, 'facilitate': 6514, 'unspectacular': 6515, 'suspends': 6516, 'devaluation': 6517, 'burberry': 6518, 'greatest': 6519, 'techsavvy': 6520, 'gill': 6521, 'hyunchang': 6522, 'newswires': 6523, 'investlesprom': 6524, 'wwwyitgroupcominvestors': 6525, 'validation': 6526, 'verifying': 6527, 'fulfil': 6528, 'gte': 6529, 'disclaims': 6530, 'tcoxdgraexkkz': 6531, 'distinctly': 6532, 'unlikely': 6533, 'divi': 6534, 'centralising': 6535, 'barron': 6536, 'cramer': 6537, 'tcolkldmvkbq': 6538, 'depend': 6539, 'maloney': 6540, 'tulla': 6541, 'sdlp': 6542, 'cpgx': 6543, 'bita': 6544, 'ete': 6545, 'sdrl': 6546, 'autocharts': 6547, 'tconxlxesny': 6548, 'balfour': 6549, 'beatty': 6550, 'reinstate': 6551, 'xlb': 6552, 'false': 6553, 'stkscodlz': 6554, 'thats': 6555, 'mtyear': 6556, 'alumina': 6557, 'hrl': 6558, 'stkscosny': 6559, 'kw': 6560, 'florida': 6561, 'hauling': 6562, 'dry': 6563, 'cee': 6564, 'accuses': 6565, 'infringement': 6566, 'stkscoga': 6567, 'surpassing': 6568, 'evacuate': 6569, 'hornborg': 6570, 'merely': 6571, 'theft': 6572, 'positioning': 6573, 'polyolefin': 6574, 'disease': 6575, 'retake': 6576, 'nqf': 6577, 'tcolmwxchw': 6578, 'lebanon': 6579, 'wax': 6580, 'alike': 6581, 'practice': 6582, 'espana': 6583, 'foothold': 6584, 'dasan': 6585, 'tmtanalyst': 6586, 'cornering': 6587, 'nand': 6588, 'curfew': 6589, 'curl': 6590, 'luceplan': 6591, 'privatization': 6592, 'chasing': 6593, 'nears': 6594, 'whats': 6595, 'looked': 6596, 'instill': 6597, 'flushed': 6598, 'intensifying': 6599, 'finnishowned': 6600, 'kft': 6601, 'staffer': 6602, 'fulfill': 6603, 'kemijoki': 6604, 'river': 6605, 'laakso': 6606, 'smarket': 6607, 'vishakapatnam': 6608, 'vizag': 6609, 'kick': 6610, 'sportsequipment': 6611, 'schuldschein': 6612, 'pool': 6613, 'replay': 6614, 'nonus': 6615, 'picking': 6616, 'stkscoeik': 6617, 'stkscoeil': 6618, 'homeowner': 6619, 'ecofriendly': 6620, 'watchtower': 6621, 'sbgrp': 6622, 'jarle': 6623, 'haug': 6624, 'finans': 6625, 'dealership': 6626, 'drawing': 6627, 'fourdoor': 6628, 'altima': 6629, 'bright': 6630, 'tcotjjvpqhhqd': 6631, 'ruuska': 6632, 'marinated': 6633, 'olive': 6634, 'pate': 6635, 'samarco': 6636, 'hanninen': 6637, 'mobileid': 6638, 'sslvpn': 6639, 'sharepoint': 6640, 'mediumsized': 6641, 'sphere': 6642, 'kauhajoen': 6643, 'teurastamokiinteistot': 6644, 'itikka': 6645, 'bicentenary': 6646, 'offsetprinting': 6647, 'aftertreatment': 6648, 'inline': 6649, 'nancheng': 6650, 'asiapacific': 6651, 'vashi': 6652, 'insteel': 6653, 'ltdiivrcl': 6654, 'cidco': 6655, 'companiesdiageo': 6656, 'boardroom': 6657, 'turmoil': 6658, 'discloses': 6659, 'shor': 6660, 'stkscors': 6661, 'harold': 6662, 'closely': 6663, 'ahold': 6664, 'basket': 6665, 'bj': 6666, 'club': 6667, 'hannaford': 6668, 'cumberland': 6669, 'creation': 6670, 'hfc': 6671, 'fibrecoaxial': 6672, 'xdsl': 6673, 'etth': 6674, 'twice': 6675, 'kiikoinen': 6676, 'dealmakers': 6677, 'ignore': 6678, 'gevo': 6679, 'plunging': 6680, 'cockroft': 6681, 'singer': 6682, 'friedlander': 6683, 'broking': 6684, 'recap': 6685, 'spinning': 6686, 'xbox': 6687, 'grind': 6688, 'octoberdecember': 6689, 'followon': 6690, 'curator': 6691, 'theme': 6692, 'envisages': 6693, 'gatehouse': 6694, 'cheered': 6695, 'thailand': 6696, 'belonging': 6697, 'circle': 6698, 'saha': 6699, 'pathana': 6700, 'interholding': 6701, 'horizonte': 6702, 'multitools': 6703, 'lighting': 6704, 'pedometer': 6705, 'placement': 6706, 'eua': 6707, 'allowance': 6708, 'gloriya': 6709, 'kolorit': 6710, 'passive': 6711, 'tcoqbsowjxost': 6712, 'education': 6713, 'edu': 6714, 'rosendal': 6715, 'parity': 6716, 'utilises': 6717, 'painting': 6718, 'coat': 6719, 'exterior': 6720, 'primer': 6721, 'sammy': 6722, 'hello': 6723, 'hitura': 6724, 'doomed': 6725, 'tcompylireqjh': 6726, 'tconlczawdohp': 6727, 'demerger': 6728, 'niemi': 6729, 'oriola': 6730, 'error': 6731, 'oborniki': 6732, 'sandwich': 6733, 'almaty': 6734, 'decorative': 6735, 'modernisation': 6736, 'highspeed': 6737, 'poser': 6738, 'impress': 6739, 'gol': 6740, 'updating': 6741, 'stkscofdmm': 6742, 'thank': 6743, 'telecomms': 6744, 'rebuilt': 6745, 'drugmaker': 6746, 'chikunov': 6747, 'surveyed': 6748, 'convenience': 6749, 'grading': 6750, 'krogerus': 6751, 'unrivalled': 6752, 'elimaki': 6753, 'apparatus': 6754, 'stkscoly': 6755, 'minimum': 6756, 'howard': 6757, 'zloty': 6758, 'naturalgas': 6759, 'dbo': 6760, 'bno': 6761, 'stkscodqx': 6762, 'bpm': 6763, 'ubi': 6764, 'econ': 6765, 'iisalmi': 6766, 'tex': 6767, 'nearshore': 6768, 'wwwcountryelementscouk': 6769, 'burt': 6770, 'hooked': 6771, 'rug': 6772, 'dyed': 6773, 'dye': 6774, 'benelux': 6775, 'eriikka': 6776, 'sderstrm': 6777, 'bobe': 6778, 'singulase': 6779, 'shadow': 6780, 'lip': 6781, 'gloss': 6782, 'mascara': 6783, 'riihimki': 6784, 'occupy': 6785, 'duo': 6786, 'diffuse': 6787, 'tension': 6788, 'electriccar': 6789, 'lately': 6790, 'lieksaare': 6791, 'regarded': 6792, 'cumulative': 6793, 'scottrade': 6794, 'sponsor': 6795, 'endowment': 6796, 'midwest': 6797, 'collision': 6798, 'shielding': 6799, 'movable': 6800, 'suunto': 6801, 'satisfactorily': 6802, 'restarting': 6803, 'older': 6804, 'explosively': 6805, 'inregard': 6806, 'depositor': 6807, 'preference': 6808, 'rank': 6809, 'iff': 6810, 'stkscopqy': 6811, 'chargz': 6812, 'thebault': 6813, 'plyland': 6814, 'bleeding': 6815, 'aspen': 6816, 'anaesthetic': 6817, 'chartlyakvujxh': 6818, 'resist': 6819, 'intuitiv': 6820, 'clne': 6821, 'spike': 6822, 'stkscotboi': 6823, 'retire': 6824, 'ppmi': 6825, 'lscc': 6826, 'stkscoadpo': 6827, 'lanlan': 6828, 'dualfuel': 6829, 'maintaining': 6830, 'designing': 6831, 'redhot': 6832, 'tcoiylcugabc': 6833, 'resettlement': 6834, 'fivestar': 6835, 'inspects': 6836, 'homebuilder': 6837, 'ryl': 6838, 'cern': 6839, 'issuing': 6840, 'lightning': 6841, 'origin': 6842, 'mac': 6843, 'stkscoaktu': 6844, 'macrumors': 6845, 'onboard': 6846, 'freighter': 6847, 'singaporean': 6848, 'masterbulk': 6849, 'spin': 6850, 'newlyformed': 6851, 'mww': 6852, 'limiting': 6853, 'heterogenous': 6854, 'brianwieser': 6855, 'tcotdneoh': 6856, 'egypt': 6857, 'attend': 6858, 'wwwglassworldexcom': 6859, 'wellbeing': 6860, 'ibio': 6861, 'shuweihat': 6862, 'desalination': 6863, 'hemphill': 6864, 'ap': 6865, 'caterpillar': 6866, 'steep': 6867, 'linking': 6868, 'dictated': 6869, 'input': 6870, 'energyefficient': 6871, 'stateoftheart': 6872, 'partial': 6873, 'ovako': 6874, 'lxe': 6875, 'enhances': 6876, 'imperial': 6877, 'elqav': 6878, 'industrialisation': 6879, 'tyo': 6880, 'equalweight': 6881, 'underweight': 6882, 'weight': 6883, 'scheduling': 6884, 'brieflegal': 6885, 'stg': 6886, 'tallon': 6887, 'walker': 6888, 'caput': 6889, 'inflatable': 6890, 'rib': 6891, 'broadbased': 6892, 'wwwsanomacom': 6893, 'siriusxm': 6894, 'marketresearch': 6895, 'entrepreneur': 6896, 'perlos': 6897, 'allshare': 6898, 'ee': 6899, 'excessive': 6900, 'versatile': 6901, 'valkama': 6902, 'kirsi': 6903, 'shifting': 6904, 'suspect': 6905, 'pavement': 6906, 'drainage': 6907, 'tcouoefjlop': 6908, 'seppnen': 6909, 'onethird': 6910, 'surcharge': 6911, 'stz': 6912, 'ew': 6913, 'scty': 6914, 'tcoiuzjelfwt': 6915, 'gunneflo': 6916, 'tolerability': 6917, 'pharmacokinetics': 6918, 'intravenously': 6919, 'plaque': 6920, 'omeo': 6921, 'wwwomeose': 6922, 'greene': 6923, 'king': 6924, 'oversee': 6925, 'withviking': 6926, 'valga': 6927, 'lihatoostus': 6928, 'mak': 6929, 'moorits': 6930, 'rejection': 6931, 'stkscotxx': 6932, 'kitai': 6933, 'stroi': 6934, 'exp': 6935, 'fing': 6936, 'dominance': 6937, 'billionpound': 6938, 'centricity': 6939, 'bseries': 6940, 'isabel': 6941, 'lnsikalkkuna': 6942, 'publishes': 6943, 'hover': 6944, 'dialog': 6945, 'trailing': 6946, 'stkscoczg': 6947, 'chs': 6948, 'trained': 6949, 'paintshop': 6950, 'assistant': 6951, 'eclipse': 6952, 'tcozgczakeu': 6953, 'radiosonde': 6954, 'observation': 6955, 'entreprenor': 6956, 'andrey': 6957, 'ilyin': 6958, 'securing': 6959, 'potash': 6960, 'stkscohcpj': 6961, 'jackson': 6962, 'ebusiness': 6963, 'aboavista': 6964, 'mold': 6965, 'tlcom': 6966, 'pellissier': 6967, 'proposition': 6968, 'priceline': 6969, 'industryleading': 6970, 'reftele': 6971, 'maskinservice': 6972, 'spare': 6973, 'voltage': 6974, 'specialization': 6975, 'recentlyvia': 6976, 'fimentor': 6977, 'ionphase': 6978, 'aura': 6979, 'ifhedieshedies': 6980, 'bullhorn': 6981, 'nervous': 6982, 'buck': 6983, 'tcojfnnx': 6984, 'wrapped': 6985, 'roofing': 6986, 'pouring': 6987, 'stkscosowg': 6988, 'lagardere': 6989, 'indicates': 6990, 'continually': 6991, 'rushydro': 6992, 'oesk': 6993, 'ercs': 6994, 'inter': 6995, 'tcodevnlkhqtz': 6996, 'moderate': 6997, 'cvd': 6998, 'nilserik': 6999, 'eklund': 7000, 'ringman': 7001, 'saysjouni': 7002, 'haikarainen': 7003, 'perpendicular': 7004, 'draught': 7005, 'netcom': 7006, 'turbo': 7007, 'turntable': 7008, 'vibrocompactors': 7009, 'gansu': 7010, 'hualu': 7011, 'costumer': 7012, 'cf': 7013, 'pty': 7014, 'renison': 7015, 'nl': 7016, 'harbour': 7017, 'exceeds': 7018, 'mursula': 7019, 'gather': 7020, 'macroeconomic': 7021, 'awr': 7022, 'affirms': 7023, 'stkscofp': 7024, 'finnishgerman': 7025, 'homebased': 7026, 'chic': 7027, 'cheerful': 7028, 'geometric': 7029, 'vaalipalveluservice': 7030, 'quench': 7031, 'thirst': 7032, 'craft': 7033, 'kohtamki': 7034, 'theacsman': 7035, 'suddenly': 7036, 'touting': 7037, 'gtat': 7038, 'euribor': 7039, 'germanwings': 7040, 'disaster': 7041, 'jnk': 7042, 'stkscofcm': 7043, 'appetite': 7044, 'bluewin': 7045, 'exist': 7046, 'incy': 7047, 'espn': 7048, 'stkscoicj': 7049, 'si': 7050, 'intouch': 7051, 'hmi': 7052, 'industrialsql': 7053, 'historian': 7054, 'qi': 7055, 'spc': 7056, 'zapadnye': 7057, 'vorota': 7058, 'convenient': 7059, 'nearby': 7060, 'jaroszowka': 7061, 'zgody': 7062, 'designer': 7063, 'homeware': 7064, 'crosstown': 7065, 'stkscoffgj': 7066, 'henning': 7067, 'bahr': 7068, 'praise': 7069, 'longed': 7070, 'materialise': 7071, 'sneed': 7072, 'boater': 7073, 'newcomer': 7074, 'rbff': 7075, 'frank': 7076, 'peterson': 7077, 'ukmerge': 7078, 'pyry': 7079, 'cfr': 7080, 'tcolxxtknfh': 7081, 'randgold': 7082, 'fray': 7083, 'bentos': 7084, 'forestal': 7085, 'oriental': 7086, 'stkscoow': 7087, 'stkscomw': 7088, 'foul': 7089, 'mood': 7090, 'paulig': 7091, 'oscar': 7092, 'puljonki': 7093, 'dallascity': 7094, 'fcu': 7095, 'hopeturock': 7096, 'yes': 7097, 'somewhat': 7098, 'twounit': 7099, 'olkiluoto': 7100, 'twh': 7101, 'fcc': 7102, 'deregulatory': 7103, 'cannedpreserved': 7104, 'inha': 7105, 'valkeakoski': 7106, 'kr': 7107, 'vasantha': 7108, 'mba': 7109, 'blond': 7110, 'unlike': 7111, 'vibe': 7112, 'newsmorrisons': 7113, 'highlighted': 7114, 'kgroup': 7115, 'bavarian': 7116, 'dkk': 7117, 'det': 7118, 'jaemsaenkoski': 7119, 'equilibrium': 7120, 'spinoff': 7121, 'antonio': 7122, 'premature': 7123, 'alloy': 7124, 'ferrochrome': 7125, 'manganese': 7126, 'rugby': 7127, 'lutterworth': 7128, 'spillrelated': 7129, 'halliburton': 7130, 'transocean': 7131, 'fundredemption': 7132, 'liquidation': 7133, 'cleared': 7134, 'footing': 7135, 'briefaviva': 7136, 'serbia': 7137, 'portugal': 7138, 'repeal': 7139, 'agrees': 7140, 'merges': 7141, 'exempting': 7142, 'antidumping': 7143, 'pact': 7144, 'jimcramer': 7145, 'true': 7146, 'austin': 7147, 'compression': 7148, 'inculding': 7149, 'clausen': 7150, 'expired': 7151, 'muling': 7152, 'kemian': 7153, 'countdown': 7154, 'rainbow': 7155, 'trout': 7156, 'shad': 7157, 'rap': 7158, 'perch': 7159, 'capcom': 7160, 'evil': 7161, 'degeneration': 7162, 'ngage': 7163, 'brisk': 7164, 'commenting': 7165, 'shane': 7166, 'lennon': 7167, 'svp': 7168, 'stan': 7169, 'scotland': 7170, 'newswolseley': 7171, 'reslilience': 7172, 'secretary': 7173, 'mexican': 7174, 'sintra': 7175, 'buenavistacuautitlan': 7176, 'suburban': 7177, 'awaits': 7178, 'ssri': 7179, 'paas': 7180, 'enp': 7181, 'intellectual': 7182, 'blending': 7183, 'dcth': 7184, 'asdfllc': 7185, 'haha': 7186, 'funny': 7187, 'bvhealthcare': 7188, 'questcor': 7189, 'stkscogoan': 7190, 'zagg': 7191, 'avg': 7192, 'chartlylhatn': 7193, 'harkonen': 7194, 'pentti': 7195, 'sipponen': 7196, 'osmo': 7197, 'suovaniemi': 7198, 'tapani': 7199, 'tiusanen': 7200, 'extreme': 7201, 'bowl': 7202, 'cherry': 7203, 'fm': 7204, 'challendge': 7205, 'distant': 7206, 'nsn': 7207, 'stkscogft': 7208, 'oems': 7209, 'wimax': 7210, 'roiled': 7211, 'sepa': 7212, 'idcc': 7213, 'crunching': 7214, 'diamond': 7215, 'poznan': 7216, 'pestka': 7217, 'resposnible': 7218, 'marek': 7219, 'hintze': 7220, 'promotion': 7221, 'higherthanexpected': 7222, 'proof': 7223, 'pharmacodynamic': 7224, 'corroborate': 7225, 'dose': 7226, 'modification': 7227, 'effluent': 7228, 'follum': 7229, 'airway': 7230, 'stkscordu': 7231, 'cfrazierjr': 7232, 'fletchtrade': 7233, 'dramatically': 7234, 'northrhine': 7235, 'westphalia': 7236, 'bm': 7237, 'dilution': 7238, 'kennedy': 7239, 'laguardia': 7240, 'newark': 7241, 'teterboro': 7242, 'stewart': 7243, 'stalwart': 7244, 'fade': 7245, 'reportable': 7246, 'imsm': 7247, 'clearer': 7248, 'grainbased': 7249, 'tietoenators': 7250, 'vip': 7251, 'bypass': 7252, 'kotka': 7253, 'itkonen': 7254, 'distinct': 7255, 'supermercado': 7256, 'contributed': 7257, 'fundamental': 7258, 'priceshare': 7259, 'gratuitous': 7260, 'arab': 7261, 'qatar': 7262, 'pierrules': 7263, 'eco': 7264, 'driver': 7265, 'andrewnyquist': 7266, 'chartology': 7267, 'stkscoitr': 7268, 'nearing': 7269, 'magnus': 7270, 'telpak': 7271, 'ranjan': 7272, 'chaudhuri': 7273, 'garry': 7274, 'mcguire': 7275, 'rmg': 7276, 'fibertothehome': 7277, 'adidas': 7278, 'outdoors': 7279, 'birchbranch': 7280, 'coaster': 7281, 'moulding': 7282, 'renews': 7283, 'parquet': 7284, 'plyfa': 7285, 'hassela': 7286, 'velimatti': 7287, 'mattila': 7288, 'amd': 7289, 'dumping': 7290, 'reorganised': 7291, 'jam': 7292, 'rfid': 7293, 'identification': 7294, 'intelligent': 7295, 'retracement': 7296, 'tcowqqhkoy': 7297, 'kuzaj': 7298, 'sanna': 7299, 'paivaniemi': 7300, 'delist': 7301, 'furlough': 7302, 'reka': 7303, 'wfm': 7304, 'laitinen': 7305, 'triple': 7306, 'stkscoqwff': 7307, 'bosse': 7308, 'beneficial': 7309, 'upfront': 7310, 'noticeable': 7311, 'airtel': 7312, 'gaming': 7313, 'finn': 7314, 'vijver': 7315, 'humo': 7316, 'lhivakuutus': 7317, 'fierce': 7318, 'felled': 7319, 'suyv': 7320, 'stabler': 7321, 'tcojnbcvdcu': 7322, 'instead': 7323, 'worm': 7324, 'corrupt': 7325, 'stead': 7326, 'teho': 7327, 'tcodnhhgvv': 7328, 'mc': 7329, 'poundstretcher': 7330, 'intangible': 7331, 'janis': 7332, 'arbidans': 7333, 'celtnieciba': 7334, 'uniontown': 7335, 'pennsylvania': 7336, 'louis': 7337, 'missouri': 7338, 'durham': 7339, 'panfish': 7340, 'bluegill': 7341, 'babcock': 7342, 'corrensponds': 7343, 'modify': 7344, 'op': 7345, 'noncoherent': 7346, 'velta': 7347, 'veltabranded': 7348, 'bang': 7349, 'stkscodedm': 7350, 'getinge': 7351, 'stumbled': 7352, 'ofix': 7353, 'holy': 7354, 'guac': 7355, 'super': 7356, 'stkscoepx': 7357, 'count': 7358, 'ilmarinen': 7359, 'ver': 7360, 'pg': 7361, 'betting': 7362, 'imporvement': 7363, 'manipulation': 7364, 'krakeroy': 7365, 'bascule': 7366, 'prlb': 7367, 'mmx': 7368, 'defect': 7369, 'tcoyvydncdndi': 7370, 'sppi': 7371, 'sweet': 7372, 'justifies': 7373, 'tcofacnhtt': 7374, 'girder': 7375, 'ylivieska': 7376, 'essent': 7377, 'enexis': 7378, 'inverter': 7379, 'incorporating': 7380, 'domestically': 7381, 'neighboring': 7382, 'organically': 7383, 'fundam': 7384, 'tcoquysrqfox': 7385, 'justified': 7386, 'rop': 7387, 'practitioner': 7388, 'stanley': 7389, 'heavyduty': 7390, 'tyumen': 7391, 'chelyabinsk': 7392, 'khantymansi': 7393, 'autonomous': 7394, 'eurobond': 7395, 'groundbreaking': 7396, 'introduces': 7397, 'trx': 7398, 'stkscokkk': 7399, 'wwwstockmanncom': 7400, 'ropax': 7401, 'iterating': 7402, 'simulating': 7403, 'dlerch': 7404, 'fmcn': 7405, 'darn': 7406, 'chit': 7407, 'sherlock': 7408, 'impressed': 7409, 'tcosabxcsncd': 7410, 'tsrandall': 7411, 'arvo': 7412, 'vuorenmaa': 7413, 'greater': 7414, 'extent': 7415, 'ruling': 7416, 'ingen': 7417, 'marchin': 7418, 'lovin': 7419, 'esf': 7420, 'cupandhandle': 7421, 'shaping': 7422, 'endofday': 7423, 'favour': 7424, 'clubcard': 7425, 'bioview': 7426, 'automates': 7427, 'mississippi': 7428, 'swoope': 7429, 'mda': 7430, 'resilient': 7431, 'withstand': 7432, 'boomerang': 7433, 'einvoice': 7434, 'talo': 7435, 'robbielolz': 7436, 'etn': 7437, 'overweight': 7438, 'sightshort': 7439, 'chartlymlv': 7440, 'heindlmeyer': 7441, 'oblivion': 7442, 'recording': 7443, 'cadmodelling': 7444, 'sokopro': 7445, 'courier': 7446, 'gallup': 7447, 'haavisto': 7448, 'lumberman': 7449, 'timber': 7450, 'neya': 7451, 'keiju': 7452, 'makuisa': 7453, 'pyszny': 7454, 'duet': 7455, 'multi': 7456, 'questioned': 7457, 'maintained': 7458, 'standing': 7459, 'wording': 7460, 'normalises': 7461, 'borealis': 7462, 'constructed': 7463, 'sierra': 7464, 'ratewould': 7465, 'wonderful': 7466, 'marriage': 7467, 'penalty': 7468, 'rebuilds': 7469, 'disgraceful': 7470, 'advert': 7471, 'slapped': 7472, 'avista': 7473, 'partihallsfrbindelsen': 7474, 'typical': 7475, 'enduses': 7476, 'ceiling': 7477, 'nonvisible': 7478, 'fencing': 7479, 'formwork': 7480, 'pourings': 7481, 'stkscojpbn': 7482, 'thirteen': 7483, 'aldv': 7484, 'ilkkayhtym': 7485, 'nextgeneration': 7486, 'alfred': 7487, 'refurbishment': 7488, 'wayyy': 7489, 'overvalued': 7490, 'digit': 7491, 'gardening': 7492, 'gkn': 7493, 'fokker': 7494, 'baked': 7495, 'breakfast': 7496, 'cereal': 7497, 'rimvesta': 7498, 'estonianowned': 7499, 'ell': 7500, 'nekilnojamas': 7501, 'turtas': 7502, 'merko': 7503, 'trumpet': 7504, 'often': 7505, 'experimenting': 7506, 'ha': 7507, 'heeeeeere': 7508, 'pump': 7509, 'tconpakcugs': 7510, 'jeffries': 7511, 'dsov': 7512, 'guatemala': 7513, 'telgua': 7514, 'cdma': 7515, 'feedschroders': 7516, 'deriving': 7517, 'utilising': 7518, 'ulitsa': 7519, 'yamskogo': 7520, 'polya': 7521, 'ext': 7522, 'unimilk': 7523, 'xvi': 7524, 'businesssummit': 7525, 'lamp': 7526, 'harri': 7527, 'pending': 7528, 'removable': 7529, 'span': 7530, 'hospitality': 7531, 'realestate': 7532, 'nonperforming': 7533, 'restructurings': 7534, 'contempus': 7535, 'repaid': 7536, 'partially': 7537, 'repay': 7538, 'addus': 7539, 'assistance': 7540, 'skilled': 7541, 'nursing': 7542, 'rehabilitative': 7543, 'adanac': 7544, 'weekend': 7545, 'freezing': 7546, 'bonus': 7547, 'lehdentekijat': 7548, 'angelesbased': 7549, 'honolulu': 7550, 'diego': 7551, 'phoenix': 7552, 'parser': 7553, 'noticed': 7554, 'bob': 7555, 'langs': 7556, 'prothious': 7557, 'wwwprothiouscom': 7558, 'detailing': 7559, 'fundementals': 7560, 'poopoo': 7561, 'roshan': 7562, 'accrue': 7563, 'adjacent': 7564, 'savon': 7565, 'koulutuskuntayhtyma': 7566, 'joemccann': 7567, 'correleation': 7568, 'dx': 7569, 'clockwork': 7570, 'ncc': 7571, 'krona': 7572, 'marietta': 7573, 'ga': 7574, 'dnabased': 7575, 'retrofit': 7576, 'ppg': 7577, 'stkscoony': 7578, 'rimi': 7579, 'magistral': 7580, 'shore': 7581, 'dispels': 7582, 'equaling': 7583, 'stkscoldx': 7584, 'clearing': 7585, 'tagrisso': 7586, 'kep': 7587, 'stkscorcet': 7588, 'glut': 7589, 'capitex': 7590, 'pullbacktime': 7591, 'tcozmdgfbi': 7592, 'leif': 7593, 'quarto': 7594, 'willich': 7595, 'pressplate': 7596, 'prefab': 7597, 'avesta': 7598, 'agj': 7599, 'paijathame': 7600, 'mechanic': 7601, 'promoting': 7602, 'handled': 7603, 'srl': 7604, 'meant': 7605, 'preprocessing': 7606, 'laavainen': 7607, 'hansotto': 7608, 'scheck': 7609, 'identifying': 7610, 'satlan': 7611, 'broadcaster': 7612, 'iptv': 7613, 'igor': 7614, 'oleg': 7615, 'yankov': 7616, 'moron': 7617, 'vitim': 7618, 'dxcm': 7619, 'saarijarvi': 7620, 'odd': 7621, 'vosstaniya': 7622, 'choosing': 7623, 'fastjet': 7624, 'stelios': 7625, 'contractual': 7626, 'suffers': 7627, 'setback': 7628, 'fails': 7629, 'consulate': 7630, 'spoke': 7631, 'escape': 7632, 'accomplish': 7633, 'cathode': 7634, 'halfahair': 7635, 'locationbased': 7636, 'tcoqqlevvwpmv': 7637, 'tcofpdqjutha': 7638, 'assessing': 7639, 'stockholmheadquartered': 7640, 'effectiveness': 7641, 'tcoqxbkobphe': 7642, 'noon': 7643, 'koff': 7644, 'karhu': 7645, 'heikkil': 7646, 'refreshment': 7647, 'trygvesta': 7648, 'webmarela': 7649, 'sad': 7650, 'ilim': 7651, 'bratsk': 7652, 'dok': 7653, 'coordinated': 7654, 'attracts': 7655, 'ryanair': 7656, 'stkscocsr': 7657, 'poy': 7658, 'tanqia': 7659, 'fzc': 7660, 'ownerengineer': 7661, 'fujairah': 7662, 'liquide': 7663, 'exempted': 7664, 'mnc': 7665, 'sought': 7666, 'topic': 7667, 'spark': 7668, 'creativity': 7669, 'match': 7670, 'swot': 7671, 'categorization': 7672, 'bullya': 7673, 'pollux': 7674, 'kor': 7675, 'blessing': 7676, 'daughter': 7677, 'tuition': 7678, 'merkel': 7679, 'tokyomitsubishi': 7680, 'ufj': 7681, 'lifetree': 7682, 'cogeneration': 7683, 'secondbiggest': 7684, 'supervise': 7685, 'inve': 7686, 'pledge': 7687, 'heed': 7688, 'critic': 7689, 'automate': 7690, 'companiestesco': 7691, 'xmas': 7692, 'italahdenkatu': 7693, 'presented': 7694, 'hiidenheimo': 7695, 'stkscoboh': 7696, 'bore': 7697, 'rettig': 7698, 'buyable': 7699, 'yakima': 7700, 'downturn': 7701, 'nord': 7702, 'pietiek': 7703, 'cmon': 7704, 'primetass': 7705, 'chu': 7706, 'hkse': 7707, 'shse': 7708, 'newlycompleted': 7709, 'allure': 7710, 'oasis': 7711, 'ghspa': 7712, 'java': 7713, 'bali': 7714, 'sumatra': 7715, 'batam': 7716, 'crude': 7717, 'separationfiltration': 7718, 'tourist': 7719, 'thiam': 7720, 'saavalainen': 7721, 'gibtelecom': 7722, 'illustration': 7723, 'tcobgvintkh': 7724, 'gapping': 7725, 'stkscoqjau': 7726, 'tradeideas': 7727, 'stkscorjc': 7728, 'christopher': 7729, 'wynne': 7730, 'papa': 7731, 'sfi': 7732, 'thermal': 7733, 'spraying': 7734, 'janno': 7735, 'reiljan': 7736, 'narva': 7737, 'delhaize': 7738, 'vocal': 7739, 'wt': 7740, 'anglo': 7741, 'aal': 7742, 'matkatoimisto': 7743, 'smt': 7744, 'bricksandmortar': 7745, 'myspacecom': 7746, 'submission': 7747, 'emmy': 7748, 'identify': 7749, 'aspiring': 7750, 'artist': 7751, 'eleven': 7752, 'leaked': 7753, 'localisation': 7754, 'provision': 7755, 'downsizing': 7756, 'penttil': 7757, 'kaluga': 7758, 'lindsey': 7759, 'liddell': 7760, 'negotiated': 7761, 'regard': 7762, 'ingenious': 7763, 'coutts': 7764, 'avoidance': 7765, 'ascs': 7766, 'pounded': 7767, 'stkscotzt': 7768, 'platts': 7769, 'elite': 7770, 'residence': 7771, 'tameer': 7772, 'marina': 7773, 'iressa': 7774, 'bjrn': 7775, 'tyc': 7776, 'pkcv': 7777, 'nutzfahrzeuge': 7778, 'harness': 7779, 'spolka': 7780, 'zoo': 7781, 'rohwedder': 7782, 'agriculture': 7783, 'helirvaldor': 7784, 'seeder': 7785, 'counterpart': 7786, 'sirkkaliisa': 7787, 'anttila': 7788, 'donald': 7789, 'brydon': 7790, 'bestofbreed': 7791, 'academic': 7792, 'android': 7793, 'twofactor': 7794, 'breakable': 7795, 'tconjxmnrglo': 7796, 'locator': 7797, 'hangin': 7798, 'thread': 7799, 'cliff': 7800, 'snapback': 7801, 'vix': 7802, 'positioned': 7803, 'rugv': 7804, 'subsidy': 7805, 'lngfuelled': 7806, 'flange': 7807, 'lloydspharmacy': 7808, 'flurry': 7809, 'doublechecked': 7810, 'aberration': 7811, 'manavigatorseptember': 7812, 'bom': 7813, 'yaroslavl': 7814, 'dojiknows': 7815, 'sp': 7816, 'qnx': 7817, 'bbm': 7818, 'coo': 7819, 'locked': 7820, 'defence': 7821, 'attacker': 7822, 'detect': 7823, 'capturing': 7824, 'grabbing': 7825, 'keylogging': 7826, 'injection': 7827, 'cndo': 7828, 'cpix': 7829, 'pricier': 7830, 'hopefully': 7831, 'stkscoeh': 7832, 'ryandetrick': 7833, 'assuming': 7834, 'weigh': 7835, 'twenty': 7836, 'ridge': 7837, 'justin': 7838, 'bao': 7839, 'yiliang': 7840, 'karczewicz': 7841, 'marta': 7842, 'xnsv': 7843, 'ultrascan': 7844, 'cracker': 7845, 'password': 7846, 'sort': 7847, 'impulse': 7848, 'umbrella': 7849, 'purse': 7850, 'shopper': 7851, 'commit': 7852, 'gme': 7853, 'quadriga': 7854, 'lewa': 7855, 'pumpmaking': 7856, 'nikkiso': 7857, 'identified': 7858, 'mmh': 7859, 'midswaps': 7860, 'casino': 7861, 'lvs': 7862, 'unusual': 7863, 'comibinations': 7864, 'barcelona': 7865, 'irrevocable': 7866, 'kansa': 7867, 'fastenal': 7868, 'tcojhqpwmn': 7869, 'noav': 7870, 'frenzy': 7871, 'manipulator': 7872, 'knight': 7873, 'possession': 7874, 'stkscotmu': 7875, 'ouch': 7876, 'skinned': 7877, 'toppy': 7878, 'imho': 7879, 'caution': 7880, 'bumpy': 7881, 'unique': 7882, 'cbs': 7883, 'cpxx': 7884, 'tcoiridzdgc': 7885, 'cbd': 7886, 'bangkok': 7887, 'tcozbkvstl': 7888, 'hourly': 7889, 'formal': 7890, 'statistical': 7891, 'hznp': 7892, 'havent': 7893, 'sina': 7894, 'untrue': 7895, 'unture': 7896, 'attributed': 7897, 'forwarding': 7898, 'deinking': 7899, 'removed': 7900, 'worse': 7901, 'postponement': 7902, 'skh': 7903, 'stkscoe': 7904, 'embrace': 7905, 'tcowcqfptn': 7906, 'abbv': 7907, 'wyn': 7908, 'amcn': 7909, 'shake': 7910, 'proforma': 7911, 'fingerfriendly': 7912, 'stkscomir': 7913, 'postions': 7914, 'zurich': 7915, 'dover': 7916, 'calais': 7917, 'rauma': 7918, 'suistio': 7919, 'latch': 7920, 'crest': 7921, 'milk': 7922, 'rlnordic': 7923, 'raiffeisenbankinggroup': 7924, 'raiffeisenleasing': 7925, 'cymed': 7926, 'brinkab': 7927, 'subjectmatter': 7928, 'cyclone': 7929, 'molten': 7930, 'particle': 7931, 'pusher': 7932, 'barge': 7933, 'osuuskauppa': 7934, 'suursavo': 7935, 'gyrating': 7936, 'stkscojqfb': 7937, 'ivan': 7938, 'seidenberg': 7939, 'dnkn': 7940, 'kkd': 7941, 'loco': 7942, 'stkscohvv': 7943, 'stkscorebn': 7944, 'dismisses': 7945, 'lufthansa': 7946, 'contest': 7947, 'sudden': 7948, 'optimism': 7949, 'ie': 7950, 'tcooktcwyib': 7951, 'roadshow': 7952, 'avoid': 7953, 'eln': 7954, 'slw': 7955, 'jmho': 7956, 'stkscoqfg': 7957, 'seeking': 7958, 'luumaki': 7959, 'sandbagged': 7960, 'cautiously': 7961, 'til': 7962, 'medimmune': 7963, 'omnis': 7964, 'schmardin': 7965, 'tightening': 7966, 'guineabased': 7967, 'lgl': 7968, 'rogers': 7969, 'rallied': 7970, 'wk': 7971, 'tcozwhthsvfsf': 7972, 'shed': 7973, 'watertight': 7974, 'vana': 7975, 'regina': 7976, 'baltica': 7977, 'potts': 7978, 'stall': 7979, 'verification': 7980, 'cast': 7981, 'election': 7982, 'logo': 7983, 'iraq': 7984, 'handout': 7985, 'chevron': 7986, 'fail': 7987, 'tarmo': 7988, 'hameenlinna': 7989, 'ugaz': 7990, 'dent': 7991, 'massive': 7992, 'stkscopvp': 7993, 'jorma': 7994, 'j': 7995, 'takanen': 7996, 'ekaterinburg': 7997, 'kazan': 7998, 'rostovondon': 7999, 'stadtverwaltung': 8000, 'mainz': 8001, 'chassis': 8002, 'pressuring': 8003, 'ssys': 8004, 'retested': 8005, 'companiesab': 8006, 'kengeter': 8007, 'stt': 8008, 'andy': 8009, 'harrison': 8010, 'blip': 8011, 'allonen': 8012, 'pointing': 8013, 'overlapping': 8014, 'rebuff': 8015, 'gakrum': 8016, 'ceivd': 8017, 'warranty': 8018, 'koistinen': 8019, 'ranking': 8020, 'gnrc': 8021, 'belttongroup': 8022, 'tcoycnclcbimv': 8023, 'ranked': 8024, 'symphony': 8025, 'invalidated': 8026, 'blindspots': 8027, 'vantage': 8028, 'unbroken': 8029, 'intermodal': 8030, 'legrand': 8031, 'silvennoinen': 8032, 'optionally': 8033, 'assist': 8034, 'datasecurity': 8035, 'emc': 8036, 'sndk': 8037, 'tcovkydwpxesn': 8038, 'sizing': 8039, 'erection': 8040, 'sinking': 8041, 'thanksto': 8042, 'theutilization': 8043, 'substantiallyimproved': 8044, 'mtd': 8045, 'mettlertoledo': 8046, 'hankkijamaatalous': 8047, 'helander': 8048, 'proceed': 8049, 'swiftly': 8050, 'jaakko': 8051, 'vilo': 8052, 'conducting': 8053, 'dukat': 8054, 'palace': 8055, 'bracing': 8056, 'certainly': 8057, 'confirming': 8058, 'reorganization': 8059, 'longestablished': 8060, 'mgkg': 8061, 'tolerated': 8062, 'pharmacokinetic': 8063, 'characteristic': 8064, 'btt': 8065, 'rheumatoid': 8066, 'arthritis': 8067, 'eq': 8068, 'hyg': 8069, 'stkscotqp': 8070, 'mbly': 8071, 'tcojspuszona': 8072, 'desk': 8073, 'tcomhobzurmx': 8074, 'lynn': 8075, 'shanahan': 8076, 'lawmaker': 8077, 'miniature': 8078, 'slitting': 8079, 'moskovia': 8080, 'ostroleka': 8081, 'mi': 8082, 'aet': 8083, 'pulpwood': 8084, 'wherever': 8085, 'blyk': 8086, 'philadelphia': 8087, 'nonrestricted': 8088, 'eurusd': 8089, 'ero': 8090, 'udn': 8091, 'stkscobeu': 8092, 'designation': 8093, 'speculative': 8094, 'lakeville': 8095, 'lnt': 8096, 'rsol': 8097, 'marketsshire': 8098, 'nederland': 8099, 'bv': 8100, 'cnl': 8101, 'jaston': 8102, 'groep': 8103, 'lubricant': 8104, 'il': 8105, 'moluballoy': 8106, 'establishment': 8107, 'uplift': 8108, 'crucially': 8109, 'etsi': 8110, 'backed': 8111, 'dvbh': 8112, 'opting': 8113, 'vie': 8114, 'stena': 8115, 'poseidon': 8116, 'panamax': 8117, 'passage': 8118, 'panama': 8119, 'pwc': 8120, 'pollution': 8121, 'scandal': 8122, 'infection': 8123, 'prompt': 8124, 'gilead': 8125, 'leukaemia': 8126, 'tcohvqxpzg': 8127, 'denies': 8128, 'starwood': 8129, 'aside': 8130, 'wi': 8131, 'prnewswire': 8132, 'scissor': 8133, 'vehvilinen': 8134, 'foremost': 8135, 'isoprene': 8136, 'specie': 8137, 'hevea': 8138, 'brasiliensis': 8139, 'rubber': 8140, 'nav': 8141, 'prerequisite': 8142, 'receipt': 8143, 'westend': 8144, 'unloader': 8145, 'conveying': 8146, 'mpwr': 8147, 'meh': 8148, 'cablevision': 8149, 'cvc': 8150, 'knickerbockers': 8151, 'ranger': 8152, 'liberty': 8153, 'woman': 8154, 'hartford': 8155, 'wolf': 8156, 'excerpt': 8157, 'onty': 8158, 'debate': 8159, 'lingers': 8160, 'ont': 8161, 'curve': 8162, 'dietz': 8163, 'stahlberg': 8164, 'vp': 8165, 'lab': 8166, 'avcomparatives': 8167, 'reczakaria': 8168, 'ticker': 8169, 'james': 8170, 'ent': 8171, 'gvp': 8172, 'prevailing': 8173, 'affordable': 8174, 'tco': 8175, 'bnpp': 8176, 'incident': 8177, 'comprised': 8178, 'carrierrelated': 8179, 'rautakesko': 8180, 'impacting': 8181, 'bylander': 8182, 'johan': 8183, 'ponten': 8184, 'fredrik': 8185, 'lundberg': 8186, 'jorgen': 8187, 'significance': 8188, 'emphasised': 8189, 'recommend': 8190, 'adopting': 8191, 'wwwupmkymmenecom': 8192, 'ghg': 8193, 'overseas': 8194, 'veracel': 8195, 'clarification': 8196, 'pipettor': 8197, 'iceland': 8198, 'troubled': 8199, 'eseries': 8200, 'stylised': 8201, 'dave': 8202, 'grannan': 8203, 'machineroomless': 8204, 'monospace': 8205, 'occupancy': 8206, 'cobalt': 8207, 'norilsk': 8208, 'eat': 8209, 'briefing': 8210, 'bargainpriced': 8211, 'audience': 8212, 'miguel': 8213, 'kirin': 8214, 'outlining': 8215, 'megadeals': 8216, 'rmb': 8217, 'tremendous': 8218, 'pankaj': 8219, 'kedia': 8220, 'ecosystem': 8221, 'gasfuelled': 8222, 'propulsion': 8223, 'vocollect': 8224, 'discounted': 8225, 'camt': 8226, 'ren': 8227, 'bottoming': 8228, 'objectively': 8229, 'drugmakers': 8230, 'heated': 8231, 'therein': 8232, 'nvestor': 8233, 'soda': 8234, 'stkscoedfp': 8235, 'lietuvos': 8236, 'respublikos': 8237, 'sveikatos': 8238, 'apsaugos': 8239, 'ministerija': 8240, 'uab': 8241, 'representation': 8242, 'vacv': 8243, 'amending': 8244, 'simplifies': 8245, 'stkscolpz': 8246, 'chartlylgbrr': 8247, 'compnay': 8248, 'robert': 8249, 'mcalpine': 8250, 'quarterended': 8251, 'thesecond': 8252, 'taylor': 8253, 'wimpey': 8254, 'bit': 8255, 'diligence': 8256, 'codenamed': 8257, 'aspect': 8258, 'abovementioned': 8259, 'geenrated': 8260, 'derives': 8261, 'seem': 8262, 'pekkarinen': 8263, 'amtd': 8264, 'longerterm': 8265, 'protrusion': 8266, 'exert': 8267, 'riser': 8268, 'maine': 8269, 'ntc': 8270, 'bohemia': 8271, 'czechrepublic': 8272, 'headquarter': 8273, 'reding': 8274, 'bratislava': 8275, 'liquor': 8276, 'quartal': 8277, 'agricole': 8278, 'epa': 8279, 'aca': 8280, 'cagr': 8281, 'bumping': 8282, 'overhead': 8283, 'stadigh': 8284, 'arteva': 8285, 'vicechairman': 8286, 'rsh': 8287, 'volc': 8288, 'probability': 8289, 'rallying': 8290, 'andor': 8291, 'forwarded': 8292, 'unknown': 8293, 'logged': 8294, 'tcodqfuzozylh': 8295, 'tcoggneayzqn': 8296, 'cgrantwsj': 8297, 'bubble': 8298, 'ppl': 8299, 'predictability': 8300, 'trustworthy': 8301, 'cue': 8302, 'playbook': 8303, 'dillon': 8304, 'heavyhitting': 8305, 'bootcut': 8306, 'jean': 8307, 'tummy': 8308, 'cowboy': 8309, 'stkscoit': 8310, 'hypotek': 8311, 'emergingmarkets': 8312, 'photonium': 8313, 'akseli': 8314, 'ukbased': 8315, 'consequence': 8316, 'withdrawn': 8317, 'twentyone': 8318, 'mids': 8319, 'browsing': 8320, 'tsco': 8321, 'tcobmvhcrwrea': 8322, 'rik': 8323, 'ferguson': 8324, 'retailed': 8325, 'organize': 8326, 'wtc': 8327, 'marski': 8328, 'aleksanterinkatu': 8329, 'lunch': 8330, 'incorporate': 8331, 'luxembourgregistered': 8332, 'amber': 8333, 'sanitas': 8334, 'signaled': 8335, 'tcovmdmxi': 8336, 'tcocvkojkxn': 8337, 'israel': 8338, 'wwwglobesonlinecom': 8339, 'itonut': 8340, 'pitprodukt': 8341, 'chicagobased': 8342, 'database': 8343, 'macquarie': 8344, 'tcoxcngwstq': 8345, 'korpinen': 8346, 'apac': 8347, 'demonstrating': 8348, 'timetomarket': 8349, 'cardona': 8350, 'intersection': 8351, 'sinappi': 8352, 'avgo': 8353, 'stk': 8354, 'stkscorfh': 8355, 'lsc': 8356, 'morna': 8357, 'cowie': 8358, 'lovely': 8359, 'continueskantar': 8360, 'ryymin': 8361, 'kaisanlahti': 8362, 'evp': 8363, 'welfare': 8364, 'antniemi': 8365, 'refuted': 8366, 'reorganizing': 8367, 'tcoxklzuik': 8368, 'rxii': 8369, 'tang': 8370, 'dragging': 8371, 'veliky': 8372, 'ustjug': 8373, 'oblast': 8374, 'fullyleased': 8375, 'wwwsampocomir': 8376, 'lule': 8377, 'equitybacked': 8378, 'rationalization': 8379, 'companiesnew': 8380, 'aggreko': 8381, 'surprising': 8382, 'sks': 8383, 'depository': 8384, 'vahur': 8385, 'cooperating': 8386, 'tnh': 8387, 'explosive': 8388, 'stkscobsvh': 8389, 'entitles': 8390, 'workday': 8391, 'contain': 8392, 'nutritional': 8393, 'outflow': 8394, 'rewarded': 8395, 'distinguished': 8396, 'wwwseahawkdrillingcom': 8397, 'tab': 8398, 'tzoo': 8399, 'aol': 8400, 'weblogs': 8401, 'upstaged': 8402, 'businessclass': 8403, 'promo': 8404, 'annulled': 8405, 'stavo': 8406, 'stavokonsult': 8407, 'kostiainen': 8408, 'africaand': 8409, 'athn': 8410, 'chrm': 8411, 'loooooongggggg': 8412, 'partition': 8413, 'tcotdgjedllz': 8414, 'niina': 8415, 'nenonen': 8416, 'arna': 8417, 'transported': 8418, 'whereas': 8419, 'ticaret': 8420, 'sanayi': 8421, 'dominating': 8422, 'longdistance': 8423, 'admired': 8424, 'vivid': 8425, 'unified': 8426, 'auditing': 8427, 'donate': 8428, 'cpps': 8429, 'undertaken': 8430, 'oriented': 8431, 'eou': 8432, 'khopoli': 8433, 'vesa': 8434, 'laisi': 8435, 'stockholmbased': 8436, 'hired': 8437, 'casper': 8438, 'earnout': 8439, 'fantuzzi': 8440, 'tomsk': 8441, 'furusund': 8442, 'etera': 8443, 'infusion': 8444, 'patch': 8445, 'derogating': 8446, 'preemptive': 8447, 'disturbing': 8448, 'prolonged': 8449, 'diameter': 8450, 'wwwruukkicominvestors': 8451, 'wasteburning': 8452, 'persistently': 8453, 'mxwl': 8454, 'oh': 8455, 'mugatushair': 8456, 'bergvik': 8457, 'earns': 8458, 'appreciation': 8459, 'prgn': 8460, 'historically': 8461, 'introducing': 8462, 'dubna': 8463, 'lhikauppa': 8464, 'mukkavilli': 8465, 'krishna': 8466, 'kiran': 8467, 'sabharwal': 8468, 'ashutosh': 8469, 'aazhang': 8470, 'behnaam': 8471, 'tconkwecrba': 8472, 'esi': 8473, 'stkscotad': 8474, 'voip': 8475, 'shdsl': 8476, 'landed': 8477, 'shark': 8478, 'stkscoiggh': 8479, 'varkaus': 8480, 'utilizing': 8481, 'willingness': 8482, 'pink': 8483, 'pojlf': 8484, 'swift': 8485, 'bsx': 8486, 'nq': 8487, 'stkscosz': 8488, 'kara': 8489, 'raiguel': 8490, 'woodworking': 8491, 'sheksna': 8492, 'numerous': 8493, 'boardman': 8494, 'housebuilder': 8495, 'cairn': 8496, 'flexiblebaseloadoperation': 8497, 'thewartsilagenerating': 8498, 'gasconversions': 8499, 'shorter': 8500, 'exbarclays': 8501, 'illegal': 8502, 'plumber': 8503, 'tomas': 8504, 'bbcn': 8505, 'wilshire': 8506, 'rg': 8507, 'tcoteaexxpe': 8508, 'teacher': 8509, 'classroom': 8510, 'twofold': 8511, 'zahariev': 8512, 'abandon': 8513, 'videostreaming': 8514, 'ambition': 8515, 'tcoxfnicwebeg': 8516, 'geography': 8517, 'megaas': 8518, 'mcdonalds': 8519, 'bigmac': 8520, 'improbe': 8521, 'kopijyva': 8522, 'sokonet': 8523, 'wellmont': 8524, 'advisor': 8525, 'archived': 8526, 'cerner': 8527, 'wwwcernercom': 8528, 'conversation': 8529, 'opk': 8530, 'goodyear': 8531, 'bridgestone': 8532, 'pirelli': 8533, 'stefan': 8534, 'tcolsahepdqhd': 8535, 'stocktrading': 8536, 'enters': 8537, 'browse': 8538, 'activating': 8539, 'talv': 8540, 'bio': 8541, 'yoku': 8542, 'stkscoqsal': 8543, 'workout': 8544, 'pleasurable': 8545, 'chore': 8546, 'dean': 8547, 'faculty': 8548, 'tkk': 8549, 'writing': 8550, 'tuuri': 8551, 'relinquishing': 8552, 'bat': 8553, 'branding': 8554, 'stkscoggcq': 8555, 'castecka': 8556, 'penttila': 8557, 'paviljonki': 8558, 'classified': 8559, 'riskware': 8560, 'kitron': 8561, 'kit': 8562, 'nst': 8563, 'semiproducts': 8564, 'tcojlgrprrk': 8565, 'gundlach': 8566, 'mini': 8567, 'stkscobdqv': 8568, 'renew': 8569, 'ooks': 8570, 'sliding': 8571, 'roaster': 8572, 'ozk': 8573, 'kardzhali': 8574, 'outpacing': 8575, 'berling': 8576, 'umo': 8577, 'veikko': 8578, 'motlanthe': 8579, 'stuk': 8580, 'petersburgbased': 8581, 'hear': 8582, 'flexibility': 8583, 'mayawas': 8584, 'ufa': 8585, 'fremantlemedia': 8586, 'advertised': 8587, 'stkscobkd': 8588, 'artv': 8589, 'constitutes': 8590, 'hspa': 8591, 'wifi': 8592, 'manturovo': 8593, 'northeast': 8594, 'ignatius': 8595, 'incl': 8596, 'serco': 8597, 'roy': 8598, 'gardner': 8599, 'excentrica': 8600, 'galeria': 8601, 'podlaska': 8602, 'bia': 8603, 'ystok': 8604, 'downstream': 8605, 'encompassing': 8606, 'kaarstroem': 8607, 'tt': 8608, 'safe': 8609, 'programmatic': 8610, 'essence': 8611, 'feedersupply': 8612, 'reflect': 8613, 'applauded': 8614, 'applying': 8615, 'factbased': 8616, 'datadriven': 8617, 'airvana': 8618, 'femto': 8619, 'softwarebased': 8620, 'offtheshelf': 8621, 'fulfilment': 8622, 'commerzbank': 8623, 'hamburg': 8624, 'arranger': 8625, 'inga': 8626, 'coarranger': 8627, 'fired': 8628, 'spruce': 8629, 'bark': 8630, 'chipped': 8631, 'milled': 8632, 'te': 8633, 'surgical': 8634, 'tcovahhjacbfg': 8635, 'tcohqtqxou': 8636, 'dump': 8637, 'gale': 8638, 'keith': 8639, 'skeoch': 8640, 'nish': 8641, 'quits': 8642, 'outplacement': 8643, 'tcoyejuscmlc': 8644, 'tcottmrnfoxj': 8645, 'strategically': 8646, 'repositioning': 8647, 'relaunch': 8648, 'virkkala': 8649, 'tcoqciiyo': 8650, 'divests': 8651, 'rumilly': 8652, 'arehu': 8653, 'stabilise': 8654, 'auburn': 8655, 'converted': 8656, 'bitmapped': 8657, 'insolvency': 8658, 'regulates': 8659, 'permitted': 8660, 'hal': 8661, 'lucky': 8662, 'enuff': 8663, 'stkscorus': 8664, 'noting': 8665, 'brno': 8666, 'vintage': 8667, 'cushion': 8668, 'pot': 8669, 'cocktail': 8670, 'rimm': 8671, 'nya': 8672, 'compq': 8673, 'ingerois': 8674, 'rebuffal': 8675, 'manty': 8676, 'typhoon': 8677, 'witty': 8678, 'instrumentation': 8679, 'krister': 8680, 'kylas': 8681, 'axdx': 8682, 'bored': 8683, 'lousy': 8684, 'spite': 8685, 'systeemitiimi': 8686, 'skogberg': 8687, 'understood': 8688, 'leerink': 8689, 'pohjantahti': 8690, 'solel': 8691, 'costeffective': 8692, 'highestquality': 8693, 'reflector': 8694, 'garner': 8695, 'londonbased': 8696, 'tiein': 8697, 'dramatic': 8698, 'rush': 8699, 'pohjoa': 8700, 'leadmanagers': 8701, 'prefriends': 8702, 'cnbc': 8703, 'lundbeck': 8704, 'southkorea': 8705, 'joerazorback': 8706, 'journal': 8707, 'tcojjvjibia': 8708, 'normandy': 8709, 'midfrance': 8710, 'intensive': 8711, 'rake': 8712, 'sakari': 8713, 'tamminen': 8714, 'principally': 8715, 'divest': 8716, 'lonmin': 8717, 'suggest': 8718, 'planmill': 8719, 'cdti': 8720, 'stkscocmf': 8721, 'dinosaur': 8722, 'omitted': 8723, 'rightfully': 8724, 'nonresponsive': 8725, 'gavrilov': 8726, 'sodium': 8727, 'nitrite': 8728, 'antwerp': 8729, 'commerciallyfunded': 8730, 'knowledgeintensive': 8731, 'chime': 8732, 'providence': 8733, 'overproduction': 8734, 'cypress': 8735, 'campofrio': 8736, 'groupe': 8737, 'sfd': 8738, 'sfo': 8739, 'gogol': 8740, 'kenneth': 8741, 'bower': 8742, 'vista': 8743, 'tcofaatoqlw': 8744, 'onemain': 8745, 'springleaf': 8746, 'neither': 8747, 'threeday': 8748, 'biotherapeutics': 8749, 'prospectus': 8750, 'revised': 8751, 'ukonaho': 8752, 'mulling': 8753, 'koreabased': 8754, 'atlas': 8755, 'ftc': 8756, 'nobody': 8757, 'lgu': 8758, 'unquoted': 8759, 'retain': 8760, 'dissolve': 8761, 'scandic': 8762, 'scholarship': 8763, 'hillier': 8764, 'multiasset': 8765, 'acquitted': 8766, 'pricefixing': 8767, 'conspiracy': 8768, 'sitronics': 8769, 'mikron': 8770, 'microelectronic': 8771, 'highpower': 8772, 'seismic': 8773, 'compressor': 8774, 'skx': 8775, 'worldleading': 8776, 'genuine': 8777, 'virpi': 8778, 'kuitunen': 8779, 'froh': 8780, 'arques': 8781, 'lanebrook': 8782, 'beneficiary': 8783, 'millhouse': 8784, 'billionaire': 8785, 'roman': 8786, 'abramovich': 8787, 'abramov': 8788, 'frolov': 8789, 'activeness': 8790, 'tomato': 8791, 'carrot': 8792, 'fortune': 8793, 'behavioural': 8794, 'scoring': 8795, 'migration': 8796, 'legacy': 8797, 'netact': 8798, 'robust': 8799, 'hervy': 8800, 'personally': 8801, 'dcuc': 8802, 'portsmouth': 8803, 'nhgordon': 8804, 'hail': 8805, 'raffle': 8806, 'archer': 8807, 'wastefired': 8808, 'brista': 8809, 'biofuelsbased': 8810, 'extendedhours': 8811, 'comtex': 8812, 'crk': 8813, 'analyzed': 8814, 'wiklof': 8815, 'weighted': 8816, 'iaci': 8817, 'stkscotju': 8818, 'commanding': 8819, 'turnbyturn': 8820, 'onstar': 8821, 'phil': 8822, 'magney': 8823, 'telematics': 8824, 'minnetonka': 8825, 'minn': 8826, 'rationalize': 8827, 'cosco': 8828, 'dohle': 8829, 'cido': 8830, 'uutislehti': 8831, 'midseptember': 8832, 'vinachem': 8833, 'occasion': 8834, 'assessment': 8835, 'artemis': 8836, 'prepares': 8837, 'postal': 8838, 'regulation': 8839, 'metsliitto': 8840, 'mcfarlane': 8841, 'reproduction': 8842, 'tconxbplje': 8843, 'restructured': 8844, 'nokiabranded': 8845, 'battery': 8846, 'lemann': 8847, 'nursery': 8848, 'connors': 8849, 'twoyear': 8850, 'traveller': 8851, 'brealout': 8852, 'bestinclass': 8853, 'entering': 8854, 'lockheed': 8855, 'stkscoexta': 8856, 'billionplus': 8857, 'pledged': 8858, 'render': 8859, 'lisle': 8860, 'ill': 8861, 'categorically': 8862, 'recommendable': 8863, 'inaugurates': 8864, 'inaugurating': 8865, 'bottomline': 8866, 'locomotive': 8867, 'mpra': 8868, 'kraski': 8869, 'cog': 8870, 'exists': 8871, 'artemyev': 8872, 'eone': 8873, 'rubbertyred': 8874, 'rtg': 8875, 'saigon': 8876, 'newport': 8877, 'snp': 8878, 'sh': 8879, 'potato': 8880, 'iii': 8881, 'abbott': 8882, 'andalusia': 8883, 'tweeple': 8884, 'clicking': 8885, 'tinyurls': 8886, 'utilise': 8887, 'catskillfishing': 8888, 'recaptured': 8889, 'stkscoawz': 8890, 'icelandair': 8891, 'bluebird': 8892, 'pcmagcom': 8893, 'kaivooja': 8894, 'economics': 8895, 'drawn': 8896, 'denver': 8897, 'stkscocyn': 8898, 'gram': 8899, 'vga': 8900, 'megabyte': 8901, 'memory': 8902, 'expandable': 8903, 'slot': 8904, 'underutilisation': 8905, 'tcomkmemtadt': 8906, 'kazgiprotsvetmet': 8907, 'tallinnhelsinki': 8908, 'streetinsider': 8909, 'stkscoyye': 8910, 'riproaring': 8911, 'investorplace': 8912, 'tcojrcfcxly': 8913, 'pave': 8914, 'burton': 8915, 'dampened': 8916, 'imrs': 8917, 'pragmatic': 8918, 'headwind': 8919, 'evenif': 8920, 'app': 8921, 'hunting': 8922, 'polled': 8923, 'brick': 8924, 'nvda': 8925, 'sideways': 8926, 'technik': 8927, 'reluctant': 8928, 'egeszsegbolt': 8929, 'gustav': 8930, 'internationalizing': 8931, 'regains': 8932, 'doubt': 8933, 'limestone': 8934, 'breeze': 8935, 'sintered': 8936, 'lump': 8937, 'ironmaking': 8938, 'temv': 8939, 'boy': 8940, 'genius': 8941, 'huh': 8942, 'terrain': 8943, 'rtch': 8944, 'dmnd': 8945, 'endaugust': 8946, 'collated': 8947, 'blocking': 8948, 'fias': 8949, 'nineyear': 8950, 'leaseback': 8951, 'syndication': 8952, 'serial': 8953, 'irregularity': 8954, 'dokumentori': 8955, 'mondi': 8956, 'barratt': 8957, 'eukor': 8958, 'rollon': 8959, 'rolloff': 8960, 'wallhamn': 8961, 'worldclass': 8962, 'karara': 8963, 'luv': 8964, 'phew': 8965, 'coz': 8966, 'multiplying': 8967, 'malkia': 8968, 'tallinna': 8969, 'sadam': 8970, 'kraftheinz': 8971, 'speedy': 8972, 'finnfund': 8973, 'alteams': 8974, 'ebita': 8975, 'telekom': 8976, 'boston': 8977, 'cfsb': 8978, 'corrected': 8979, 'alphatrends': 8980, 'vmarkets': 8981, 'stkscoagn': 8982, 'indigo': 8983, 'somoncom': 8984, 'alternately': 8985, 'vacant': 8986, 'constantly': 8987, 'saunalahti': 8988, 'acquirer': 8989, 'vyvanse': 8990, 'bingeeating': 8991, 'disorder': 8992, 'pehulehtonen': 8993, 'kretailer': 8994, 'trainee': 8995, 'qualified': 8996, 'kstores': 8997, 'taxpayer': 8998, 'gathered': 8999, 'sihvonen': 9000, 'redeemed': 9001, 'poyv': 9002, 'venezuel': 9003, 'filling': 9004, 'boring': 9005, 'fti': 9006, 'night': 9007, 'tcoextzrriyp': 9008, 'stkscotvel': 9009, 'crox': 9010, 'gogo': 9011, 'sinocast': 9012, 'province': 9013, 'haapakoski': 9014, 'editorinchief': 9015, 'satin': 9016, 'styled': 9017, 'outfit': 9018, 'worn': 9019, 'beckham': 9020, 'prom': 9021, 'basestations': 9022, 'towertop': 9023, 'microwave': 9024, 'horrible': 9025, 'huhtamki': 9026, 'moisio': 9027, 'yekaterinburg': 9028, 'flying': 9029, 'kpy': 9030, 'voimatel': 9031, 'stuttgart': 9032, 'sunday': 9033, 'comcast': 9034, 'nbcuniversal': 9035, 'strictly': 9036, 'regulated': 9037, 'divestments': 9038, 'redelivery': 9039, 'tonnage': 9040, 'investrend': 9041, 'broadcast': 9042, 'firstalert': 9043, 'oefirstalert': 9044, 'specifying': 9045, 'recruited': 9046, 'stavebni': 9047, 'doprava': 9048, 'mechanizace': 9049, 'vivus': 9050, 'tapro': 9051, 'cytx': 9052, 'finfiz': 9053, 'stkscofr': 9054, 'coupled': 9055, 'nibbling': 9056, 'fenestra': 9057, 'goodwin': 9058, 'scottish': 9059, 'prosecution': 9060, 'bone': 9061, 'stkscozwe': 9062, 'erp': 9063, 'nihd': 9064, 'insider': 9065, 'improves': 9066, 'relacom': 9067, 'bawag': 9068, 'kapuli': 9069, 'mantsala': 9070, 'beside': 9071, 'hankomantsalaporvoo': 9072, 'jarvenpaa': 9073, 'valid': 9074, 'den': 9075, 'boschbased': 9076, 'cayman': 9077, 'islandsbased': 9078, 'convert': 9079, 'technologically': 9080, 'merchandise': 9081, 'wintek': 9082, 'west': 9083, 'uwe': 9084, 'bakosch': 9085, 'yorkbased': 9086, 'html': 9087, 'dinner': 9088, 'wwwcityconcom': 9089, 'unveiled': 9090, 'resurrect': 9091, 'ailing': 9092, 'lansio': 9093, 'usko': 9094, 'maatta': 9095, 'carton': 9096, 'lighter': 9097, 'equal': 9098, 'onemed': 9099, 'januarymay': 9100, 'blood': 9101, 'rme': 9102, 'telcontar': 9103, 'calculate': 9104, 'userdefined': 9105, 'navigability': 9106, 'multimodal': 9107, 'routing': 9108, 'forget': 9109, 'kinder': 9110, 'installment': 9111, 'olavi': 9112, 'linden': 9113, 'artistic': 9114, 'refurbish': 9115, 'kersberga': 9116, 'observes': 9117, 'perttu': 9118, 'puro': 9119, 'tradeka': 9120, 'conagra': 9121, 'hillshire': 9122, 'connolly': 9123, 'geographically': 9124, 'astx': 9125, 'stkscoeovw': 9126, 'cracking': 9127, 'correction': 9128, 'lundmark': 9129, 'reliance': 9130, 'upcoming': 9131, 'jamnagar': 9132, 'han': 9133, 'dalborg': 9134, 'reelection': 9135, 'sitting': 9136, 'sideline': 9137, 'mediacityuk': 9138, 'shellbg': 9139, 'interconnected': 9140, 'mega': 9141, 'wgo': 9142, 'bod': 9143, 'mismanagement': 9144, 'unc': 9145, 'charlotte': 9146, 'deploy': 9147, 'connector': 9148, 'tikkakoski': 9149, 'hydrogen': 9150, 'peroxide': 9151, 'pb': 9152, 'gypsiipowered': 9153, 'symbianbased': 9154, 'promised': 9155, 'lotus': 9156, 'traveler': 9157, 'eliiv': 9158, 'eletric': 9159, 'testament': 9160, 'hammerson': 9161, 'dundrum': 9162, 'spreader': 9163, 'finbow': 9164, 'hiring': 9165, 'differs': 9166, 'principle': 9167, 'sculptural': 9168, 'rabochy': 9169, 'kolkhoznitsa': 9170, 'farmer': 9171, 'melting': 9172, 'viable': 9173, 'reino': 9174, 'tammela': 9175, 'programming': 9176, 'computerrelated': 9177, 'exclusivebp': 9178, 'cnpc': 9179, 'achievement': 9180, 'accident': 9181, 'tielinja': 9182, 'mou': 9183, 'collaborate': 9184, 'ngb': 9185, 'nn': 9186, 'eei': 9187, 'xm': 9188, 'sml': 9189, 'rut': 9190, 'iwc': 9191, 'sox': 9192, 'smallcaps': 9193, 'semi': 9194, 'fakeout': 9195, 'bout': 9196, 'seatbelt': 9197, 'fix': 9198, 'dominated': 9199, 'bestselling': 9200, 'refileupdate': 9201, 'rescue': 9202, 'ephc': 9203, 'cto': 9204, 'esko': 9205, 'myllyla': 9206, 'rtkm': 9207, 'enthusiasm': 9208, 'churning': 9209, 'topical': 9210, 'bill': 9211, 'drafted': 9212, 'oldage': 9213, 'eloholma': 9214, 'pntz': 9215, 'pervorouralsky': 9216, 'gama': 9217, 'endustri': 9218, 'tesisleri': 9219, 'imalat': 9220, 'montaj': 9221, 'minimize': 9222, 'ensuring': 9223, 'beneath': 9224, 'saimaa': 9225, 'undisturbed': 9226, 'com': 9227, 'photomsn': 9228, 'mso': 9229, 'companyeur': 9230, 'romanian': 9231, 'lei': 9232, 'tcotkmqvwhqq': 9233, 'multidiscipline': 9234, 'deepwater': 9235, 'faz': 9236, 'adobe': 9237, 'tcoljnxpevhzn': 9238, 'bbva': 9239, 'pd': 9240, 'ntls': 9241, 'ngl': 9242, 'sbrcy': 9243, 'fgl': 9244, 'mtnoy': 9245, 'ttt': 9246, 'aqua': 9247, 'multifunctional': 9248, 'hsc': 9249, 'arabian': 9250, 'lacquered': 9251, 'clock': 9252, 'stripy': 9253, 'handcrocheted': 9254, 'oras': 9255, 'steek': 9256, 'bordeaux': 9257, 'george': 9258, 'optimisation': 9259, 'thereby': 9260, 'boomed': 9261, 'helped': 9262, 'marilyn': 9263, 'monroe': 9264, 'finnishrussian': 9265, 'expiration': 9266, 'umm': 9267, 'nviro': 9268, 'independently': 9269, 'correspondingly': 9270, 'ru': 9271, 'vmw': 9272, 'sweeper': 9273, 'highdensity': 9274, 'tcohhmxsitwm': 9275, 'initiate': 9276, 'ytyv': 9277, 'wittine': 9278, 'longbow': 9279, 'album': 9280, 'calciners': 9281, 'submit': 9282, 'yomiuri': 9283, 'breakingviews': 9284, 'iag': 9285, 'mono': 9286, 'feeling': 9287, 'allstarcharts': 9288, 'stksconv': 9289, 'veil': 9290, 'stereotyping': 9291, 'speciality': 9292, 'stretch': 9293, 'vladivostok': 9294, 'swissswedish': 9295, 'honkajoki': 9296, 'findest': 9297, 'bcd': 9298, 'porvoo': 9299, 'seventeen': 9300, 'outperforms': 9301, 'connecting': 9302, 'fusing': 9303, 'personalized': 9304, 'mortar': 9305, 'swks': 9306, 'accumulated': 9307, 'northamerican': 9308, 'iiiii': 9309, 'pathological': 9310, 'gambling': 9311, 'addiction': 9312, 'uzbek': 9313, 'interfax': 9314, 'reng': 9315, 'feedstock': 9316, 'destination': 9317, 'recycle': 9318, 'inform': 9319, 'ir': 9320, 'johanna': 9321, 'participation': 9322, 'onefifth': 9323, 'authorise': 9324, 'rehabilitation': 9325, 'bukoba': 9326, 'musoma': 9327, 'situated': 9328, 'pentikinen': 9329, 'entitle': 9330, 'wisconsin': 9331, 'alabama': 9332, 'illinois': 9333, 'indiana': 9334, 'kentucky': 9335, 'ohio': 9336, 'magnet': 9337, 'easy': 9338, 'cdli': 9339, 'stkscodzu': 9340, 'qe': 9341, 'fed': 9342, 'aalto': 9343, 'jyvskyl': 9344, 'stkscodrhf': 9345, 'inge': 9346, 'larsen': 9347, 'optiflex': 9348, 'kdg': 9349, 'bidirectionality': 9350, 'subgroup': 9351, 'difference': 9352, 'longpos': 9353, 'nr': 9354, 'obj': 9355, 'investigated': 9356, 'stichting': 9357, 'pensioenfonds': 9358, 'alise': 9359, 'bxs': 9360, 'bancorpsouth': 9361, 'stkscospdf': 9362, 'wft': 9363, 'closng': 9364, 'xerox': 9365, 'teamed': 9366, 'tailor': 9367, 'igen': 9368, 'shortrun': 9369, 'ondemand': 9370, 'gavin': 9371, 'hattersley': 9372, 'suspended': 9373, 'hayward': 9374, 'califbased': 9375, 'casual': 9376, 'footwear': 9377, 'sandal': 9378, 'jensen': 9379, 'njastein': 9380, 'critch': 9381, 'dovre': 9382, 'toivola': 9383, 'universiti': 9384, 'sains': 9385, 'soaring': 9386, 'salmi': 9387, 'hanna': 9388, 'vuolteenaho': 9389, 'viitasaari': 9390, 'bovine': 9391, 'kuopio': 9392, 'rdc': 9393, 'rowan': 9394, 'punishing': 9395, 'reminder': 9396, 'spear': 9397, 'herd': 9398, 'australian': 9399, 'sulfide': 9400, 'unprofitable': 9401, 'speaks': 9402, 'healthier': 9403, 'txrh': 9404, 'refurbishing': 9405, 'sewer': 9406, 'pipe': 9407, 'protects': 9408, 'stored': 9409, 'bman': 9410, 'primed': 9411, 'eght': 9412, 'reorganise': 9413, 'rana': 9414, 'glassfiber': 9415, 'reinforcement': 9416, 'require': 9417, 'customized': 9418, 'companieslse': 9419, 'exsec': 9420, 'schapiro': 9421, 'absentee': 9422, 'nsm': 9423, 'traction': 9424, 'stkscogbsh': 9425, 'pivotable': 9426, 'rotation': 9427, 'axis': 9428, 'tcobfwsdbmu': 9429, 'conservative': 9430, 'raab': 9431, 'tue': 9432, 'wwwsampocom': 9433, 'lonnfors': 9434, 'ross': 9435, 'rbc': 9436, 'arabized': 9437, 'neareast': 9438, 'winwin': 9439, 'snack': 9440, 'justfood': 9441, 'tcocllmpinm': 9442, 'tcofpfbytypuy': 9443, 'companiesunilever': 9444, 'cream': 9445, 'medically': 9446, 'underwritten': 9447, 'stockdemons': 9448, 'reversing': 9449, 'wx': 9450, 'decent': 9451, 'latvenergo': 9452, 'xpower': 9453, 'isps': 9454, 'patrick': 9455, 'jeambar': 9456, 'hsea': 9457, 'hanoi': 9458, 'rentakran': 9459, 'alreadyworking': 9460, 'saltwaternurse': 9461, 'manhattan': 9462, 'flatiron': 9463, 'kolkata': 9464, 'frei': 9465, 'hull': 9466, 'dipping': 9467, 'tcojeplmzqkgw': 9468, 'nordalu': 9469, 'uh': 9470, 'dirk': 9471, 'custody': 9472, 'scorecard': 9473, 'milestonebased': 9474, 'processrelated': 9475, 'criterion': 9476, 'jrvi': 9477, 'fragmented': 9478, 'ineta': 9479, 'zaharova': 9480, 'britvic': 9481, 'pulling': 9482, 'warren': 9483, 'henry': 9484, 'gilchrist': 9485, 'withdraw': 9486, 'algeria': 9487, 'koduextra': 9488, 'nonfood': 9489, 'rukax': 9490, 'scantukka': 9491, 'virent': 9492, 'catalytic': 9493, 'biofuels': 9494, 'mar': 9495, 'innovational': 9496, 'beginner': 9497, 'spectrum': 9498, 'gemalto': 9499, 'fastest': 9500, 'hurting': 9501, 'markku': 9502, 'hangasjarvi': 9503, 'eriksson': 9504, 'yeah': 9505, 'kershaw': 9506, 'sankey': 9507, 'markka': 9508, 'oksanen': 9509, 'harald': 9510, 'kaaja': 9511, 'salokannel': 9512, 'kangasala': 9513, 'palin': 9514, 'viiala': 9515, 'topped': 9516, 'funded': 9517, 'delisted': 9518, 'nmlist': 9519, 'accordingly': 9520, 'costefficient': 9521, 'phosphorous': 9522, 'storing': 9523, 'nexbtl': 9524, 'hienonen': 9525, 'quantity': 9526, 'dinmukhamet': 9527, 'idrisov': 9528, 'tripadvisor': 9529, 'tcoolwcdxmbr': 9530, 'haakon': 9531, 'skaarer': 9532, 'ttmv': 9533, 'bestin': 9534, 'prof': 9535, 'excellently': 9536, 'braking': 9537, 'fasten': 9538, 'sharkbiotech': 9539, 'sellingshorting': 9540, 'prosperous': 9541, 'changshu': 9542, 'hongmei': 9543, 'reopen': 9544, 'pde': 9545, 'phosphodiesterase': 9546, 'inhibitor': 9547, 'discovered': 9548, 'compound': 9549, 'carmaker': 9550, 'hyundai': 9551, 'optimistically': 9552, 'bretagne': 9553, 'sotkamo': 9554, 'petri': 9555, 'ailus': 9556, 'isto': 9557, 'hantila': 9558, 'tom': 9559, 'justdrinks': 9560, 'performed': 9561, 'namely': 9562, 'franc': 9563, 'sharpest': 9564, 'reed': 9565, 'elsevier': 9566, 'underwhelming': 9567, 'wayne': 9568, 'greensmith': 9569, 'something': 9570, 'challenger': 9571, 'logset': 9572, 'aldi': 9573, 'hansen': 9574, 'knowledgebased': 9575, 'prnits': 9576, 'mainor': 9577, 'drag': 9578, 'slashing': 9579, 'practically': 9580, 'except': 9581, 'vimpelcom': 9582, 'kyivstar': 9583, 'soared': 9584, 'chartlyxbhb': 9585, 'disk': 9586, 'multimediacard': 9587, 'invented': 9588, 'ahvenainen': 9589, 'ruutana': 9590, 'orascom': 9591, 'oth': 9592, 'indital': 9593, 'bangalorebased': 9594, 'imap': 9595, 'lcd': 9596, 'moody': 9597, 'emtn': 9598, 'mwe': 9599, 'deka': 9600, 'showroom': 9601, 'fortitude': 9602, 'loser': 9603, 'renn': 9604, 'topping': 9605, 'tail': 9606, 'renounce': 9607, 'greek': 9608, 'skid': 9609, 'lappeenranta': 9610, 'fintech': 9611, 'cafn': 9612, 'cachet': 9613, 'surging': 9614, 'tcobjcski': 9615, 'jkhy': 9616, 'fisv': 9617, 'momo': 9618, 'sari': 9619, 'baldauf': 9620, 'hintikka': 9621, 'excise': 9622, 'nepal': 9623, 'karelia': 9624, 'nomura': 9625, 'caseus': 9626, 'tanking': 9627, 'suction': 9628, 'anchor': 9629, 'kalajoki': 9630, 'maemo': 9631, 'genvec': 9632, 'biopharmaceutical': 9633, 'vaccine': 9634, 'dialing': 9635, 'appearing': 9636, 'traceyryniec': 9637, 'wwg': 9638, 'kaman': 9639, 'edible': 9640, 'readymeal': 9641, 'icecream': 9642, 'multinational': 9643, 'ervi': 9644, 'wwd': 9645, 'decor': 9646, 'anasmotet': 9647, 'junction': 9648, 'marieholm': 9649, 'gasco': 9650, 'ruwais': 9651, 'sunk': 9652, 'depth': 9653, 'mud': 9654, 'vacuum': 9655, 'wwwmosmetroru': 9656, 'magnetic': 9657, 'aegn': 9658, 'adx': 9659, 'stkscoqzzx': 9660, 'monitoring': 9661, 'oneyear': 9662, 'kontturi': 9663, 'rajamaki': 9664, 'ylinen': 9665, 'hyvinkaa': 9666, 'usercontrolled': 9667, 'thirtyone': 9668, 'sheryl': 9669, 'tcovxtamnpk': 9670, 'yvonne': 9671, 'chameleon': 9672, 'domain': 9673, 'asm': 9674, 'lettable': 9675, 'ratasmaki': 9676, 'fgvoip': 9677, 'accomplishing': 9678, 'cree': 9679, 'yoy': 9680, 'scientist': 9681, 'movie': 9682, 'nonbinding': 9683, 'stoxx': 9684, 'primark': 9685, 'rack': 9686, 'ndquarter': 9687, 'gawker': 9688, 'wackiness': 9689, 'familyfun': 9690, 'liising': 9691, 'proper': 9692, 'enhancing': 9693, 'succeeds': 9694, 'gnter': 9695, 'technologyoriented': 9696, 'firmed': 9697, 'consultation': 9698, 'occupier': 9699, 'tcojvxqdoxnp': 9700, 'barum': 9701, 'matador': 9702, 'vnoto': 9703, 'internals': 9704, 'oka': 9705, 'nonresidential': 9706, 'logistical': 9707, 'enbca': 9708, 'pipleline': 9709, 'mainstream': 9710, 'vicepresident': 9711, 'shivakumar': 9712, 'fullfunction': 9713, 'kilogram': 9714, 'centimeter': 9715}\n"
          ]
        }
      ],
      "source": [
        "print(word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD1SJNRycU6e",
        "outputId": "ed99f499-99f4-4a76-b532-47a4ffcfece6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[105, 5, 2, 64, 860, 1031, 529]\n"
          ]
        }
      ],
      "source": [
        "print(xtrain_seq[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToqQXg4kAhS-"
      },
      "outputs": [],
      "source": [
        "#create embedding matrix for words that we have in dataset\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_index)+1, 300))\n",
        "for word,i in word_index.items():\n",
        "    embedding_vector = word2vec_pretrained_dict.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlj_0NP8AhS-"
      },
      "outputs": [],
      "source": [
        "# Build Custom Metrics (F1-Score)\n",
        "\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m_():\n",
        "    def f1_m(y_true, y_pred):\n",
        "        precision = precision_m(y_true, y_pred)\n",
        "        recall = recall_m(y_true, y_pred)\n",
        "        return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "    return f1_m\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LfjM27Pz4-P"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYyHx0JoAhS_"
      },
      "source": [
        "### Using LSTM and word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHBOabFmAhS_",
        "outputId": "2002f11b-9803-49fd-aeaf-7fb52fd53647"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "10/10 [==============================] - 19s 2s/step - loss: 1.0342 - f1_m: 0.1748 - val_loss: 0.9701 - val_f1_m: 0.4831\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.9583 - f1_m: 0.4576 - val_loss: 0.9728 - val_f1_m: 0.4713\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.9093 - f1_m: 0.4798 - val_loss: 0.8773 - val_f1_m: 0.4648\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.8842 - f1_m: 0.5099 - val_loss: 0.8796 - val_f1_m: 0.5244\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.8827 - f1_m: 0.4927 - val_loss: 0.8560 - val_f1_m: 0.5108\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.8500 - f1_m: 0.5369 - val_loss: 0.8958 - val_f1_m: 0.5520\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.8514 - f1_m: 0.5547 - val_loss: 0.8098 - val_f1_m: 0.5587\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.8379 - f1_m: 0.5670 - val_loss: 0.8011 - val_f1_m: 0.5973\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 15s 2s/step - loss: 0.8314 - f1_m: 0.5797 - val_loss: 0.7911 - val_f1_m: 0.6004\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.8165 - f1_m: 0.5748 - val_loss: 0.7897 - val_f1_m: 0.6145\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.7982 - f1_m: 0.5914 - val_loss: 0.7755 - val_f1_m: 0.6069\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.7992 - f1_m: 0.5967 - val_loss: 0.7798 - val_f1_m: 0.5833\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.7644 - f1_m: 0.6122 - val_loss: 0.7506 - val_f1_m: 0.6159\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 27s 3s/step - loss: 0.7636 - f1_m: 0.6154 - val_loss: 0.7195 - val_f1_m: 0.6370\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 16s 2s/step - loss: 0.7475 - f1_m: 0.6371 - val_loss: 0.7128 - val_f1_m: 0.6361\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.7415 - f1_m: 0.6302 - val_loss: 0.6877 - val_f1_m: 0.6487\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.7417 - f1_m: 0.6321 - val_loss: 0.7348 - val_f1_m: 0.6678\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.7239 - f1_m: 0.6666 - val_loss: 0.6989 - val_f1_m: 0.6610\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.6945 - f1_m: 0.6697 - val_loss: 0.6961 - val_f1_m: 0.6332\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.7047 - f1_m: 0.6616 - val_loss: 0.6589 - val_f1_m: 0.6619\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 16s 1s/step - loss: 0.6946 - f1_m: 0.6708 - val_loss: 0.6766 - val_f1_m: 0.6673\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 15s 2s/step - loss: 0.6578 - f1_m: 0.6844 - val_loss: 0.6696 - val_f1_m: 0.6736\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.6834 - f1_m: 0.6782 - val_loss: 0.6620 - val_f1_m: 0.6495\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.6821 - f1_m: 0.6788 - val_loss: 0.6486 - val_f1_m: 0.6737\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.6679 - f1_m: 0.6714 - val_loss: 0.6430 - val_f1_m: 0.6596\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.6466 - f1_m: 0.6844 - val_loss: 0.6407 - val_f1_m: 0.6828\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.6454 - f1_m: 0.6962 - val_loss: 0.6144 - val_f1_m: 0.6805\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 16s 2s/step - loss: 0.6148 - f1_m: 0.7140 - val_loss: 0.6434 - val_f1_m: 0.6503\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 15s 2s/step - loss: 0.6065 - f1_m: 0.7093 - val_loss: 0.6180 - val_f1_m: 0.6799\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.6060 - f1_m: 0.7214 - val_loss: 0.6116 - val_f1_m: 0.6983\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.6185 - f1_m: 0.7041 - val_loss: 0.5904 - val_f1_m: 0.7034\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.6000 - f1_m: 0.7222 - val_loss: 0.6152 - val_f1_m: 0.6753\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.6022 - f1_m: 0.7237 - val_loss: 0.6261 - val_f1_m: 0.6907\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.6062 - f1_m: 0.7090 - val_loss: 0.6357 - val_f1_m: 0.6925\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 17s 2s/step - loss: 0.5958 - f1_m: 0.7205 - val_loss: 0.6098 - val_f1_m: 0.7026\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.5726 - f1_m: 0.7376 - val_loss: 0.5888 - val_f1_m: 0.7336\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.5599 - f1_m: 0.7401 - val_loss: 0.5948 - val_f1_m: 0.7221\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.5514 - f1_m: 0.7508 - val_loss: 0.5852 - val_f1_m: 0.7266\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.5361 - f1_m: 0.7519 - val_loss: 0.5780 - val_f1_m: 0.7218\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.5306 - f1_m: 0.7532 - val_loss: 0.5699 - val_f1_m: 0.7130\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.5262 - f1_m: 0.7649 - val_loss: 0.5785 - val_f1_m: 0.7231\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.5011 - f1_m: 0.7640 - val_loss: 0.5967 - val_f1_m: 0.7460\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - 15s 2s/step - loss: 0.5070 - f1_m: 0.7690 - val_loss: 0.5564 - val_f1_m: 0.7272\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - 15s 2s/step - loss: 0.4856 - f1_m: 0.7690 - val_loss: 0.5686 - val_f1_m: 0.7518\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - 15s 2s/step - loss: 0.4885 - f1_m: 0.7718 - val_loss: 0.6701 - val_f1_m: 0.7310\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - 16s 2s/step - loss: 0.5149 - f1_m: 0.7622 - val_loss: 0.5660 - val_f1_m: 0.7413\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - 15s 2s/step - loss: 0.4943 - f1_m: 0.7766 - val_loss: 0.6495 - val_f1_m: 0.6969\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.5281 - f1_m: 0.7631 - val_loss: 0.6123 - val_f1_m: 0.7310\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.4769 - f1_m: 0.7787 - val_loss: 0.5631 - val_f1_m: 0.7478\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - 15s 2s/step - loss: 0.4732 - f1_m: 0.7824 - val_loss: 0.5697 - val_f1_m: 0.7475\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.4723 - f1_m: 0.7833 - val_loss: 0.5751 - val_f1_m: 0.7366\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - 16s 2s/step - loss: 0.4466 - f1_m: 0.7964 - val_loss: 0.6120 - val_f1_m: 0.7340\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - 18s 2s/step - loss: 0.4643 - f1_m: 0.7884 - val_loss: 0.6028 - val_f1_m: 0.7381\n",
            "Epoch 53: early stopping\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "\n",
        "#input Layers\n",
        "model.add(Embedding(len(word_index) + 1,\n",
        "                     300,\n",
        "                     weights=[embedding_matrix],\n",
        "                     trainable=False))\n",
        "#hedden Layers\n",
        "model.add(SpatialDropout1D(0.3))\n",
        "model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n",
        "\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.8))\n",
        "\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.8))\n",
        "\n",
        "#output layer , 3 lebals\n",
        "model.add(Dense(3))\n",
        "\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = [f1_m_() , precision_m() ,recall_m ,\"accuracy\"  ])\n",
        "\n",
        "# Fit the model with early stopping callback\n",
        "earlystop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
        "\n",
        "history = model.fit(xtrain_pad, y=y_train_enc, batch_size=512, epochs=100,\n",
        "          verbose=1, validation_data = (xtest_pad, y_test_enc), callbacks=[earlystop])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7C1p3ILkc60J"
      },
      "outputs": [],
      "source": [
        "# Saving the model for Future Inferences\n",
        "model_json = model.to_json()\n",
        "with open(dir_models+\"LSTM_word2vec.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(dir_models+\"LSTM_word2vec.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQGsu5Se0IQH"
      },
      "source": [
        "## Transfer Learning with (**BERT and GPT-2**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkuVG4N_AhTH"
      },
      "source": [
        "### BERT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458,
          "referenced_widgets": [
            "b2edbba27f6d4046bc8ced5e624309d3",
            "5d11c3fe7cf249aaae1f04b1ca13adae",
            "4be786d83188451881999979e17399d3",
            "6394383e65834965bcc53a5f3dfdea52"
          ]
        },
        "id": "tgpRO5XrAhTH",
        "outputId": "562187aa-1ff0-4c22-8672-178f9aa1fa59"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2edbba27f6d4046bc8ced5e624309d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d11c3fe7cf249aaae1f04b1ca13adae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4be786d83188451881999979e17399d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6394383e65834965bcc53a5f3dfdea52",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load the pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "bert_model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73PKOP-8AhTH",
        "outputId": "ec1a506a-8c87-434e-87b5-7cd18096ce32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_for_sequence_classification\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bert (TFBertMainLayer)      multiple                  109482240 \n",
            "                                                                 \n",
            " dropout_37 (Dropout)        multiple                  0         \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  2307      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109,484,547\n",
            "Trainable params: 109,484,547\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "bert_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynW88g2fAhTI"
      },
      "outputs": [],
      "source": [
        "# Define the optimizer, loss function, and metrics\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "# Here we can add our needed matrices like\n",
        "# metrics = [\n",
        "#     tf.keras.metrics.CategoricalAccuracy(),\n",
        "#     tf.keras.metrics.Precision(),\n",
        "#     tf.keras.metrics.Recall(),\n",
        "#     tf.keras.metrics.AUC(name='auc'),\n",
        "#     tf.keras.metrics.TruePositives(),\n",
        "#     tf.keras.metrics.FalsePositives(),\n",
        "#     tf.keras.metrics.TrueNegatives(),\n",
        "#     tf.keras.metrics.FalseNegatives()\n",
        "# ]\n",
        "#metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
        "metrics = [f1_m_() , tf.keras.metrics.Precision() ,recall_m ,\"accuracy\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGI3Ni50AhTI"
      },
      "outputs": [],
      "source": [
        "# Define the model inputs and outputs\n",
        "input_ids = tf.keras.layers.Input(shape=(98,), dtype=tf.int32, name='input_ids')\n",
        "attention_mask = tf.keras.layers.Input(shape=(98,), dtype=tf.int32, name='attention_mask')\n",
        "output = bert_model({'input_ids': input_ids, 'attention_mask': attention_mask})[0]\n",
        "\n",
        "# Add a dense layer with softmax activation for classification\n",
        "output = tf.keras.layers.Dense(3, activation='softmax')(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "id35tECDAhTI"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0cSZihgAhTI"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J763h7geAhTJ"
      },
      "outputs": [],
      "source": [
        "# Define the training and validation datasets\n",
        "\n",
        "def datasets(tokenizer):\n",
        "\n",
        "    # Split the data into training and validation sets\n",
        "    train_df, val_df, train_labels, val_labels = train_test_split(df.Sentence, df.Sentiment,\n",
        "                                                                  test_size=0.2, random_state=42,stratify = df.Sentiment)\n",
        "\n",
        "    # Tokenize the input sequences and convert to input IDs and attention masks\n",
        "    train_encodings = tokenizer(list(train_df.values), truncation=True, padding=True,max_length=98)\n",
        "    train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=3)\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']}, train_labels))\n",
        "\n",
        "    val_encodings = tokenizer(list(val_df.values), truncation=True, padding=True,max_length=98)\n",
        "    val_labels = tf.keras.utils.to_categorical(val_labels, num_classes=3)\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': val_encodings['input_ids'], 'attention_mask': val_encodings['attention_mask']}, val_labels))\n",
        "\n",
        "\n",
        "    # Batch and shuffle the datasets\n",
        "    batch_size = 32\n",
        "    train_dataset = train_dataset.batch(batch_size).shuffle(1000)\n",
        "    val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "    return train_dataset,val_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsRF9AWTAhTJ"
      },
      "outputs": [],
      "source": [
        "# Define the training and validation datasets\n",
        "\n",
        "train_dataset,val_dataset = datasets(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBKf8sB0BfqE",
        "outputId": "d4251c63-24f7-48ba-e141-489f8aad1bb2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_ShuffleDataset element_spec=({'input_ids': TensorSpec(shape=(None, 63), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(None, 63), dtype=tf.int32, name=None)}, TensorSpec(shape=(None, 3), dtype=tf.float32, name=None))>"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "732EVk6xAhTJ",
        "outputId": "0b414f40-9192-4b3c-d340-3a4b0639e857"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-b032e9485641>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model for a few epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 98), found shape=(None, 63)\n"
          ]
        }
      ],
      "source": [
        "# Train the model for a few epochs\n",
        "num_epochs = 3\n",
        "model.fit(train_dataset, validation_data=val_dataset, epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBIz34Kt3b_G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzzEJr3jHMEw"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e1PYge7AhTJ"
      },
      "source": [
        "### BERT and RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "hqmGU8TkAhTK",
        "outputId": "c3c77fcb-c9f8-41ae-f6fe-eaf35d1ef102"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "057d459cee714b39a101035ec3145df4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb91663db59e45c8aea20e8de027d798",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f180a684593643d799d53578012a2c45",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "65474081476d406c8fb2032a5fc3059e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['roberta.embeddings.position_ids', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load the RoBERTa tokenizer and model\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "roberta_model = TFRobertaModel.from_pretrained('roberta-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGTnYGtNAhTK"
      },
      "outputs": [],
      "source": [
        "# Define the training and validation datasets\n",
        "\n",
        "train_dataset,val_dataset = datasets(roberta_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y_SnhHYiAhTK",
        "outputId": "4951b75e-b25c-428c-abc1-3a7da2f59d78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " attention_mask (InputLayer)    [(None, 98)]         0           []                               \n",
            "                                                                                                  \n",
            " input_ids (InputLayer)         [(None, 98)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['attention_mask[0][0]',         \n",
            " el)                            thPoolingAndCrossAt               'input_ids[0][0]']              \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 98,                                                \n",
            "                                768),                                                             \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 3)            2307        ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 124,647,939\n",
            "Trainable params: 124,647,939\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-79f704733a9d>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Train the RoBERTa model on the sentiment analysis task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mroberta_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_1\" is incompatible with the layer: expected shape=(None, 98), found shape=(None, 65)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define the input shape for the model\n",
        "max_length = 98\n",
        "\n",
        "# Define the custom top layer for classification\n",
        "num_labels = 3\n",
        "top_layer = tf.keras.layers.Dense(num_labels, activation='softmax')\n",
        "\n",
        "# Define the RoBERTa model with the custom top layer\n",
        "input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
        "attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n",
        "roberta_output = roberta_model({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
        "roberta_output = roberta_output.last_hidden_state[:, 0, :]\n",
        "roberta_output = top_layer(roberta_output)\n",
        "roberta_model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=[roberta_output])\n",
        "\n",
        "print(roberta_model.summary())\n",
        "\n",
        "# Define the loss function and metrics for training\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "\n",
        "# Compile the RoBERTa model for training\n",
        "roberta_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "\n",
        "# Train the RoBERTa model on the sentiment analysis task\n",
        "roberta_model.fit(train_dataset, epochs=3, validation_data=val_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8FMTvYEGJlB"
      },
      "source": [
        "#Bert_with_roberta. **vm**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKWftCutGR_s"
      },
      "outputs": [],
      "source": [
        "dir = \"/content/drive/MyDrive/AI-projects/SentimentAnalysis/data.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ExmMEd8HKZe"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRlXiyKyGkBk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from transformers import (pipeline , BertTokenizer,\n",
        "                          TFBertForSequenceClassification,\n",
        "                          InputExample, InputFeatures ,\n",
        "                         AutoTokenizer, TFAutoModelForSequenceClassification,\n",
        "                         TFRobertaModel, TFGPT2Model, RobertaTokenizer, GPT2Tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73UR4LmuGE98"
      },
      "outputs": [],
      "source": [
        "# df = pd.read_csv(dir)\n",
        "\n",
        "# dicto = {'positive': 1, 'neutral': 0 , 'negative': -1}\n",
        "\n",
        "# df.Sentiment = df.Sentiment.map(dicto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "mV4zALnsGrZA",
        "outputId": "8d530015-3ead-4efd-adbf-27278c0a76b3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c98245e26ef0489bbe9510b2c7e7dc4d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)solve/main/vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca7882a5bf8f4d7cb981ee57c7e66f62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e480dd44b084e75a4a2834cd3b6bdf9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e84baab40af541909d998452e66be929",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load the pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "bert_model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu5mK0wkGrZB",
        "outputId": "4e28f0b1-bb52-4f17-c8f3-0066dc3f3544"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_for_sequence_classification\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bert (TFBertMainLayer)      multiple                  109482240 \n",
            "                                                                 \n",
            " dropout_37 (Dropout)        multiple                  0         \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  2307      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109,484,547\n",
            "Trainable params: 109,484,547\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "bert_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsh9eoK1GrZC"
      },
      "outputs": [],
      "source": [
        "# Define the optimizer, loss function, and metrics\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "metrics = [tf.keras.metrics.CategoricalAccuracy()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gm_oUD95GrZC"
      },
      "outputs": [],
      "source": [
        "# Define the model inputs and outputs\n",
        "input_ids = tf.keras.layers.Input(shape=(98,), dtype=tf.int32, name='input_ids')\n",
        "attention_mask = tf.keras.layers.Input(shape=(98,), dtype=tf.int32, name='attention_mask')\n",
        "output = bert_model({'input_ids': input_ids, 'attention_mask': attention_mask})[0]\n",
        "\n",
        "# Add a dense layer with softmax activation for classification\n",
        "output = tf.keras.layers.Dense(3, activation='softmax')(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEW6SIcTGrZC"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plykNnxeGrZD"
      },
      "outputs": [],
      "source": [
        "# Define the training and validation datasets\n",
        "\n",
        "def datasets(tokenizer):\n",
        "\n",
        "    # Split the data into training and validation sets\n",
        "    train_df, val_df, train_labels, val_labels = train_test_split(df.Sentence, df.Sentiment,\n",
        "                                                                  test_size=0.2, random_state=42,stratify = df.Sentiment)\n",
        "\n",
        "    # Tokenize the input sequences and convert to input IDs and attention masks\n",
        "    train_encodings = tokenizer(list(train_df.values), truncation=True, padding=True,max_length=98)\n",
        "    train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=3)\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']}, train_labels))\n",
        "\n",
        "    val_encodings = tokenizer(list(val_df.values), truncation=True, padding=True,max_length=98)\n",
        "    val_labels = tf.keras.utils.to_categorical(val_labels, num_classes=3)\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': val_encodings['input_ids'], 'attention_mask': val_encodings['attention_mask']}, val_labels))\n",
        "\n",
        "\n",
        "    # Batch and shuffle the datasets\n",
        "    batch_size = 32\n",
        "    train_dataset = train_dataset.batch(batch_size).shuffle(1000)\n",
        "    val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "    return train_dataset,val_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "MyTsv5ivGrZE",
        "outputId": "a8d345d7-306c-4e6a-8db9-59e3be3abc79"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "906266999c0a45bea48e3998d03860c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)olve/main/vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8cf5c2f07f4b416ca6eccdc92c100508",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)olve/main/merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d956e73f78c24a909442951520caeb72",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5960b5a256924dd68a423275605c279b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['roberta.embeddings.position_ids', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load the RoBERTa tokenizer and model\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "roberta_model = TFRobertaModel.from_pretrained('roberta-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9uMeLaoGrZF"
      },
      "outputs": [],
      "source": [
        "# Define the training and validation datasets\n",
        "\n",
        "train_dataset,val_dataset = datasets(roberta_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSe6aOElGrZF",
        "outputId": "703e5275-f7ac-44cc-dca6-6325f548e58f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " attention_mask (InputLayer)    [(None, 98)]         0           []                               \n",
            "                                                                                                  \n",
            " input_ids (InputLayer)         [(None, 98)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['attention_mask[0][0]',         \n",
            " el)                            thPoolingAndCrossAt               'input_ids[0][0]']              \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 98,                                                \n",
            "                                768),                                                             \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 3)            2307        ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 124,647,939\n",
            "Trainable params: 124,647,939\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 598/1222 [=============>................] - ETA: 6:45:51 - loss: 0.5064 - categorical_accuracy: 0.7856"
          ]
        }
      ],
      "source": [
        "# Define the input shape for the model\n",
        "max_length = 98\n",
        "\n",
        "# Define the custom top layer for classification\n",
        "num_labels = 3\n",
        "top_layer = tf.keras.layers.Dense(num_labels, activation='softmax')\n",
        "\n",
        "# Define the RoBERTa model with the custom top layer\n",
        "input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
        "attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n",
        "roberta_output = roberta_model({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
        "roberta_output = roberta_output.last_hidden_state[:, 0, :]\n",
        "roberta_output = top_layer(roberta_output)\n",
        "roberta_model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=[roberta_output])\n",
        "\n",
        "print(roberta_model.summary())\n",
        "\n",
        "# Define the loss function and metrics for training\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "# Here we can add our needed matrices like\n",
        "# metrics = [\n",
        "#     tf.keras.metrics.CategoricalAccuracy(),\n",
        "#     tf.keras.metrics.Precision(),\n",
        "#     tf.keras.metrics.Recall(),\n",
        "#     tf.keras.metrics.AUC(name='auc'),\n",
        "#     tf.keras.metrics.TruePositives(),\n",
        "#     tf.keras.metrics.FalsePositives(),\n",
        "#     tf.keras.metrics.TrueNegatives(),\n",
        "#     tf.keras.metrics.FalseNegatives()\n",
        "# ]\n",
        "\n",
        "metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "\n",
        "# Compile the RoBERTa model for training\n",
        "roberta_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train the RoBERTa model on the sentiment analysis task\n",
        "roberta_model.fit(train_dataset, epochs=3, validation_data=val_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg-ErMjoKLCs"
      },
      "source": [
        " - **147/147 : This bold text** indicates that the training process has completed 147 batches. It represents the progress of training through the available training data.\n",
        "\n",
        "- **[==============================]**: This progress bar represents the completion of the batches. Each \"=\" symbol represents the completion of one batch.\n",
        "\n",
        "- **3694s 25s/step**: The training process took approximately 3694 seconds (or about 61.57 minutes) to complete all the batches. The \"25s/step\" indicates the average time taken for each training step.\n",
        "- **loss: 0.2756:** The training loss at the end of the epoch is 0.2756. The loss value indicates the discrepancy between the predicted and actual values during training. Lower values indicate better model performance.\n",
        "\n",
        "- **categorical_accuracy: 0.8566:** The training accuracy at the end of the epoch is 0.8566. This metric represents the percentage of correctly predicted labels for the training data.\n",
        "\n",
        "- **val_loss: 0.4078:** The validation loss at the end of the epoch is 0.4078. This loss value is calculated on a separate validation dataset and provides an indication of how well the model generalizes to unseen data.\n",
        "\n",
        "- **val_categorical_accuracy: 0.8246:** The validation accuracy at the end of the epoch is 0.8246. This metric represents the percentage of correctly predicted labels for the validation data.\n",
        "\n",
        "- **<keras.callbacks.History at 0x7f78bb4c5600>:** This line indicates that the training history object has been returned. It is typically assigned to a variable and can be used to analyze the training progress or plot learning curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEBOGdLjNFjs"
      },
      "outputs": [],
      "source": [
        "# Saving the model for Future Inferences\n",
        "dir_models = \"/content/drive/MyDrive/AI-projects/SentimentAnalysis/\"\n",
        "from tensorflow.keras.models import model_from_json\n",
        "\n",
        "model_json = roberta_model.to_json()\n",
        "with open(dir_models+\"roberta_model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "roberta_model.save_weights(dir_models+\"roberta_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DE6f1uqVPVQ6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lamMogaWHH4G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTtz2E6vG_TB"
      },
      "source": [
        "# Bert new **VM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bp32Bg2dIL5T"
      },
      "outputs": [],
      "source": [
        "dir = \"/content/drive/MyDrive/AI-projects/SentimentAnalysis/data.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xe5WI2T_IL5U",
        "outputId": "97ebe4ea-7c30-45d6-c3d2-889e227f533b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wIhBX1eIL5V"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from transformers import (pipeline , BertTokenizer,\n",
        "                          TFBertForSequenceClassification,\n",
        "                          InputExample, InputFeatures ,\n",
        "                         AutoTokenizer, TFAutoModelForSequenceClassification,\n",
        "                         TFRobertaModel, TFGPT2Model, RobertaTokenizer, GPT2Tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmtqeO6iIL5V"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(dir)\n",
        "\n",
        "dicto = {'positive': 1, 'neutral': 0 , 'negative': -1}\n",
        "\n",
        "df.Sentiment = df.Sentiment.map(dicto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "InLS7ohnIL5W",
        "outputId": "9236a4bf-602a-4b88-cef9-2ba7d2399e9c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2bbb27005e524677a3f4f1a5c4d049ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "967efbad62b04145ba7d1301289fc451",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2eb6d04a7d14399a1612a8a6d86103e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4698fa5311a4961b6acf079a2d6cfd4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading tf_model.h5:   0%|          | 0.00/536M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load the pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "bert_model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "469Krbj2IL5W",
        "outputId": "79f29e14-9469-45c2-ebc6-0e6764433cf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_for_sequence_classification\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bert (TFBertMainLayer)      multiple                  109482240 \n",
            "                                                                 \n",
            " dropout_37 (Dropout)        multiple                  0         \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  2307      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109,484,547\n",
            "Trainable params: 109,484,547\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "bert_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jj8hynS4IL5W"
      },
      "outputs": [],
      "source": [
        "# Define the optimizer, loss function, and metrics\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "metrics = [tf.keras.metrics.CategoricalAccuracy()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k093Q_MHIL5X"
      },
      "outputs": [],
      "source": [
        "# Define the model inputs and outputs\n",
        "input_ids = tf.keras.layers.Input(shape=(98,), dtype=tf.int32, name='input_ids')\n",
        "attention_mask = tf.keras.layers.Input(shape=(98,), dtype=tf.int32, name='attention_mask')\n",
        "output = bert_model({'input_ids': input_ids, 'attention_mask': attention_mask})[0]\n",
        "\n",
        "# Add a dense layer with softmax activation for classification\n",
        "output = tf.keras.layers.Dense(3, activation='softmax')(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1TL8dMlIL5X"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEVdVPsVIL5X",
        "outputId": "94d52b31-1c8e-4598-82c9-84e47f205572"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " attention_mask (InputLayer)    [(None, 98)]         0           []                               \n",
            "                                                                                                  \n",
            " input_ids (InputLayer)         [(None, 98)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_for_sequence_classific  TFSequenceClassifie  109484547  ['attention_mask[0][0]',         \n",
            " ation (TFBertForSequenceClassi  rOutput(loss=None,               'input_ids[0][0]']              \n",
            " fication)                      logits=(None, 3),                                                 \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 3)            12          ['tf_bert_for_sequence_classifica\n",
            "                                                                 tion[0][0]']                     \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,484,559\n",
            "Trainable params: 109,484,559\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iD38_hFFIL5Y"
      },
      "outputs": [],
      "source": [
        "# Define the training and validation datasets\n",
        "\n",
        "def datasets(tokenizer):\n",
        "\n",
        "    # Split the data into training and validation sets\n",
        "    train_df, val_df, train_labels, val_labels = train_test_split(df.Sentence, df.Sentiment,\n",
        "                                                                  test_size=0.2, random_state=42,stratify = df.Sentiment)\n",
        "\n",
        "    # Tokenize the input sequences and convert to input IDs and attention masks\n",
        "    train_encodings = tokenizer(list(train_df.values), truncation=True, padding=True,max_length=98)\n",
        "    train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=3)\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']}, train_labels))\n",
        "\n",
        "    val_encodings = tokenizer(list(val_df.values), truncation=True, padding=True,max_length=98)\n",
        "    val_labels = tf.keras.utils.to_categorical(val_labels, num_classes=3)\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': val_encodings['input_ids'], 'attention_mask': val_encodings['attention_mask']}, val_labels))\n",
        "\n",
        "\n",
        "    # Batch and shuffle the datasets\n",
        "    batch_size = 32\n",
        "    train_dataset = train_dataset.batch(batch_size).shuffle(1000)\n",
        "    val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "    return train_dataset,val_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRnFd6bcIL5Y"
      },
      "outputs": [],
      "source": [
        "# Define the training and validation datasets\n",
        "\n",
        "train_dataset,val_dataset = datasets(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFuw8AgfIL5Y"
      },
      "outputs": [],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_8rhUKHIL5Y",
        "outputId": "2d00faf0-e955-4f35-cba1-bea0b8df64fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "147/147 [==============================] - 5680s 38s/step - loss: 0.7878 - categorical_accuracy: 0.6670 - val_loss: 0.5487 - val_categorical_accuracy: 0.7819\n",
            "Epoch 2/3\n",
            "147/147 [==============================] - 5224s 36s/step - loss: 0.4406 - categorical_accuracy: 0.8162 - val_loss: 0.4515 - val_categorical_accuracy: 0.7998\n",
            "Epoch 3/3\n",
            "147/147 [==============================] - 5233s 36s/step - loss: 0.3014 - categorical_accuracy: 0.8568 - val_loss: 0.4694 - val_categorical_accuracy: 0.7836\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe9aa09dae0>"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model for a few epochs\n",
        "num_epochs = 3\n",
        "model.fit(train_dataset, validation_data=val_dataset, epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXXQzZHTIL5Z"
      },
      "outputs": [],
      "source": [
        "# Saving the model for Future Inferences\n",
        "dir_models = \"/content/drive/MyDrive/AI-projects/SentimentAnalysis/\"\n",
        "from tensorflow.keras.models import model_from_json\n",
        "\n",
        "model_json = model.to_json()\n",
        "with open(dir_models+\"Bert.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(dir_models+\"Bert.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFRhPi3xN3s_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di-oXuoFAhTK"
      },
      "source": [
        "# GPT-2 ( Transfer Learning )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Su29bxmAhTL",
        "outputId": "4f4debac-49a4-4dcb-f263-fb006a564e50"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ecb9f04e0c9447109262cfc2134bc187",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77f4d61bb19249c5877a734c2948c2d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37d2d2efb720465b87b995c36a9edec1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01b438d86d554853841ce615aa52e472",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1916b481e28e4566b31984a341c57581",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "Downloading:   0%|          | 0.00/475M [00:00<?, ?B/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2Model.\n",
            "\n",
            "All the layers of TFGPT2Model were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "# Load the GPT-2 tokenizer and model\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "gpt2_model = TFGPT2Model.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSNQed8mAhTL"
      },
      "outputs": [],
      "source": [
        "# Define the training and validation datasets\n",
        "\n",
        "# Set pad_token\n",
        "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
        "\n",
        "train_dataset,val_dataset = datasets(gpt2_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-FmCTN7AhTL",
        "outputId": "c3331ad3-6011-4ef9-aefc-728b8d03fe0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.7/site-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['attention_mask'] which did not match any model input. They will be ignored by the model.\n",
            "  inputs = self._flatten_to_reference_inputs(inputs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "147/147 [==============================] - 2001s 13s/step - loss: 1.1850 - categorical_accuracy: 0.4800 - val_loss: 0.9770 - val_categorical_accuracy: 0.5338\n",
            "Epoch 2/3\n",
            "147/147 [==============================] - 1925s 13s/step - loss: 1.0113 - categorical_accuracy: 0.5087 - val_loss: 0.9935 - val_categorical_accuracy: 0.5535\n",
            "Epoch 3/3\n",
            "147/147 [==============================] - 1954s 13s/step - loss: 0.9545 - categorical_accuracy: 0.5632 - val_loss: 0.8883 - val_categorical_accuracy: 0.6279\n"
          ]
        },
        {
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7fe06430dfd0>"
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the input shape for the model\n",
        "max_length = 98\n",
        "\n",
        "# Define the custom top layer for classification\n",
        "num_labels = 3\n",
        "top_layer = tf.keras.layers.Dense(num_labels, activation='softmax')\n",
        "\n",
        "# Define the GPT-2 model with the custom top layer\n",
        "input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
        "gpt2_output = gpt2_model(input_ids)[0]\n",
        "gpt2_output = gpt2_output[:, -1, :]\n",
        "gpt2_output = top_layer(gpt2_output)\n",
        "gpt2_model = tf.keras.models.Model(inputs=input_ids, outputs=gpt2_output)\n",
        "\n",
        "# Define the loss function and metrics for training\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "\n",
        "# Compile the GPT-2 model for training\n",
        "gpt2_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "\n",
        "# Train the GPT-2 model on the sentiment analysis task\n",
        "gpt2_model.fit(train_dataset, epochs=3, validation_data=val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQ2UTFEgICqp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "PPMs8Ea01HR3",
        "bfsjvuOl1ZD9",
        "F76q3G991c_J",
        "rBWOtrf-UOGE",
        "99_lxPWs1ibp",
        "7dFdZLZuXq7Q",
        "RLjf4dofXxBk",
        "bnqdG0VinmWM",
        "R2efmRvXdDXn",
        "6TXwKifbg8Au",
        "-e1PYge7AhTJ",
        "e8FMTvYEGJlB",
        "hTtz2E6vG_TB",
        "Di-oXuoFAhTK"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}